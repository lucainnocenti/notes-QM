\documentclass[12pt]{report}

% =============== bibliography  ========================
\usepackage[
    backend=biber,
    style=authoryear-comp,
    maxcitenames=1,
    url=true,
    doi=true,
    sorting=ynt
]{biblatex}
\DeclareNameAlias{author}{first-last}
% use the following to print URL only if no DOI is defined
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map[overwrite]{
      \step[fieldsource=doi, final]
      \step[fieldset=url, null]
      % \step[fieldset=eprint, null]
    }  
  }
}
% ================ packages to use Italian characters ================
% Note that the utf8 encoding only works if the file is saved in UTF-8 encoding
% \usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{csquotes}
\usepackage[italian,english]{babel}

\usepackage[a4paper, margin=1.2in]{geometry}
% \usepackage[a4paper,showframe,textwidth=00pt]{geometry}
% \usepackage{geometry}
%  \geometry{
%  a4paper,
%  total={170mm,257mm},
%  left=20mm,
%  top=20mm,
%  }

% \usepackage[T1]{fontenc}
% ================ general page layout ================
\usepackage{fontspec}
\usepackage{microtype}
% \usepackage{fullpage}
% ================ mathematics and fonts ================
\usepackage{bm} % provides more bold symbols

% bbold provides \mathbb{1}, but ruins letters, and \mathbb{1} itself is pixelated.
% \usepackage{bbold}
\usepackage[usenames,dvipsnames,table]{xcolor}

\usepackage{easyReview}
\usepackage{physics} % provides Dirac notation and lots of other things
\usepackage{minitoc}

\usepackage{dsfont} % provides more \mathbb \smallskipymbols
\usepackage{amsmath,amssymb,amsthm,thmtools}
\usepackage{mathtools}
\usepackage{cases}
\usepackage{calc}
\usepackage{mathrsfs} % provides the nice \matchsrc font
% \usepackage{soulutf8} % provides \st and \ul
\usepackage{soul}
\usepackage[normalem]{ulem} %provides \sout command for striking through text

\usepackage{float} % provides the option H for includegraphics

% =========== HYPERREF AND CO. =============
\usepackage{nameref, hyperref, cleveref}
\crefname{appsec}{Appendix}{Appendices}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = green!80!black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = red!80!black %Colour of citations
}

\usepackage{graphicx}

\usepackage[skins]{tcolorbox}
\tcbuselibrary{breakable}

\newtcolorbox[auto counter]{depthbox}[2][]{%
    enhanced, breakable, title=Box~\thetcbcounter: #2 #1%
}


\usepackage[font=small,justification=centering]{caption} % tweak fonts in labels
\usepackage{subcaption}
% \newenvironment{Figure}
%   {\par\medskip\noindent\minipage{\linewidth}}
%   {\endminipage\par\medskip}

% placeins provides \FloatBarrier command. Also, with the section option it automatically puts a \FloatBarrier before each section (same with subsection option ecc.)
\usepackage{placeins}
\usepackage{multicol}
\usepackage{multirow,tabularx,booktabs}
\setlength{\columnsep}{1cm}

\usepackage{parskip} % suppress indenting new paragraphs


\usepackage[printonlyused,withpage,nohyperlinks,smaller]{acronym}
\input{fix_acronym.tex}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{backgrounds}
\usetikzlibrary{shapes.geometric}

\graphicspath{{./figures/}}
\addbibresource{bibliography.bib}
\input{fix_authoryear_style}

%=================== CUSTOM MACROS ==============================
\newcommand{\CC}{\mathbb{C}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\mxm}{{m \!\times\! m}}
\newcommand{\NxN}{{N \!\times\! N}}
\newcommand{\xxy}[2]{{#1 \! \times \! #2}}
\newcommand{\dimens}[2]{{#1$\!\times\!$#2}}
\newcommand{\on}[1]{\operatorname{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}


% \newcommand{\F}{\mathcal{F}}
% \newcommand{\D}{\mathcal{D}}
% \renewcommand{\calS}{\mathcal{S}}
% \newcommand{\I}{\mathcal{I}}
% \renewcommand{\L}{\mathcal{L}}

\newcommand{\calA}{{\mathcal{A}}}
\newcommand{\calB}{{\mathcal{B}}}
\newcommand{\calC}{{\mathcal{C}}}
\newcommand{\calD}{{\mathcal{D}}}
\newcommand{\calE}{{\mathcal{E}}}
\newcommand{\calF}{{\mathcal{F}}}
\newcommand{\calG}{{\mathcal{G}}}
\newcommand{\calH}{{\mathcal{H}}}
\newcommand{\calI}{{\mathcal{I}}}
\newcommand{\calJ}{{\mathcal{J}}}
\newcommand{\calY}{{\mathcal{Y}}}
\newcommand{\calL}{{\mathcal{L}}}
\newcommand{\calM}{{\mathcal{M}}}
\newcommand{\calO}{{\mathcal{O}}}
\newcommand{\calP}{{\mathcal{P}}}
\newcommand{\calQ}{{\mathcal{Q}}}
\newcommand{\calS}{{\mathcal{S}}}
\newcommand{\calU}{{\mathcal{U}}}
\newcommand{\calV}{{\mathcal{V}}}
\newcommand{\calW}{{\mathcal{W}}}
\newcommand{\calX}{{\mathcal{X}}}
\newcommand{\calZ}{{\mathcal{Z}}}

\newcommand{\rmC}{{\mathrm{C}}}
\newcommand{\rmD}{{\mathrm{D}}}
\newcommand{\rmL}{{\mathrm{L}}}
\newcommand{\rmT}{{\mathrm{T}}}
\newcommand{\rmU}{{\mathrm{U}}}

\newcommand{\wondering}[1]{{\color{red}\itshape\bfseries (#1)}}
\newcommand{\uncertain}[1]{{\color{red}\bfseries (#1)}}
\newcommand{\nice}[1]{{\color{green!50!black} #1}}
\newcommand{\bad}[1]{{\color{red!60!black} #1}}

% \newcommand{\parTitle}[1]{\noindent{\color{Mahogany}(\emph{#1})}}
\newcommand{\heading}[1]{{\color{Mahogany}\emph{(#1)}}}


\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Imag}{Imag}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Pos}{Pos}
\DeclareMathOperator{\Herm}{Herm}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Lin}{Lin}
\DeclareMathOperator{\Prob}{Prob}
\DeclareMathOperator{\Proj}{Proj}

\DeclareMathOperator*{\argmax}{argmax}


% ========== EXTERNAL FILES ==============================
\input{theorems.tex}

\date{\today}

\begin{document}

\title{Quantum mechanics}
\author{Luca Innocenti}
\maketitle

\dominitoc
\tableofcontents

\chapter{Schr\"{o}dinger equation and stuff}

\begin{prop}
	The Hamiltonian is by definition the infinitesimal generator of the propagator:
	\begin{equation}
		\frac{d}{dt}\mathcal U(t,t_0) = (-iH(t))\mathcal U(t,t_0).
	\end{equation}
\end{prop}

\begin{thm}[Schr\"{o}dinger-Robertson uncertainty relations]
	Let $A$ and $B$ be Hermitian operators, and let us denote their variances on some state $\ket\psi$ as
	\[ \sigma_A^2\equiv\Var[A]=\EE[(A-\EE[A])^2] = \expval{A^2}-\expval{A}^2, \]
	and similarly for $\sigma_B^2$.
	Then the following holds:
	\begin{equation}
		\sigma_A^2 \sigma_B^2 \ge
		\abs{\expval{\{A,B\}}-\frac{1}{2}\expval A\expval B}^2
		+ \abs{\frac{1}{2i} \expval{[A,B]}}^2,
	\end{equation}
	and thus, in particular,
	\begin{equation}
		\sigma_A\sigma_B\ge\frac{1}{2}\abs{\expval{[A,B]}}.
	\end{equation}
\end{thm}
\begin{proof}
	The main observation is that $\sigma_A^2\equiv\Var(A)=\|A-\EE[A]\|^2$.
\end{proof}

\begin{prop}
	Let $\ket n,\ket m$ be (time-dependent) eigenvectors of the time-dependent Hamiltonian $H$, with
	$H\ket n=E_n\ket n$ and $H\ket m=E_m\ket m$, $E_n\neq E_m$. Then,
	\[
		\mel{m}{\partial_t H}{n}=(E_n-E_m)\braket{m}{\dot n}
		+ \delta_{nm} \dot E_n.
	\]
	\label{prop:components_dotH}
\end{prop}

\begin{remark}
	If two eigenstates orthonormal $\ket m, \ket n$ are degenerate,~\cref{prop:components_dotH} says that $\mel{m}{\dot H}{n}=0$. While this is true, it should be noted that it only holds if the eigenstates are chosen to be \emph{differentiable} at all times. This is a nontrivial point when there is a degeneracy: consider for example the case $H(t)=t\sigma_x$. If we choose $\ket0, \ket1$ as initial eigenstates, we get a discontinuity because the eigenstates for $t>0$ are $\ket\pm$.
\end{remark}

\begin{prop}
	Let $\ket\psi=\ket{\psi(t)}$ be the state at time $t$, evolving with a time-dependent Hamiltonian $H=H(t)$, whose instantaneous eigenvalues and eigenvectors are $E_n=E_n(t)$ and $\ket n=\ket{n(t)}$, respectively.
	Then,
	\begin{equation}
		\frac{d}{dt}\braket{n}{\psi} = -i\mel{n}{H}{\psi} + \braket{\dot n}{\psi}.
	\end{equation}
	Using~\cref{prop:components_dotH} this translates into
	\begin{equation}
		\frac{d}{dt}\braket{n}{\psi} =
		- i \mel{n}{H}{\psi}
		+ \sum_k' \braket{k}{\psi} \frac{\dot H_{nk}}{E_k - E_n}
		+ \sum_k'' \braket{\dot n}{k}\braket{k}{\psi},
	\end{equation}
	where the primed sum is over all eigenstates $E_k$ with $E_k\neq E_n$, while the doubly primed sum is over all eigenstates $E_k$ with $E_k=E_n$.
	\label{prop:derivative_expectation_values}
\end{prop}

\begin{prop}
	For any instantaneous eigenstate $\ket n\equiv\ket{n(t)}$, $\braket{\dot n}{n}$ is purely imaginary.
	\label{prop:ndotn_is_purely_imag}
\end{prop}
\begin{proof}
	Follows immediately by differentiating $\braket{n(t)}{n(t)}=1$.
\end{proof}

\begin{prop}
	If $\ket{\psi(t_0)}=\ket{n(t_0)}$, then~\cref{prop:derivative_expectation_values} becomes
	\begin{align}
		\left. \frac{d}{dt}\braket{n}{\psi}\right\rvert_{t=t_0} &= 
		-i E_n(t_0) + \braket{\dot n(t_0)}{n(t_0)}, \\
		\left. \frac{d}{dt}\braket{k}{\psi}\right\rvert_{t=t_0} &= 
		\braket{\dot k(t_0)}{n(t_0)} = \frac{\dot H_{kn}(t_0)}{E_k(t_0)-E_n(t_0)},\quad E_k(t_0)\neq E_n(t_0).
	\end{align}
\end{prop}

\begin{prop}
	Let $H(t)$ be a time-dependent Hamiltonian, and
	$\ket{\Psi}\equiv \sum_n c_n(t)e^{i\theta_n(t)}\ket{\psi_n(t)}$ with
	$\theta_n(t)\equiv-\int_0^t d\tau\,E_n(\tau)$.
	Then, assuming $E_n\neq E_m$ for all $n,m$, the Schr\"{o}dinger equation for $\ket{\Psi(t)}$ is equivalent to the following equations for the coefficients:
	\begin{equation}
	\begin{aligned}
		\dot c_n &= -c_n \braket{\psi_n}{\dot n}
		- \sum_m c_m e^{i(\theta_m-\theta_n)} \braket{ n}{\dot m} \\
		&= -c_n \braket{ n}{\dot n}
		- \sum_m c_m e^{i(\theta_m-\theta_n)} \frac{\mel{n}{\partial_t H}{ m}}{E_m-E_n}.
	\end{aligned}
	\end{equation}
\end{prop}

\begin{question}
	What about the degenerate case? Also, more precise statements about precision of this approximation? What is the probability of excited states being occupied?
\end{question}

Integrability of time-dependent Hamiltonian~\autocite{sinitsyn2018integrable}.

\begin{prop}
	Consider a state $\ket{\psi_0}=\sum_n c_n \ket n$ evolving according to a time-independent Hamiltonian $H$, with $H\ket n=E_n \ket n$. Let us denote with $\ket{\psi_t}=e^{-iHt}\ket{\psi_0}$ the state evolved at time $t$, and with $S(t)=\braket{\psi_0}{\psi_t}$ the associated overlap.
	We then have $S(t)=\sum_n \abs{c_n}^2 e^{-iE_n t}=\expval{\exp(-iHt)}$, and
	\begin{equation}
		\arccos\abs{S(t)} = \Delta H\, t + \mathcal O(t^3).
	\end{equation}
\end{prop}

\chapter{States}

\section{State space geometry}

The main reference is~\parencite{bengtsson2016a}.

\begin{prop}
	Given a complete basis of Hermitian, traceless, orthogonal, operators, with normalisation $\Tr(\Lambda_i\Lambda_j)=d\delta_{ij}$, any state $\rho$ can be represented in terms of its expectation values over this basis as
	\begin{equation}
		\rho = \frac{1}{d}\left[ I + \sum_k \Tr(\rho\Lambda_k)\Lambda_k \right].
	\end{equation}
	This is the so-called \emph{Bloch space} representation of $\rho$.
\end{prop}

\begin{prop}
	The set of states $\calS(\calH)$, with $\calH$ $d$-dimensional, is homeomorphic to the \emph{Bloch space} $B(\RR^{d^2-1})$, which is defined as the set of expectation values of all states $\rho$ over a complete set of traceless, orthogonal observables:
	\begin{equation}
		B(\RR^{d^2-1})\equiv\left\{
			(\tr(\rho\Lambda_k))_{k=1}^{d^2-1}:\,\,\rho\in\calS(\calH)
		\right\} \subset \RR^{d^2-1}.
	\end{equation}
\end{prop}

\begin{prop}[\cite{lidar2013quantum} and \href{https://physics.stackexchange.com/a/425101/58382}{physics.SE answer}]
	The \emph{Bloch space} $B(\RR^{d^2-1})$ can be written as
	\begin{equation}
		B(\RR^{d^2-1}) \equiv \left\{
			\bs b\in\RR^{d^2-1}\,:\,\,\|\bs b\|\le\frac{1}{\abs{m(\Lambda_{\hat{\bs b}})}}
		\right\},
	\end{equation}
	where $\hat{\bs b}\equiv\bs b/\|\bs b\|$, $\Lambda_{\hat{\bs b}}\equiv\sum_k \hat{\bs b}_k \Lambda_k$, and $m(\Lambda_{\hat{\bs b}})$ denotes the smallest eigenvalue of $\Lambda_{\hat{\bs b}}$.
\end{prop}

\begin{prop}[a]
	The set of linear operators on an Euclidean space $\calX$ is spanned by the set of density matrices on $\calX$:
	\begin{equation}
		\on{span}\{\rho_{a,b}\,:\,\,(a,b)\in\Sigma\times\Sigma\}
		= \calL(\CC^\Sigma).
	\end{equation}
\end{prop}

\section{State tomography methods}

\textcite{gross2010quantum} propose a tomography protocol to retrieve an $n$-qubit state using Pauli measurements with only $O(rd \log^2 d)$ measurement settings, with $d=2^n$ and $r$ the rank of state.
A follow-up of this work by the same authors is~\parencite{flammia2012quantum}.

\textcite{blumekohout2010hedged} propose and analyse the ``\emph{hedged maximum likelihood estimation}'' state tomography method.
\begin{itemize}
	\item \heading{Goal} The goal of the algorithm is to find an estimator $\hat\rho$ which is as close as possible to the true (unknown) state $\rho$, according to some metric $d(\rho,\hat\rho)$.
	\item \heading{Previous state of the art} The most common tomography scheme was maximum likelihood estimation. This has the shortcoming of being unstable when there are zero eigenvalues --- this so-called ``zero eigenvalue problem'' can be ``disastrous in practice''.
	\item\heading{Problem with standard MLE} Standard MLE works as follows: if sampling from a probability distribution $\bs p$ we find $n_k$ ``k''s, the likelihood $\calL(\bs p)=\prod_k p_k^{n_k}$ is maximised by the estimators
	$\hat p_k = n_k/N$. This is problematic because if we find some $n_k=0$ the guess becomes $\hat p_k=0$, which might not be the case when $p_k$ is simply small enough that the limited sample has not picked up that $k$.
	In the quantum case, the standard approach is, given a POVM $\{\mu_i\}$, to find $\hat\rho$ by inverting the linear system
	\begin{equation}
		\Tr[\hat\rho \mu_i] = \frac{n_i}{N},
	\end{equation}
	where $n_a$ is the number of $a$ outcomes.
	Inverting the linear system is problematic: for one, it often leads to non-positive --- and thus non-physical --- operators $\hat\rho$.
	An alternative method is MLE, where one instead defines $\hat\rho$ as the operator maximising the likelihood $\calL(\rho)\equiv\Pr(\{n_i\}|\hat\rho)$.
	With this method one can restrict the optimisation to only positive $\rho$. This method has, however, other shortcomings, and in particular the ``zero eigenvalue problem''.
	\item\heading{The novel idea} They propose using an ``hedged estimator''. These are estimators which, they claim, better handle the cases where zero events are observed for some outcome.
\end{itemize}

\textcite{blumekohout2010optimal} propose a \emph{Bayesian mean estimation} tomography method, which they claim handles the zero eigenvalue problem.
They nicely review standard and MLE tomography methods, and highlight their similarities and common shortcomings. In particular, their both sharing the zero eigenvalue problem.
They propose, as an alternative, \emph{Bayesian mean estimation} methods.
\begin{itemize}
	\item \heading{BME scheme} BME schemes work by (1) use the data to generate the likelihood function $\calL(\rho) =p(\calM|\rho)$. Here, $\calM$ is the observed data, and $\rho$ is the ``proposed state''. (2) Choose a prior distribution over the states, $\pi_0(\rho)\mathrm d\rho$. (3) Multiply the prior by the likelihood and normalise to get the \emph{posterior distribution}.
	(4) The new candidate state is the mean of the posterior:
	\begin{equation}
		\hat\rho_{\rm BME}
		= \int \mathrm d\rho \,\rho \pi_f(\rho)
		= \int \mathrm d\rho \,\rho \calL(\rho)\pi_0(\rho).
	\end{equation}
	\item The author also includes, towards the end of the paper, a few interesting observations about the relations between MLE and BME methods, and Aaronson's PAC and Gross' compressed tomography approaches.
\end{itemize}

\textcite{mahler2013adaptive} present a quantum state tomography scheme with expected infidelity scaling $O(1/N)$, with $N$ the number of samples required to estimate the true state within a target accuracy.
\begin{itemize}
	\item They observe that, if one measures in the basis in which $\rho$ is diagonal, then infidelity scales as $O(1/N)$.
	Their idea is therefore to perform a preliminary standard tomography procedure to estimate the state, and therefore its diagonal basis, and then follow this with the known-basis tomography schemes.
\end{itemize}

\parencite{aaronson2007learnability,flammia2012quantum}


\section{Entropy and stuff}
Holevo's theorem~(\href{Watrous notes, chapter 12}{https://cs.uwaterloo.ca/~watrous/LectureNotes/CS766.Fall2011/12.pdf}).

\section{Purification of states}

\begin{defn}
	Let $\rho$ be a state acting on $\calH_A$.
	A state $\tilde\rho$ acting on $\calH_A\otimes\calH_B$ is said to be an \emph{extension} of $\rho$ if $\tr_B\tilde\rho=\rho$.
	A \emph{pure} state $\ket\psi\in\calH_A\otimes\calH_B$ is said to be a \emph{purification} of $\rho$ if $\tr_B\ketbra\psi=\rho$.
\end{defn}

\begin{prop}
	Let $\ket\psi$ be a purification of $\rho$: $\tr_1\ketbra\psi=\rho$. Then there are unitaries $U, V$ such that
	\begin{equation}
		\ket\psi=(U\otimes \sqrt\rho\,V)\sum_k \ket{kk}.
	\end{equation}
\end{prop}
\begin{proof}
	If $\ket\psi$ is a purification of $\rho$, then $\tr_1\ketbra\psi=\rho$.
	The components of the vector $\ket\psi$ can be expressed using two indices as $\psi_{ij}$. This means that we think of $\psi$ as a matrix, and thus apply the \ac{SVD} to it:
	\begin{equation}
		\psi_{ij} = \sum_k \lambda_k u_{ik} v_{jk},\quad
		\lambda_k\in\RR,
	\end{equation}
	where $u_{ik}, v_{jk}$ are elements of the unitary matrices $U$ and $V$, respectively. In vector notation, this reads
	\begin{equation}
		\ket\psi=\sum_k\lambda_k\ket{u_k}\otimes\ket{v_k}.
	\end{equation}
	Note that this is nothing but the Schmidt decomposition of $\ket\psi$.
	Imposing $\braket\psi=1$ gives $\sum_k \lambda_k=1$.
	The condition of the partial trace gives
	\begin{equation}
		\sum_k \lambda_k^2 \ketbra{v_k}=\rho
		\Longrightarrow \rho\ket{v_k}=\lambda_k^2\ket{v_k}.
	\end{equation}
	The result is then obtained by noticing that $U\ket k=\ket{u_k}$ and $V\ket k=\ket{v_k}$. Note how this also shows us that while the choice of $U$ does not matter, $V$ has to be the unitary sending $\ket k$ into the eigenbase of $\rho$.
\end{proof}

\begin{prop}
	Let $\rho$ be a state. Then, $\ket\psi$ is a purification of $\rho$ if and only if
	$\ket\psi=\sum_k\sqrt{p_k}\ket{u_k}\otimes\ket{v_k}$,
	for some orthonormal basis $\{\ket{u_k}\}_k$, and some eigenbasis $\{\ket{v_k}\}_k$ of $\rho$ with $\rho\ket{v_k}=p_k\ket{v_k}$.
\end{prop}
Note that in the above the choice of $\ket{v_k}$ is unique if and only if $\rho$ is nondegenerate.

\section{Fidelity}

\begin{defn}\label{def:fidelity}
	The \emph{fidelity} between two states $\rho$ and $\sigma$ is the quantity:
	\begin{equation}
		F(\rho,\sigma) \equiv \tr(\sqrt{\rho^{1/2}\sigma\rho^{1/2}}).
	\end{equation}
\end{defn}

\begin{prop}\label{def:fidelity_with_abs}
	\Cref{def:fidelity} is equivalent to
	\begin{equation}
	F(\rho,\sigma)=\tr\lvert\sqrt\rho\sqrt\sigma\rvert
	\equiv \|\sqrt\rho\sqrt\sigma\|_1,	
	\end{equation}
	where $|A|\equiv\sqrt{A^\dagger A}$ and $\|\cdot\|_1$ denotes the trace one-norm.
\end{prop}
\begin{proof}
	$(\sqrt\rho\sqrt\sigma)^\dagger (\sqrt\rho\sqrt\sigma) = \sqrt\sigma\rho\sqrt\sigma$.
\end{proof}

\begin{prop}
	The fidelity is \emph{symmetric}: $F(\rho,\sigma)=F(\sigma,\rho)$.
\end{prop}
\begin{proof}
	This is more easily shown from~\cref{def:fidelity_with_abs}, and follows from the more general fact that for any matrix $A$, $\tr|A^\dagger|=\tr|A|$.
	To see this, consider the \ac{SVD} of $A$:
	\begin{equation}
		A = \sum_k |\lambda_k| \ketbra{v_k}{w_k},
		\label{eq:svd_decomposition_for_A}
	\end{equation}
	with $|\lambda_k|^2\in\RR$ the eigenvalues of $A^\dagger A$, and $\{\ket{v_k}\}_k,\{\ket{w_k}\}_k$ the eigenbases of $AA^\dagger$ and $A^\dagger A$, respectively.
	Then, $|A|\equiv\sqrt{A^\dagger A}$ is written as
	\begin{equation}
		|A| = \sum_k |\lambda_k| \ketbra{w_k}{w_k},
		\label{eq:svd_decomposition_with_wk}
	\end{equation}
	so that the trace gives $\tr|A| = \sum_k |\lambda_k|$.
	Note that if we had defined $|A|$ as $|A|=\sqrt{AA^\dagger}$, we would have had $\ket{v_k}$ instead of $\ket{w_k}$ in~\cref{eq:svd_decomposition_with_wk}, but taking the trace we would have arrived to the same final result.
	The expression of $A^\dagger$ is equal to~\cref{eq:svd_decomposition_for_A}, except for $v_k$ and $w_k$ being switched. Using similar reasoning as above, we thus conclude that $\tr|A^\dagger|=\tr|A|$.
\end{proof}

\begin{prop}
	If $\rho$ is pure, with $\rho=\ketbra\psi$, then
	$\calF(\rho,\sigma)=\mel{\psi}{\sigma}{\psi}^{1/2}$.
\end{prop}

\begin{prop}
	If both $\rho=\ketbra{\psi_\rho}$ and $\sigma=\ketbra{\psi_\sigma}$ are pure, then
	$\calF(\rho,\sigma)=\lvert\braket{\psi_\rho}{\psi_\sigma}\rvert$.
\end{prop}

\begin{prop}
	If $[\rho,\sigma]=0$, then $\rho$ and $\sigma$ share an eigenbasis. Denoting with $p_k, q_k$ their eigenvalues, we have $\calF(\rho,\sigma)=\sum_k\sqrt{p_k q_k}$.
	In other words, for commuting states, the fidelity is equal to the \emph{Bhattacharyya distance} between the corresponding probability distribution.
\end{prop}

\begin{thm}[Uhlmann's theorem]
	Let us denote with $\ket{\psi_\rho}$ a \emph{purification} of the state $\rho$. Then,
	\begin{equation}
		\calF(\rho,\sigma)=\max_{\ket{\psi_\rho},\ket{\psi_\sigma}}\lvert\braket{\psi_\rho}{\psi_\sigma}\rvert.
	\end{equation}
\end{thm}

A few propositions that will turn useful later on:
\begin{prop}
	Let $P,Q\in\Pos(\calX)$ be positive semidefinite operators.
	Then $F(P,Q)^2 \le \Tr(P)\Tr(Q)$, with equality \emph{iff} $P$ and $Q$ are linearly dependent.
\end{prop}
\begin{proof}
	\highlight{Watrous}
\end{proof}

\begin{prop}
	Let $P,Q\in\Pos(\calX)$ be positive semidefinite operators, and $V\in\rmU(\calX,\calY)$ an isometry into some larger space $\calY$ with $\dim(\calY)\ge\dim(\calX)$. Then
	\begin{equation}
		F(P,Q) = F(VPV^\dagger, VQV^\dagger).
	\end{equation}
\end{prop}
\begin{proof}
	\highlight{Watrous}
\end{proof}

\begin{prop}
	Let $P\in\Pos(\calX)$ and $v\in\calX$. Then
	$F(P,vv^\dagger) = \sqrt{v^\dagger  P v}$.
\end{prop}
\begin{proof}
	\highlight{Watrous}
\end{proof}

\begin{prop}
	Let $P,Q\in\Pos(\calX)$. Then
	$F(P,QPQ) = \langle P,Q\rangle$.
\end{prop}
\begin{proof}
	Here $\langle P,Q\rangle=\Tr(PQ)$. See \highlight{Watrous}.
\end{proof}


\begin{prop}
	Let $\{E_k\}_k$ be a POVM. The fidelity can be written as
	\begin{equation}
		F(\rho,\sigma)=\min_{\{E_k\}_k} F(\bs p,\bs q),
	\end{equation}
	where the minimum is taken over all possible POVMs, and $\bs p,\bs q$ are the probability distributions given by $\rho,\sigma$ for a given POVM:
	\begin{equation}
		p_k=\tr(E_k \rho),\quad q_k=\tr(E_k \sigma).
	\end{equation}
\end{prop}


\section{Bipartite states}

\begin{defn}[Separable vectors]
	Let $\calX,\calY$ be complex vector spaces.
	A vector $u\in\calX\otimes\calY$ is \emph{separable} if there are $u_1\in\calX$ and $u_2\in\calY$ such that $u=u_1\otimes u_2$.
	If a vector $u\in\calX\otimes \calY$ is \emph{not} separable, then it can be written, via SVD decomposition, as a sum of the form
	\begin{equation}
		u = \sum_a u_1(a) \otimes u_2(a),
	\end{equation}
	for some collections of orthogonal vectors $\{u_1(a)\}_a\subset\calX$ and $\{u_2(a)\}_a\subset\calY$.
	% We denote the set of such vectors with $\on{Sep}(\calX:\calY)$.

	The minimal number of elements required to decompose $u$ in such way is its \emph{Schmidt rank}.
	This coincides with the rank of the matrix corresponding to $\on{unvec}(u)\in\Lin(\calY,\calX)$.
\end{defn}

Consider an arbitrary vector $\psi\in\calX\otimes\calY$ with Schmidt decomposition
$\psi=x_a\otimes y_a$. We can write this as
\begin{equation}
	\psi = (X\otimes Y) e_+ = \on{vec}(XY^T),
\end{equation}
where $e_+\equiv\sum_a e_a\otimes e_a$ is the maximally entangled vector in some auxiliary space $\calZ$ with $\dim(\calZ)=\rank(\psi)$, and the matrices $X\in\Lin(\calZ,\calX)$ and $Y\in\Lin(\calZ,\calY)$ are defined by
$X e_a=x_a$ and $Y e_a=y_a$.

\begin{defn}
	A \emph{positive semidefinite operator} $R\in\Pos(\calX\otimes\calY)$ is said to be \emph{separable} if there are collections of positive semidefinite operators
	\begin{equation}
		\{P_a : a \in\Sigma\} \subset\Pos(\calX),
		\quad
		\{Q_a : a \in\Sigma\} \subset\Pos(\calY),
	\end{equation}
	such that
	\begin{equation}
		R = \sum_{a\in\Sigma} P_a \otimes Q_a.
	\end{equation}
	Denote the set of such operators with $\on{Sep}(\calX:\calY)$.
	The corresponding subset of states is denoted with $\on{SepD}(\calX:\calY)$:
	\begin{equation}
		\on{SepD}(\calX:\calY) \equiv \on{Sep}(\calX:\calY)\cap \rmD(\calX\otimes\calY).
	\end{equation}
\end{defn}

\begin{prop}
	For any choice of $\calX,\calY$, the set
	$\on{SepD}(\calX:\calY)$ is convex, and the set
	$\on{Sep}(\calX:\calY)$ is a convex cone.
\end{prop}

\begin{prop}
	Let $\calA\subset\Pos(\calZ)$ be a cone in some space $\calZ$. Then $\calA$ is generated by the states it contains. In other words:
	\begin{equation}
		\calA = \on{cone}(\calA\cap \rmD(\calZ)).
	\end{equation}
\end{prop}
We should also note that any cone in $\Pos(\calZ)$ contains some states: we can always find a rescaling of any $P\in\Pos(\calZ)$ such that $\alpha P\in\rmD(\calZ)$.

\begin{prop}
	Any $S\in\on{Sep}(\calX:\calY)$ can be written as
	\begin{equation}
		S = \sum_a x_a x_a^\dagger\otimes y_a y_a^\dagger
	\end{equation}
	for some collections $\{x_a\}_a\subset\calX$ and $\{y_a\}_a\subset\calY$.
\end{prop}

It follows that for a generic $\psi\in\calX\otimes\calY$ with SVD decomposition
$\psi=\sum_a u_a\otimes v_a$ we have a decomposition for $\psi\psi^\dagger\in\Pos(\calX\otimes\calY)$ of the form
\begin{equation}
	\psi\psi^\dagger = \sum_{a,b} u_a u_b^\dagger \otimes v_a v_b^\dagger.
\end{equation}
If $P\in\Pos(\calX\otimes\calY)$ is a generic positive operator, it admits a decomposition of the form $P=\sum_a \psi_a\psi_a^\dagger$ with $\{\psi_a\}_a$ orthogonal.
Thus, writing $\psi_a=\sum_b u_{ab}\otimes v_{ab}$, we have
\begin{equation}
	P = \sum_{abc} u_{ab}u_{ac}^\dagger \otimes v_{ab}v_{ac}^\dagger.
\end{equation}
In matrix notation, we have $P=\Psi\Psi^\dagger$, where
$\Psi\in\Lin(\calZ,\calX\otimes\calY)$ for some $\calZ$ with $\dim(\calZ)=\rank(P)$,
and the columns of $\Psi$ are the vectors $\psi_a$:
$\Psi e_a=\psi_a$.
Moreover, each $\psi_a$ can be written as $\psi_a=\on{vec}(U_a V_a^T)\equiv (U_a \otimes V_a)\sum_i \ket{i,i}$, for some matrices $U_a,V_a$ with orthogonal columns.
Thus,
\begin{equation}
	P = \sum_a (U_a \otimes V_a) \PP_m (U_a\otimes V_a)^\dagger.
\end{equation}
\highlight{(so what??)}

\begin{prop}
	Let $\xi\in\on{SepD}(\calX:\calY)$ be a separable state. Then
	\begin{equation}
		\xi = \sum_{a\in\Sigma} p(a) \,x_a x_a^\dagger\otimes y_a y_a^\dagger,
	\end{equation}
	for some collections of unit vectors
	$\{x_a: a\in\Sigma\}\subset\calX$
	and
	$\{y_a: a\in\Sigma\}\subset\calY$,
	some register $\Sigma$ with $|\Sigma|\le \rank(\xi)^2$,
	and some probability vector $p\in\calP(\Sigma)$.
\end{prop}
\begin{proof}
	(\emph{Sketch}) The rank of $\xi$ gives the ``effective dimension'' of the underlying space. Its square is then the dimension of the space of Hermitians in this space.
	Moreover, all separable states are in the convex hull of product states, thus conclusion follows from Caratheodory.
\end{proof}

\highlight{(can we dispose of the normalisation condition in this proposition? Maybe just make it about $\on{Sep}(\calX:\calY)$?)}

\section{Entanglement}

\begin{prop}
	Let $R\in\Pos(\calX\otimes\calY)$. Then
	$R\in\on{Sep}(\calX:\calY)$ \emph{iff}
	\begin{equation}
		(\Phi\otimes I_{\Lin(\calY)})(R) \in \Pos(\calZ\otimes\calY),
	\end{equation}
	for any space $\calZ$ and positive map $\Phi\in\rmT(\calX,\calZ)$.
\end{prop}

This can be specialised on positive unital maps $\Phi\in\rmT(\calX,\calY)$. \highlight{(Proof on Watrous, but to understand)}

\begin{example}
	Consider the Swap operator, $W\in\Lin(\calX,\calY)$ with $\dim(\calX)=\dim(\calY)$ such that
	\begin{equation}
		W \equiv \sum_{a,b\in\Sigma} E_{a,b} \otimes E_{b,a}.
	\end{equation}
	This is the operator acting as $W(a\otimes b)=b\otimes a$.
	It is unitary and Hermitian with eigenvalues $\pm1$.
	(??????)
\end{example}

\begin{example}
	Consider the pure maximally entangled state:
	\begin{equation}
		\PP_{\rm ME} \equiv \frac{1}{n} \sum_{a,b\in\Sigma} E_{a,b} \otimes E_{a,b}
		\equiv \frac{1}{n} \sum_{a,b\in\Sigma} \ketbra{a,a}{b,b}.
	\end{equation}
	Observe that
	\begin{equation}
		(T\otimes I_{\Lin(\calY)}) \PP_{\rm ME} = \frac{1}{n} W,
	\end{equation}
	where $W$ is the Swap operation. The swap is not positive, being its eigenvalues $\pm1$, which proves that $\PP_{\rm ME}$ is not separable.
\end{example}

\begin{example}
	Isotropic states are states of the form
	\begin{equation}
		\lambda \PP_{\rm ME} + (1-\lambda) \frac{I - \PP_{\rm ME}}{n^2-1},
	\end{equation}
	where
	\begin{equation}
		\PP_{\rm ME} \equiv \frac{1}{n} \sum_{a,b\in\Sigma} E_{a,b} \otimes E_{a,b}
		\equiv \frac{1}{n} \sum_{a,b\in\Sigma} \ketbra{a,a}{b,b}.
	\end{equation}
\end{example}


\subsection{Necessary and sufficient characterizations of entanglement (??)}
\Cref{def:positive_maps} states that if $\Lambda$ is \ac{CP},
then for any state $\rho$ we have $(\Lambda \otimes \mathds1)\rho \ge 0$.
On the other hand, if $\Lambda$ is a positive, non-\ac{CP} operator, there may be states $\rho$ such that
$(\Lambda \otimes \mathds1)\rho < 0$.
But if $\rho$ is a product state, $\rho = \rho_1 \otimes \rho_2$, then
$(\Lambda \otimes \mathds1)\rho = (\Lambda \rho_1) \otimes \rho_2$ is always positive,
even if $\Lambda$ is not \ac{CP}.
This simple observation suggest that whether a state remains positive when a non-\ac{CP} map acts on it may tell us something about its entanglement structure.

A first important result is that any entangled state can be linearly separated from the set of all separable states.
This follows from basic theorems in convex analysis.
The proof invokes the existence of a separating hyperplane between the compact convex set of separable density matrices on $\mathcal H_1 \otimes \mathcal H_2$ and a point,
the entangled density matrix $\rho$,
that does not belong to it.
This separating hyperplane can be characterized by a vector (in this case, an operator $A$),
that is normal to it.
The hyperplane is then defined as the set of density matrices $\sigma$ such that
$\Tr(A\rho) = 0$.
\begin{thm}[\emph{\cite{horodecki1996separability}}]
	For any \uline{inseparable} state $\rho \in \mathcal A_1 \otimes \mathcal A_2$,
	there exists an Hermitian operator $A$ such that
	\begin{equation}
		\Tr(A\rho) < 0
		\quad \text{and} \quad
		\Tr(A\sigma) \ge 0
		\,\, \text{ for any separable }\sigma.
	\end{equation}
\end{thm}

\begin{remark}
	The choice of ``origin'' in the above theorem is clearly irrelevant.
	We could have stated the result with $\Tr(A\rho) < c$ and
	$\Tr(A\sigma) \ge c$ for any $c \in \mathbb R$.
\end{remark}

\begin{lemma}[\emph{\cite{horodecki1996separability}}]
	A state $\rho \in \mathcal A_1 \otimes \mathcal A_2$ is \uline{separable} iff
	$\Tr(A\rho) \ge 0$
	for any Hermitian operator $A$ such that
	$\Tr(A P\otimes Q) \ge 0$, where $P$ and $Q$ are projections acting on
	$\mathcal H_1$ and $\mathcal H_2$, respectively.
	\label{lemma:horodecki96_separable_iff_TrArhoPositive}
\end{lemma}

\begin{remark}
	If an operator $A$ satisfies the condition $\Tr\big( A(P \otimes Q) \big) \ge 0$
	for any projectors $P$ and $Q$ then it is Hermitian.
\end{remark}

\begin{remark}
	\Cref{lemma:horodecki96_separable_iff_TrArhoPositive} can be generilized for infinitely dimensional Hilbert spaces.
	Namely, the condition becomes $\Tr(A\rho) \ge 0$ for any bounded $A$ such that
	$\Tr\big( A(P\otimes Q) \big) \ge 0$ for any projectors $P$ and $Q$.
\end{remark}

\begin{thm}[\emph{\cite{horodecki1996separability}}]
	Let $\rho$ be a state in the Hilbert space $\mathcal H_1 \otimes \mathcal H_2$.
	Then $\rho$ is separable iff for any positive map
	$\Lambda : \mathcal A_2 \to \mathcal A_1$,
	the operator $(\mathds1 \otimes \Lambda)\rho$ is positive.
\end{thm}

In the special case of bipartite states of 2 qubits, or 1 qubit and 1 qutrit,
the partial transposition criterion completely characterizes entanglement:
\begin{thm}[\emph{\cite{horodecki1996separability}}]
	A state $\rho$ acting on $\mathbb C^2 \otimes \mathbb C^2$
	or $\mathbb C^2 \otimes \mathbb C^3$ is separable iff
	its partial transposition is a positive operator: $\rho^{T_1} \ge 0$.
\end{thm}


\section{General structure of multipartite entanglement}

Higher dimensional entanglement without correlations~\autocite{klobus2018higher}.
\begin{lemma}[\emph{\cite{dr2000three}}]
	Let $\ket\psi, \ket\phi \in \mathbb{C}^n \otimes \mathbb{C}^m$ be 2 bipartite vectors, and suppose that for some local operator $A$ we have
	$\ket\phi = A\otimes \mathds1_B \ket\psi$.
	Then, the ranks of the corresponding reduced density matrices satisfy
	$r(\rho_A^\psi) \ge r(\rho_A^\phi)$ and
	$r(\rho_B^\psi) \ge r(\rho_B^\phi)$.
\end{lemma}
\begin{proof}
	Consider the Schmidt decomposition of $\ket\psi$:
	\begin{equation}
		\ket\psi = \sum_{1=1}^{n_\psi} \sqrt{\lambda_i^\psi} \ket{i} \ket{i},
		\quad \text{with } \lambda_i^\psi > 0
		\quad\text{ and }\quad n_\psi \le \min(n,m),
	\end{equation}
	and write the operator $A$  as
	$A = \sum_{i=1}^n \ketbra{\mu_i}{i}$,
	where the vectors $\ket{\mu_i} \in \mathbb{C}^n$ do not need to be normalized nor linearly independent.
	The reduced density matrices can therefore be written as
	\begin{equation}
		\rho_A^\psi = 
		\sum_{i=1}^{n_\psi} \lambda_i^\psi \ketbra{i}{i},
		\qquad
		\rho_A^\phi = 
		\sum_{i=1}^{n_\psi} \lambda_i^\psi \ketbra{\mu_i}{\mu_i}.
	\end{equation}
	If follows that $r(\rho_A^\phi) \le r(\rho_A^\psi)$, because $\rho_A^\phi$ can be written as a convex sum of \emph{at most} $n_\psi$ terms,
	and $n_\psi$ is the rank of $\rho_A^\psi$.
	Moreover, for an bipartite vector $r(\rho_A) = r(\rho_B)$, so that the second inequality of the lemma follows.
\end{proof}
Using the above lemma, we can prove the more general statement:
\begin{lemma}[\cite{dr2000three}]
	If two vectors $\ket\psi, \ket\phi \in \calH_1 \otimes \cdots \otimes \calH_N$
	are connected by local operators as
	$\ket\phi = A_1 \otimes \cdots \otimes A_N \ket\psi$, then the local ranks satisfy
	$r(\rho^\psi_k) \ge r(\rho^\phi_k)$ for all $k=1,...,N$.
\end{lemma}
\begin{thm}
	Two pure states of a multipartite system are equivalent under \ac{SLOCC}
	if they are related by a local invertible operator.
\end{thm}

\section{Entanglement criteria}
\subsection{Entanglement witnesses}

\begin{defn}
	Let $\calX,\calY$ be finite-dimensional complex vector spaces.
	An \emph{entanglement witness} is an operator $W\in\Herm(\calX\otimes\calY)$ such that
	\begin{enumerate}
		\item $\Tr(W X_s)\ge0$ for all \emph{separable} $ X_s\in\on{Sep}(\calX:\calY)$;
		\item $\Tr(W X_e)<0$ for some \emph{entangled} $X_e\in\Pos(\calX\otimes\calY)\setminus\on{Sep}(\calX:\calY)$.
	\end{enumerate}
\end{defn}
More generally, we can think of $W$ as any observable that marks a boundary of the convex set of separable posdef operators $\on{Sep}(\calX:\calY)$.
The choice of $0$ as boundary, like the choice of entangled states being certified by \emph{negative}, rather than positive, expectation value, is of course completely conventional.

We remark that whether one requires $W$ to have positive or negative expectation value on \emph{states}, rather than just positive semidefinite operators, does not affect the definition.
However, if the threshold is chosen to be some number other than $0$, than one should take care of defining the threshold only with respect to states.

The geometrical interpretation of witnesses is obtained noticing that $\Tr(WX)=\langle W,X\rangle$ for any Hermitian $H$. Therefore, any such $W$ corresponds to a \emph{direction} in the space of states.

A common choice of witness operator is one of the form
\begin{equation}
	W_\psi = \alpha_\psi I - \psi\psi^\dagger,
\end{equation}
for some entangled $\psi\in\calS(\calX\otimes\calY)$, with $\alpha_\psi$ the maximum squared overlap between $\ket\psi$ and any separable state.
The set of separable states is the convex hull of the set of pure product states, and therefore we can write
\begin{equation}
	\alpha_\psi = \max_{u\in\calS(\calX),v\in\calS(\calY)}
	\lvert \langle u\otimes v, \psi\rangle\rvert^2.
\end{equation}
This maximum can be expressed in terms of the Schmidt decomposition of $\psi$. More precisely, $\alpha_\psi$ equals the largest squared Schmidt coefficient of $\psi$, that is,
$\alpha_\psi = \|\on{unvec}(\psi)\|_{\rm op}^2$, where $\on{unvec}(\psi)\in\Lin(\calY,\calX)$ is the linear operator obtained \emph{unvectorising} $\psi$.
In~\parencite{guhne2020geometry} these types of witnesses are called \emph{fidelity-based entanglement witnesses} \highlight{(can't \emph{any} witness be written in this form for some $\psi$?)}.

\begin{example}
	Let $\psi\in\calS(\calX\otimes\calX)$ be a maximally entangled state, with $\dim(\calX)=d$. Then the corresponding witness operator is
	\begin{equation}
		W_\psi = \frac{I}{d} - \psi\psi^\dagger.
	\end{equation}
\end{example}



\begin{defn}[Noise tolerance, \cite{tth2009practical}]
	Let $\mathcal W$ be a witness detecting the pure quantum state $\rho$.
	In a real experiment the realised state will be mixed with some noise, so that the actual state will be of the form
	\begin{equation}
		\rho_{\text{noisy}}(p) = (1-p) \rho + p \rho_{\text{noise}},
	\end{equation}
	for some modeling of the noise encoded in $\rho_{\text{noise}}$.
	The \emph{noise tolerance} of the witness $\mathcal W$ is then defined as the largest $p$ such that we still have $\Tr{\mathcal W \rho_{\text{noisy}}}$.
\end{defn}


\begin{thm}[\cite{tth2005detection}]
	For biseparable quantum states $\rho$ of $N$ qubits,
	denoting with $D_N^{(N/2)}$ the $N$-qubit Dicke state with $N/2$ excitations,
	the following inequality holds:
	\begin{equation}
		\Tr(\rho \ketbra*{D_{N/2,N}}{D_{N/2,N}})
		\le C_{N/2, N},
		\quad\text{with}\quad
		C_{N/2,N} \equiv \frac{1}{2}\frac{N}{N-1}.
	\end{equation}
	If follows that a projective witness for $\ket{D_{N/2,N}}$ is
	\begin{equation}
		\mathcal W = \frac{1}{2} \frac{N}{N - 1} \mathds 1 -
			\ketbra*{D_{N/2,N}}{D_{N/2,N}}.
	\end{equation}
\end{thm}

\begin{thm}[\cite{hffner2005scalable}]
	A projective witness for $\ket{D_{1, N}}$ is
	\begin{equation}
		\mathcal W = \frac{1}{2} \frac{N}{N - 1} \mathds 1 -
			\ketbra*{D_{N/2,N}}{D_{N/2,N}}.
	\end{equation}
\end{thm}

\begin{thm}[\cite{bergmann2013entanglement}]
	A projective witness for $\ket{D_{k, N}}$, $1 < k < N/2$, is given by
	\begin{equation}
		\mathcal W = \frac{N - k}{N} \mathds 1 - \ketbra*{D_{k,N}}{D_{k,N}}.
	\end{equation}
\end{thm}

\subsection{Schmidt number}
For a bipartite pure state that we write in its Schmidt decomposition:
$\ket\psi = \sum_{i=1}^k \sqrt{\lambda_k} \ket{a_i} \otimes \ket{b_i}$,
the number $k$ is the \emph{Schmidt rank} of the pure state.
This number is equal to the tank of the reduced density matrix (regardless of which space is traced).
A necessary condition for a pure state to be convertible by \ac{LOCC} to another pure state is that the Schmidt rank of the first pure state is larger than or equal to the Schmidt rank of the second pure state.
A generalization of the Schmidt rank to density matrices is the so-called \emph{Schmidt number}~\parencite{terhal2000schmidt}:
\begin{defn}[Schmidt number, \cite{terhal2000schmidt}]
	Let $\rho$ be a bipartite state.
	We say that $\rho$ has Schmidt number $k$ if
	i) for any decomposition of $\rho$, $\{p_i \ge 0, \ket{\psi_i}\}$ with $\rho = \sum_i p_i \ketbra{\psi}$, at least one of the vectors $\ket{\psi_i}$ has Schmidt rank at least $k$,
	and ii) there exists a decomposition of $\rho$ with all vectors $\ket{\psi_i}$ having Schmidt rank \emph{at most} $k$.
\end{defn}
For a pure state, the Schmidt number is simply equal to the Schmidt rank.
This immediately follows from remembering that a pure state $\rho$ only admits a single decomposition, of the form $\rho=\ketbra{\psi}$, and this decomposition clearly satisfies both (i) and (ii) with Schmidt rank $k$.
\begin{lemma}
	The set of separable density matrices $S_1$ is equal to the set of density matrices with Schmidt number equal to 1.
	In other words, the separable states are all and only the states with unitary Schmidt number.
\end{lemma}

\begin{thm}[\cite{terhal2000schmidt}]
	The Schmidt number of a density matrix cannot increase under \ac{LOCC}.
\end{thm}

\begin{defn}[$k$-positive maps, \cite{terhal2000schmidt}]
	Let $S_k$ denote the set of density matrices on $\calH_n\otimes\calH_n$ that have Schmidt number $k$ or less.
	The set $S_k$ is a convex compact subset of the set of density matrices, denotes by $S$,
	and $S_{k-1} \subset S_k$.
	A linear Hermiticity-preserving map $\Lambda$ is $k$-positive if and only if
	$(\mathds1 \otimes \Lambda)\ketbra\psi \ge 0$ for all $\ketbra\psi \in S_k$.
\end{defn}

\begin{thm}[\cite{terhal2000schmidt}]
	A density matrix $\rho$ has Schmidt number at least $k+1$ if and only if there exists a $k$-positive linear map $\Lambda_k: \mathcal M_n(\mathbb C)\rightarrow \mathcal M_n(\mathbb C)$ such that 
	$(\mathds1\otimes \Lambda_k)\rho < 0$.
\end{thm}

\chapter{Channels}
\minitoc

% Let $\rho\in\rmD(\calX)$ be a state.
A \emph{map} acting on operators $X\in\Lin(\calX)$ is an operator $\Phi\in\Lin(\Lin(\calX),\Lin(\calY))$. We denote the set of such maps with $\rmT(\calX,\calY)$.
When we wish to restrict our discussions to finite-dimensional spaces, or specify their dimensions, we will assume $\calX,\calY$ to be spaces of the form $\CC^n$ for some $n\in\NN$.

A given $\Phi\in\rmT(\calX,\calY)$ is said to be
\begin{itemize}
	\item \emph{unital} if $\Phi(I_{\calX})=I_{\calY}$, where $I_{\calX},I_{\calY}$ are the identities in $\calX,\calY$, respectively;
	\item \emph{trace-preserving} if $\Tr(\Phi(X))=\Tr(X)$;
	\item \emph{Hermitian preserving} if $\Phi(X^\dagger)=\Phi(X)^\dagger$, for all $X\in\Lin(\calX)$;
	\item \emph{positive} if $\Phi(X)\ge0$ for all $X\ge0$;
	\item \emph{completely positive} if $(\Phi\otimes I_n)\ge0$ for any $n\ge0$.
\end{itemize}

A map is said to be CPTP if it is trace-preserving and completely positive. CPTP maps are also referred to as \emph{channels}. We denote the set of channels with $\rmC(\calX,\calY)$.


\begin{prop}\label{prop:characterisation_of_positive_maps_from_rank1ops}
	A map $\Phi\in\rmT(\calX,\calY)$ is positive \emph{iff} it preserves the positivity of rank-one operators with unit trace, that is, \emph{iff} $\Phi(uu^\dagger)\ge0$ for all $u\in\calS(\calX)$.
\end{prop}
\begin{proof}
	We want to prove that $\Phi(X)\ge0$ for all $X\in\Pos(\calX)$ \emph{iff} $\Phi(uu^\dagger)\ge0$ for all $u\in\calX$.
	One direction is immediate from $uu^\dagger\in\Pos(\calX)$.
	Let us then assume that $\Phi(uu^\dagger)\ge0$ for all $u\in\calS(\calX)$.
	Any positive $X\in\Pos(\calX)$ is, via its eigendecomposition, a linear combination with positive coefficients of such rank-one operators.
	Thus $\Phi(X)\ge0$ because a positive linear combination of positive operators is also a positive operator.
\end{proof}

\section{Operator-vector correspondence}

We have a correspondence between operators $\Lin(\calX,\calY)$ and vectors in $\calY\otimes\calX$, via the mapping
\begin{equation}
	\on{vec}:\Lin(\calX,\calY)\ni E_{ab} \mapsto e_a\otimes e_b\in\calY\otimes\calX.
\end{equation}

\begin{prop}
	Let $X\in\Lin(\calX,\calY)$. Then
	\begin{equation}
		\on{vec}(X) = (X\otimes I_{\calX}) e_+^\calX
		= (I_{\calY}\otimes X^T) e_+^\calY,
	\end{equation}
	where $e_+^\calX\equiv\sum_a e_a\otimes e_a\in\calX\otimes\calX$ is the maximally entangled vector in $\calX\otimes\calX$, and $e_+^\calY$ is similarly defined.
	To recover $X$ from $\on{vec}(X)$, we can write
	\begin{equation}
		\on{unvec}(v) = \sum_{ij} E_{ij} \langle i,j|v\rangle
	\end{equation}
	\highlight{(there should be a better way to express this without sums)}
\end{prop}

\begin{prop}
	For any $A\in\Lin(\calX,\calY)$, we have
	\begin{equation}
		\Tr_\calX[\on{vec}(A)\on{vec}(A)^\dagger] = AA^\dagger.
	\end{equation}
\end{prop}

\begin{example}
	$\on{vec}(yx^\dagger) = y\otimes \bar x$ for any $x\in\calX$ and $y\in\calY$.
\end{example}

\section{Complete positivity}

One interesting aspect about the positivity of extensions is that complete positivity of a given map $\Phi\in\rmT(\calX,\calY)$ is equivalent to whether $\Phi\otimes I_{\Lin(\calX)}$ preserves the positivity of the maximally entangled state $\calX\otimes\calX$.

\begin{prop}
	There is a linear bijection between $\rmT(\calX,\calY)$ and $\Lin(\calY\otimes\calX)$, via the mapping
	\begin{equation}
		\rmT(\calX,\calY)\ni \Phi\mapsto
		J(\Phi) \equiv (\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger)\in\Lin(\calY\otimes\calX).
	\end{equation}
\end{prop}
\begin{proof}
	Define $J(\Phi) \equiv (\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger)$.
	The linearity of the mapping $\Phi\mapsto J(\Phi)$ is clear.
	We thus only need to show that this mapping is injective, and thus an isomorphism.
	For the purpose, we observe that, for any $X\in\Lin(\calX)$,
	\begin{equation}
		\Phi(X) = \Tr_\calX [J(\Phi)(I_{\Lin(\calY)}\otimes X^T)].
	\end{equation}
	Therefore $J(\Phi)=0$ implies $\Phi=0$.
\end{proof}

More generally, we have for all $i,j,k,\ell$
\begin{equation}
	(\Phi\otimes I_{\Lin(\calX)})\ketbra{i,j}{k,\ell}
	= \Phi(E_{ik}) \otimes E_{j\ell}.
\end{equation}
For any separable vector $u\equiv u_1\otimes u_2\in\calX\otimes\calX$, we have
\begin{equation}
	(\Phi\otimes I_{\Lin(\calX)}) (uu^\dagger)
	= \Phi(u_1 u_1^\dagger) \otimes u_2 u_2^\dagger.
\end{equation}
Any vector $\psi\in\calX\otimes\calX$ can be decomposed as
$\psi=\sum_a u_a\otimes v_a$ for some orthogonal collections $\{u_a\}_a,\{v_a\}_a\subset\calX$.
Thus
\begin{equation}
	(\Phi\otimes I_{\Lin(\calX)})(\psi\psi^\dagger)
	= \sum_{a,b} \Phi(u_a u_b^\dagger)\otimes v_a v_b^\dagger.
\end{equation}
For any $X\in\Lin(\calX\otimes\calX)$, define
% \begin{equation}
	$J_X(\Phi)
		= (\Phi\otimes I_{\Lin(\calX)})(X)$.
% \end{equation}
Then
\begin{equation}
	\mel{i,j}{J_X(\Phi)}{k,\ell}
	= \sum_{ab} \langle i|\Phi(E_{ab})|k\rangle X_{aj,b\ell}
\end{equation}
\highlight{(when is this still a bijection?)}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$. Then $\Phi\in\rmC(\calX,\calY)$ \emph{iff}
	\begin{equation}
		(\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger)\ge0,
	\end{equation}
	where $e_+\in\calX\otimes\calX$ is the maximally entangled (unnormalised) state.
\end{prop}


\begin{proof}
	The standard argument involves the observations:
	\begin{itemize}
		\item There is a linear bijection between $\rmT(\calX,\calY)$ and $\Lin(\calY\otimes\calX)$, via the mapping
		\begin{equation}
			\rmT(\calX,\calY)\ni \Phi\mapsto (\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger)\in\Lin(\calY\otimes\calX).
		\end{equation}
	\end{itemize}
	 applying the spectral theorem for positive operators to
	$J(\Phi)\equiv (\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger$
\end{proof}

\begin{proof}
	If $\Phi$ is CPTP then obviously $(\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger)\ge0$ by definition of complete positivity.
	For the other direction, observe that
	\begin{equation}
		\langle i,j| (\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger) |k,\ell\rangle
		= \langle i | \Phi(E_{j\ell}) | k\rangle.
	\end{equation}
	Therefore, $(\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger)\ge0$ implies the existence of a set of orthogonal vectors $\{u(a)\}_a\subset\calY\otimes\calX$ such that
	\begin{equation}
		\langle i|\Phi(E_{j\ell})|k\rangle
		= \sum_a u(a)_{ij} \overline{u(a)}_{k\ell}.
	\end{equation}
	Moreover, for any $X\in\Lin(\calX\otimes\calX)$ and $\ket\alpha\in \calY\otimes\calX$ we have,
	\begin{equation}
		\langle \alpha| (\Phi\otimes I_{\Lin(\calX)})(X) |\alpha\rangle
		= \sum_{ijk\ell mn}
		\bar\alpha_{ij} \alpha_{k\ell}
		\langle i|\Phi(E_{mn})|k\rangle X_{mj,n\ell}.
	\end{equation}
	We want to prove that, if $X\in\Pos(\calX\otimes\calY)$, then this quantity is positive for all $\ket\alpha$.

	The positivity of $X$ implies that there is a set $\{v(a)\}_a\subset\calX\otimes\calX$ of orthogonal vectors such that
	$X = \sum_a v(a) v(a)^\dagger$, which componentwise reads
	\begin{equation}
		X_{ij,k\ell} = \sum_a v(a)_{ij} \overline{v(a)}_{k\ell}.
	\end{equation}
	Consequently we can write
	\begin{equation}
	\begin{gathered}
		\langle \alpha| (\Phi\otimes I_{\Lin(\calX)})(X) |\alpha\rangle
		= \sum_{ijk\ell mn a} \bar\alpha_{ij} \alpha_{k\ell}
		\langle i|\Phi(E_{mn})|k\rangle v(a)_{mj} \overline{v(a)}_{n\ell} \\
		= \sum_{ijk\ell mn ab}
		\bar\alpha_{ij} \alpha_{k\ell}
		u(b)_{im} \overline{u(b)}_{kn}
		v(a)_{mj} \overline{v(a)}_{n\ell}
		= \sum_{ab} |\beta_{ab}|^2 \ge 0,
	\end{gathered}
	\end{equation}
	where $\beta_{ab}\in\CC$ are defined as
	\begin{equation}
		\beta_{ab} \equiv \sum_{ijm}
		\bar\alpha_{ij} u(b)_{im} v(a)_{mj}
	\end{equation}
\end{proof}

\begin{proof}
	Let us try now a slightly different proof that is less reliant on explicit index decompositions.
	Observe that
	\begin{equation}
		(\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger)
		= \sum_{ij} \Phi(E_{ij}) \otimes E_{ij}.
	\end{equation}
	\highlight{(to finish)}
\end{proof}


\begin{prop}
	Let $\Phi\in\rmC(\calX,\calY)$. Then there are operators $\{A_a\}_a\subset\Lin(\calX,\calY)$ such that, for all $X\in\Lin(X)$, we have
	\begin{equation}
		\Phi(X) = \sum_a A_a X A_a^\dagger.
	\end{equation}

\end{prop}

Let $\Phi\in\rmC(\calX,\calY)$ and let $e_+\equiv \sqrt{\dim(\calX)}\ket +\in\calX\otimes\calX$ denote the maximally entangled state in $\calX\otimes\calX$.
Consider the operator $J(\Phi)\equiv (\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger)$.
This is an operator $J(\Phi)\in\Lin(\calY\otimes\calX)$ such that
\begin{equation}
	\langle i,j| (\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger) |k,\ell\rangle
	= \langle i | \Phi(E_{j\ell}) | k\rangle.
\end{equation}
Because $\Phi$ is CPTP, we know that $J(\Phi)\ge0$.
Therefore there are positive numbers $s_a\ge0$ and orthonormal vectors $u_a,v_a\in\calY\otimes\calX$ with $\langle u_a,u_b\rangle=\langle v_a,v_b\rangle = \delta_{ab}$ such that
\begin{equation}
	J(\Phi) = \sum_a s_a u_a v_a^\dagger.
\end{equation}
\highlight{(finish)}

Let $e_{ij}\in \calX\otimes\calX$ be such that $(e_{ij})_{k\ell}=\delta_{ik}\delta_{j\ell}$. Then for any $X\in\Lin(\calX\otimes\calX)$ we have
\begin{equation}
	e_{ij}^\dagger  [(\Phi\otimes I_{\Lin(\calX)})(X)] e_{ij}
	= \sum (\Phi\otimes I_{\Lin(\calX)})_{(ij)(ij),JK} X_{JK}
	% = \sum_{k,\ell} K(\Phi)_{ii,k\ell} X_{kj,\ell j}
\end{equation}

\section{Representations}

\subsection{Natural representation}

Operators $X\in\Lin(\calX)$ can be understood as vectors, being $\Lin(\calX)$ itself a vector space with $\dim(\Lin(\calX))=\dim(\calX)^2$. A standard way of doing this involves choosing a basis $\{\sigma_k\}_k\subset\Lin(\calX)$ and performing the decomposition $X=\sum_k c_k\sigma_k$. We can then identify $X$ with the corresponding vector of coefficients $\bs c\equiv (c_k)_k$.
An example of such a basis is $\{E_{ij}\}_{i,j=1}^n\subset\Lin(\calX)$, where $E_{ij}\equiv \ketbra{i}{j}$.
The corresponding coefficients of $X$ in such basis are simply its matrix elements. We denote such set of coefficients with $\on{vec}(X)\in\CC^{\dim(\calX)^2}$, which thus satisfies
% \begin{equation}
	$\on{vec}(X)_{ij} \equiv X_{ij} \equiv \mel{i}{X}{j}$.
% \end{equation}
More formally, if $X\in\Lin(\calX,\calY)$, then $\on{vec}(X)\in\calY\otimes\calX$.

\begin{example}
	Denote with $I_{\Lin(\calX)}$ the identity map in the operator space $\Lin(\calX)$ for some $\calX$. This is by definition the map such that $I_{\Lin(\calX)}(X)=X$ for all $X\in\Lin(\calX)$.
	Then
	\begin{equation}
		K(I_{\Lin(\calX)})_{ij,k\ell}
		= \delta_{ik}\delta_{j\ell}
	\end{equation}
\end{example}

A map $\Phi\in\rmT(\calX,\calY)$ can then be represented as the matrix sending $\on{vec}(X)$ to $\on{vec}(\Phi(X))$. We denote this operator with $K(\Phi)\in\Lin(\calY\otimes\calY,\calX\otimes\calX)$, and refer to $K(\Phi)$ as the \emph{natural representation} of $\Phi$.
We thus have
\begin{equation}
	K(\Phi)_{ij,k\ell}
	\equiv \mel{i,j}{K(\Phi)}{k,\ell}
	= \mel{i}{\Phi(E_{k\ell})}{j}.
\end{equation}
We can also get to this expression observing that, for any $X\in\Lin(\calX)$,
\begin{equation}
	\Phi(X)
	= \sum_{ij} \langle i|\Phi(X) |j\rangle E_{ij}
	= \sum_{ijk\ell} X_{k\ell}
	\underbrace{\langle i|\Phi(E_{k\ell})|j\rangle}_{\equiv K(\Phi)_{ij,k\ell}} E_{ij}.
\end{equation}

\begin{prop}
	A map $\Phi\in\rmT(\calX,\calY)$ is Hermitian-preserving \emph{iff}
	\begin{equation}
		K(\Phi)_{ij,k\ell} = \overline{K(\Phi)}_{ji,\ell k}.
	\end{equation}
\end{prop}

\begin{prop}
	Denote with $e_+\equiv \sum_i \ket{i,i}\in\calY\otimes\calY$ the unnormalised maximally entangled state in $\calY$.
	For any map $\Phi\in\rmT(\calX,\calY)$ we have
	\begin{equation}
		e_+^\dagger K(\Phi) = K(\Tr\circ\, \Phi) \in \Lin(\calX\otimes\calX,\CC)
	\end{equation}
\end{prop}

\paragraph{Natural representation and multipartite spaces}
We write the matrix elements of $X\in\Lin(\calX\otimes\calY)$ as
\begin{equation}
	X_{x_1 y_1,x_2 y_2}
	\equiv \langle x_1, y_1 | X | x_2, y_2\rangle
	= \Tr[X (E_{x_1 x_2} \otimes E_{y_1 y_2})].
\end{equation}
We thus denote on the LHS of the comma indices referring to the \emph{output space}, and on the RHS indices referring to the \emph{input space}. Here both are $\calX\otimes\calY$.

Consider a map acting on a bipartite system: $\Phi\in\rmT(\calX\otimes\calY,\calW\otimes\calZ)$. Then
\begin{equation}
	\Phi(X)_{w_1 z_1, w_2 z_2}
	= \sum_{x_1 x_2 y_1 y_2}
	K(\Phi)_{(w_1 z_1, w_2 z_2),(x_1 y_1, x_2 y_2)}
	X_{x_1 y_1,x_2 y_2}.
\end{equation}
We use parentheses on the indices of $K(\Phi)$ to distinguish between the two different ``classes'' of input and output spaces: the input space for $K(\Phi)$ is $(\calX\otimes\calY)^{\otimes 2}$, which can in turn be divided into ``input and output indices'', according to how the indices connect to the operator $X$.

In the general case of $\Phi\in\rmT(\bigotimes_i \calX_i,\bigotimes_i \calY_i)$ and $X\in\Lin(\bigotimes_i \calX_i)$, we write
\begin{equation}
	X_{I,J} \equiv X_{i_1\cdots i_n, j_1\cdots j_n},
\end{equation}
and thus
\begin{equation}
	\Phi(X)_{I,J}
	= \sum_{KL} K(\Phi)_{IJ,KL} X_{K,L}.
\end{equation}
Here, the index $L$ connects to the input space of the input operator space, $K$ to the output space of the input operator space, $J$ to the input space of the output operator space, and finally $I$ to the output space of the output operator space.
Each one of these indices is in turn a \emph{tuple} of indices marking the underlying multipartite structure of the space.

In particular, if $X\in\Lin(\calX), Y\in\Lin(\calY)$, we have
% \begin{equation}
	$(X\otimes Y)_{I,J} = X_{i_1,j_1} Y_{i_2,j_2}.$
% \end{equation}
More generally, if $X_i\in\Lin(\calX_i)$, we write
\begin{equation}
	\left(\bigotimes_k X_k\right)_{I,J}
	= \prod_k (X_k)_{I_k,J_k}.
\end{equation}

If $\Phi_i\in\rmT(\calX_i,\calY_i)$, and $X_i\in\Lin(\calX_i)$, then
\begin{equation}
	\left[
	\left(\bigotimes_i \Phi_i\right)\left(\bigotimes_i X_i\right)
	\right]_{IJ}
	= \prod_i \Phi_i(X_i)_{I_i,J_i}.
\end{equation}

If $\Phi_1\in\rmT(\calX,\calW)$ and $\Phi_2\in\rmT(\calY,\calZ)$,
then $\Phi_1\otimes\Phi_2\in\rmT(\calX\otimes\calY,\calW\otimes\calZ)$ and
\begin{equation}
	K(\Phi_1\otimes \Phi_2)_{IJ,KL}
	= K(\Phi_1)_{I_1 J_1, K_1 L_1}
	K(\Phi_2)_{I_2 J_2, K_2 L_2}.
\end{equation}
More generally, if $\Phi_i\in\rmT(\calX_i,\calY_i)$, we have
\begin{equation}
	K\big(\bigotimes_i \Phi\big)_{IJ,KL}
	= \prod_i K(\Phi_i)_{I_i J_i,K_i L_i}.
\end{equation}

\begin{example}
	Let $\Phi\in\rmT(\calX,\calY)$, and let
	$e_+\equiv \sqrt{\dim(\calX)}\ket+\in\calX\otimes\calX$
	be the unnormalised maximally entangled state. Then
	\begin{equation}
		K(\Phi\otimes I_{\Lin(\calX)})_{IJ,KL}
		= K(\Phi)_{I_1 J_1,K_1 L_1}
		\delta_{I_2 K_2} \delta_{J_2 L_2},
	\end{equation}
	and therefore, for any $X\in\Lin(\calX\otimes\calX)$,
	\begin{equation}
		[(\Phi\otimes I_{\Lin(\calX)})(X)]_{(ij),(k\ell)}
		= \sum_{mn} K(\Phi)_{ik,mn} X_{(mj),(n\ell)}.
	\end{equation}
	For $X=e_+ e_+^\dagger$ we thus get
	\begin{equation}
		[(\Phi\otimes I_{\Lin(\calX)})(e_+ e_+^\dagger)]_{(ij),(k\ell)}
		= \sum_{mn}
		K(\Phi)_{ik,mn} \delta_{mj} \delta_{n\ell}
		= K(\Phi)_{ik,j\ell}.
	\end{equation}
\end{example}

\begin{prop}
	Consider $\Phi\in\rmT(\calX,\calY)$ and $\Psi\in\rmT(\calY,\calZ)$.
	Then
	\begin{equation}
		K(\Psi\circ\Phi)
		= K(\Psi) K(\Phi).
	\end{equation}
\end{prop}
\begin{proof}
	Observe that
	\begin{equation}
		\Phi(E_{k\ell}) = \sum_{m,n} E_{mn} K(\Phi)_{mn,k\ell}.
	\end{equation}
	Thus
	\begin{equation}
		K(\Psi\circ\Phi)_{ij,k\ell}
		= \langle i| (\Psi\circ\Phi) (E_{k\ell}) | j\rangle
		= \sum_{m,n} K(\Psi)_{ij,mn} K(\Phi)_{mn,k\ell},
	\end{equation}
	that is, $K(\Psi\circ\Phi) = K(\Psi)K(\Phi)$.
\end{proof}

\subsection{Unitary representation}

\begin{prop}\label{prop:CPTP_unitary_representation}
	Let $\Phi\in\rmC(\calX,\calY)$. Then there are auxiliary spaces $\calZ,\calW$, some isometry $U\in\rmU(\calX\otimes\calZ,\calY\otimes\calW)$, and some pure state $u\in\calS(\calZ)$, such that for all $X\in\Lin(\calX)$ the channel can be represented as
	\begin{equation}
		\Phi(X) = \Tr_\calW[U(X\otimes uu^\dagger) U^\dagger ].
	\end{equation}
\end{prop}
\begin{proof}
	Prove it from the Kraus representation of the channel.
\end{proof}

\begin{prop}
	Given $U\in\rmU(\calX\otimes\calZ,\calY\otimes\calW)$ and $u\in\calZ$, consider the map $\Phi\in\rmT(\calX,\calY)$ sending each $X\in\Lin(\calX)$ to
	\begin{equation}
		\Phi(X) = \Tr_\calW[U(X\otimes uu^\dagger)U^\dagger].
	\end{equation}
	Then for all $\psi\in\calX$,
	\begin{equation}
		\Phi(\psi\psi^\dagger) = \Tr_\calW(\PP[U(\psi\otimes u)]).
	\end{equation}
	Moreover,
	\begin{equation}
		K(\Phi)_{ij,k\ell}
		\equiv \mel{i}{\Phi(E_{k\ell})}{j}
		= \sum_{amn}
		U_{ia,km} \bar U_{ja,\ell n} u_m \bar u_n
	\end{equation}
	\begin{equation}
		K(\Phi) =
		(I\otimes e_{+,\calW}^\dagger)
		(U\otimes \bar U)
		(I \otimes u \otimes \bar u).
	\end{equation}
	\highlight{(probably some swap to put here)}
	Thus its Choi reads
	\begin{equation}
		\mel{i,j}{J(\Phi)}{k,\ell}
		= 
	\end{equation}
\end{prop}

Any map that admits a unitary representation can be directly seen to be completely positive.

\begin{prop}
	Given $U\in\rmU(\calX\otimes\calZ,\calY\otimes\calW)$ and $u\in\calZ$, define the map $\Phi\in\rmT(\calX,\calY)$ as
	\begin{equation}
		\Phi(X) = \Tr_\calW[U(X\otimes uu^\dagger)U^\dagger],
	\end{equation}
	for all $X\in\Lin(\calX)$.
	Then $\Phi$ is completely positive.
	Moreover, if $\|u\|=1$, then $\Phi$ is trace-preserving, and thus $\Phi\in\rmC(\calX,\calY)$.
\end{prop}
\begin{proof}
	Given an arbitrary auxiliary space $\calQ$ and $X'\in\Lin(\calX\otimes\calQ)$, the action of
	$\Phi\otimes I_{\Lin(\calQ)}\in\rmT(\calX\otimes\calQ,\calY\otimes\calQ)$ reads
	\begin{equation}\label{eq:action_extended_phi_in_unitary_repr}
		(\Phi\otimes I_{\Lin(\calQ)})X'
		= \Tr_\calW[(U\otimes I_\calQ) S(X'\otimes uu^\dagger)S (U\otimes I_\calQ)^\dagger ],
	\end{equation}
	where $S$ is the \emph{swap operator} defined via its action
	$S\ket{a,b,c} = \ket{a,c,b}$.
	The swap operator is needed here for purely formal reasons, to take take of the fact that $X'$ acts on first and third space, and would thus be awkward to write down directly.
	From~\cref{eq:action_extended_phi_in_unitary_repr} we see that
	\begin{equation}
		(\Phi\otimes I_{\Lin(\calQ)})X' = \Tr_\calW[\calU (X'\otimes uu^\dagger)\calU^\dagger],
	\end{equation}
	for some isometry $\calU\in\rmU(\calX\otimes\calQ\otimes\calZ,\calY\otimes\calQ\otimes\calW)$.
	Thus $(\Phi\otimes I_{\Lin(\calQ)})$ is positive, \emph{i.e.} $\Phi$ is \emph{completely} positive.
\end{proof}

\begin{prop}
	Given $U\in\rmU(\calX\otimes\calZ,\calY\otimes\calW)$ and $u\in\calZ$, consider the map
	% \begin{equation}
		$\Phi(X) = \Tr_\calW[U(X\otimes uu^\dagger)U^\dagger].$
	% \end{equation}
	We can always write this as
	\begin{equation}
		\Phi(X) = \sum_{a} A_a X A_a^\dagger
	\end{equation}
	with $A_a\in\Lin(\calX,\calY)$ defined by
	% \begin{equation}
	$A_a = (I \otimes e_a^\dagger)U(I \otimes u)$,
	% \end{equation}
	with $\{e_a\}_a\subset\calW$ an orthonormal basis for $\calW$.
	For $\psi\in\calX$, this reads
	$\Phi(\psi\psi^\dagger) = \sum_a \PP[A_a \psi]$.
	Moreover, we have
	\begin{equation}
		\sum_a A_a^\dagger A_a = I_{\calX} \|u\|^2.
	\end{equation}
\end{prop}

\highlight{(what about the more general case of $\Phi\in\rmT(\calX,\calY)$?)}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$ be a map, and $\{A_a\}_a\subset\Lin(\calX,\calY)$ a collection of operators such that
	\begin{equation}
		\Phi(X) = \sum_a A_a X A_a^\dagger, \qquad \forall X\in\Lin(\calX).
	\end{equation}
	Then let $V\in\Lin(\calX,\calY\otimes\calW)$ be defined by
	% \begin{equation}
		$V \psi = \sum_a (A_a \psi \otimes e_a)$,
	% \end{equation}
	Then
	\begin{equation}
		\Phi(X) = \Tr_\calW[V X V^\dagger].
	\end{equation}
\end{prop}
\begin{proof}
	The matrix elements of $V$ read
	\begin{equation}
		(e_i^\dagger \otimes e_j^\dagger) V e_k = e_i^\dagger A_j e_k \equiv (A_j)_{i,k}.
	\end{equation}
	The adjoint of $V$ acts as
	\begin{equation}
		V^\dagger (Y\otimes e_a) = A_a^\dagger Y,
	\end{equation}
	and thus
	\begin{equation}
		V^\dagger V e_k =
		\sum_a A_a^\dagger A_a e_k
		\Longleftrightarrow V^\dagger V = \sum_a A_a^\dagger A_a.
	\end{equation}
	Therefore $V$ is an isometry \emph{iff} $\sum_a A_a^\dagger A_a = I_{\calX}$.
\end{proof}

The unitary representation also allows to write composition of channels in a natural way:
\begin{prop}
	Let $\Phi_i\in\rmT_{\rm CP}(\calX)$ be completely positive operators with unitary representations
	\begin{equation}
		\Phi_i(X) = \Tr_{\calW_i}[U_i(X\otimes u_i u_i^\dagger) U_i^\dagger],
	\end{equation}
	where $U\in\rmU(\calX\otimes\calZ_i,\calX\otimes\calW_i)$ and $u_i\in\calZ_i$.
	Then
	\begin{equation}
		\Phi_1 \circ \Phi_2 (X)
		= \Tr_{\calW1\otimes\calW_2}[
		\tilde U_1 \tilde U_2
		(X\otimes u_1 u_1^\dagger \otimes u_2 u_2^\dagger)
		\tilde U_2^\dagger \tilde U_1^\dagger
		].
	\end{equation}
\end{prop}

What does the adjoint map look like in the unitary representation?

\begin{prop}
	Let $\Phi\in\rmC(\calX,\calY)$, and let $\Phi^\dagger\in\rmT(\calY,\calX)$ denote its adjoint map.
	Suppose $\Phi$ has a unitary representation as in~\cref{prop:CPTP_unitary_representation}.
	Then
	\begin{equation}
		\Phi^\dagger(Y) =
		(I_{\calX}\otimes u^\dagger)
		[U^\dagger (Y\otimes I_{\calW}) U]
		(I_{\calX}\otimes u).
	\end{equation}
\end{prop}
\begin{proof}
	We have for any $Y\in\Lin(\calY)$ and $X\in\Lin(\calX)$,
	\begin{equation}
	\begin{gathered}
		\langle Y,\Phi(X)\rangle
		= \Tr_\calY\big[Y^\dagger \Tr_\calW[U(X\otimes uu^\dagger)U^\dagger]\big] \\
		= \Tr_{\calY\otimes\calW} \big[
		(Y^\dagger\otimes I_{\calW}) U (X\otimes uu^\dagger) U^\dagger
		\big] \\
		= \Tr_{\calX\otimes\calZ} \big[
		U^\dagger (Y^\dagger\otimes I_{\calW}) U (X\otimes uu^\dagger)
		\big] \\
		= \Tr_{\calX} \big[
		(I_\calX\otimes u^\dagger) [
			U^\dagger (Y^\dagger\otimes I_{\calW}) U
		] (I_\calX\otimes u) \,\, X
		\big].
	\end{gathered}
	\end{equation}
	Note that here $u\in\calZ$. We conclude that
	\begin{equation}
		\Phi^\dagger(Y) =
		(I_{\calX}\otimes u^\dagger)
		[U^\dagger (Y\otimes I_{\calW}) U]
		(I_{\calX}\otimes u)
		\in\Lin(\calX).
	\end{equation}
\end{proof}

\subsection{Choi representation}

\begin{defn}
	Let $\Phi\in\rmT(\calX,\calY)$ be a map between finite-dimensional complex spaces.
	We define its \emph{Choi representation} as the operator $J(\Phi)\in\Lin(\calY\otimes\calX)$ such that
	\begin{equation}
		J(\Phi) = (\Phi\otimes I_{\calX})(e_+ e_+^\dagger)
		= \sum_{ij} \Phi(E_{ij})\otimes E_{ij},
	\end{equation}
	where $e_+\equiv \sqrt{\dim(\calX)}\ket+\equiv \sum_i \ket{i,i}\in\calX\otimes\calX$ is the maximally entangled state.
\end{defn}

\begin{prop}
	The matrix elements of $J(\Phi)$ are related to the map and its natural representation via
	\begin{equation}
		\mel{i,j}{J(\Phi)}{k,\ell}
		= \mel{i}{\Phi(E_{j,\ell})}{k}
		= \mel{i,k}{K(\Phi)}{j,\ell}.
	\end{equation}
\end{prop}

Note that $(I_{\calY}\otimes\bra i)J(\Phi)(I_{\calY}\otimes\ket \ell) = \Phi(E_{i,\ell})$.
This means that we can represent $J(\Phi)$ as a ``matrix of operators'' in $\Lin(\calY)$, that is, as a block matrix in which each block gives the action of $\Phi(E_{i,\ell})$ for some $E_{i,\ell}$.

The mapping $\rmT(\calX,\calY)\to \Lin(\calY\otimes \calX)$ sending each map to its Choi representation is a bijection. Its inverse can also be written explicitly:
\begin{prop}\label{prop:map_from_choi}
	Let $J\in\Lin(\calY\otimes\calX)$ be a linear operator.
	Define the map $\Phi_J\in\rmT(\calX,\calY)$ by the relation
	\begin{equation}
		\Phi_J(X) \equiv \Tr_\calX[(I_\calY\otimes X^T)J].
	\end{equation}
	Then $J(\Phi_J) = J$, that is, $\Phi_J$ is the map corresponding to $J$ under the Choi isomorphism.
\end{prop}
\begin{proof}
	We observe that
	\begin{equation}
		J(\Phi_J)
		= \sum_{ij} \Phi_J(E_{ij})\otimes E_{ij}
		= \sum_{ij} \Tr_\calX[(I_\calY\otimes E_{ji})J] \otimes E_{ij}
		= J.
	\end{equation}
\end{proof}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$ be a map. Then,
	\begin{enumerate}
		\item $\Phi$ is Hermitian-preserving \emph{iff} $J(\Phi)\in\Herm(\calY\otimes\calX)$.
		\item $\Phi$ is trace-preserving \emph{iff}
	$\Tr_\calY(J(\Phi)) = I_{\calX}$.
	\end{enumerate}
\end{prop}
\begin{proof}$\,$
	\begin{enumerate}
		\item Remember that $\Phi$ being Hermitian-preserving means that $\Phi(X)\in\Herm(\calY)$ for all $X\in\Herm(\calX)$.
		This is equivalent to $\Phi(X^\dagger)=\Phi(X)^\dagger$ for all $X\in\Lin(\calX)$. Thus
		\begin{equation}
			J(\Phi)^\dagger =
			\sum_{ij} \Phi^\dagger(E_{ij})\otimes E_{ij}^\dagger
			= \sum_{ij} \Phi(E_{ji}) \otimes E_{ji}
			= J(\Phi).
		\end{equation}
		For the other direction, we use~\cref{prop:map_from_choi} to observe that $\Phi$ can be written as a composition of Hermitian-preserving maps:
		\begin{equation}
			\Phi = \Tr_\calX \circ R_{J(\Phi)} \circ E_\calY \circ T,
		\end{equation}
		where $T$ denotes the transpose map, $E_\calY$ the map $\Lin(\calX)\to\Lin(\calY\otimes\calX)$ sending $A\mapsto I_\calY\otimes A$, and $R_{J(\Phi)}$ is the map multiplying its input to the right with $J(\Phi)$. All of these maps are Hermitian-preserving --- in particular, $R_{J(\Phi)}$ is Hermitian-preserving when $J(\Phi)$ is Hermitian --- and thus so is their composition.
		\item If $\Phi$ is trace-preserving, then
		\begin{equation}
			\Tr_\calY(J(\Phi))
			= \sum_{ij} \Tr[\Phi(E_{ij})]\otimes E_{ij}
			= \sum_i E_{ii} = I_\calX
		\end{equation}
		For the other direction, assuming $\Tr_\calY(J(\Phi))=I_\calX$, we again use~\cref{prop:map_from_choi} and observe that
		\begin{equation}
			\Tr[\Phi(X)]
			= \Tr[(I_\calY\otimes X^T)J(\Phi)]
			= \Tr(X^T) = \Tr(X).
		\end{equation}
	\end{enumerate}
\end{proof}



\subsection{Stinespring representation}

\subsection{Kraus representation}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$ be a map 
\end{prop}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$ be a map. There are collections of operators $\{A_a: a\in\Sigma\},\{B_a: a\in\Sigma\}\subset\Lin(\calX,\calY)$, for some finite alphabet $\Sigma$ with $\lvert\Sigma\rvert=\rank(J(\Phi))$, such that for all $X\in\Lin(\calX)$ we have
	\begin{equation}
		\Phi(X) = \sum_{a\in\Sigma} A_a X B_a^\dagger.
	\end{equation}
\end{prop}
% \begin{proof}
% 	In the natural representation, the statement reads
% 	\begin{equation}
% 		K(\Phi) = \sum_{a\in\Sigma} A_a\otimes \bar B_a.
% 	\end{equation}
% 	Remember that $K(\Phi)\in\Lin(\calX\otimes\calX,\calY\otimes\calY)$.
% 	From the singular value decomposition, we know there are collections of orthogonal vectors
% 	\begin{equation}
% 		\{u(a):a\in\Gamma\}\subset \calY\otimes\calY, \qquad
% 		\{v(a):a\in\Gamma\}\subset \calX\otimes\calX,
% 	\end{equation}
% 	for some finite alphabet $\Gamma$ with $\lvert\Gamma\rvert\le \rank(K(\Phi))$, such that
% 	\begin{equation}
% 		K(\Phi) = \sum_{a\in\Gamma} u(a) v(a)^\dagger.
% 	\end{equation}
% \end{proof}
\begin{proof}
	In the Choi representation, the statement reads
	\begin{equation}
		J(\Phi) = \sum_{a\in\Sigma} \on{vec}(A_a) \on{vec}(B_a)^\dagger.
	\end{equation}
	Remember that $J(\Phi)\in\Lin(\calY\otimes\calX)$.
	From the singular value decomposition, we know there are collections of orthogonal vectors
	\begin{equation}
		\{u(a):a\in\Sigma\}\subset \calY\otimes\calX, \qquad
		\{v(a):a\in\Sigma\}\subset \calY\otimes\calX,
	\end{equation}
	for some finite alphabet $\Sigma$ with $\lvert\Sigma\rvert=\rank(J(\Phi))$, such that
	\begin{equation}
		J(\Phi) = \sum_{a\in\Sigma} u(a) v(a)^\dagger.
	\end{equation}
	The statement follows with $A_a,B_a$ such that $\on{vec}(A_a)=u(a), \on{vec}(B_a)=v(a)$.
\end{proof}
\begin{proof}
	Observe that $u\mapsto\Phi(uv^\dagger)$ is a linear mapping $\calX\to\Lin(\calY)$.
	\highlight{TBQEOINC}

	For any $u,v\in\calX$, we can write the singular value decomposition of $\Phi(uv^\dagger)$ as
	\begin{equation}
		\Phi(uv^\dagger) = \sum_{a\in\Sigma_{u,v}} x_a(u) y_a(v)^\dagger,
	\end{equation}
	for some collections of orthogonal vectors
	\begin{equation}
		\{x_a(u): a \in\Sigma_{u,v}\},
		\{y_a(v): a \in\Sigma_{u,v}\}\subset\calY,
	\end{equation}
	and some finite alphabets $\Sigma_{u,v}$ with $\lvert\Sigma_{u,v}\rvert=\rank(\Phi(uv^\dagger))$.
	Upon some relabelling, we can assume $\Sigma_{u,v}=\Sigma$ with $\lvert\Sigma\rvert=\max_{u,v\in\calX}\rank(\Phi(uv^\dagger))$.
	Observe that 
\end{proof}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$.
	Suppose there are operators $\{A_a: a\in\Sigma\}\subset\Lin(\calX,\calY)$ such that
	$\Phi(X)=\sum_{a\in\Sigma}A_a X A_a^\dagger$ for all $X\in\Lin(\calX)$.
	Then $\Phi$ is completely positive.
\end{prop}
\begin{proof}
	\begin{equation}
		(\Phi\otimes I_{\calX}) \psi\psi^\dagger
		= \sum_{a\in\Sigma} sdfsfsdfijdfoisjfmlerkmgrermgs
	\end{equation}
\end{proof}

\paragraph{SVD? this paragraph must be reviewed}
Any operator $A\in\Lin(\calX,\calY)$ can be uniquely characterised as a sum of the form
\begin{equation}
	A = \sum_{a\in\Sigma} s(a)
	\sum_{b=1}^{m_a} u(a,b) v(a,b)^\dagger,
\end{equation}
where $s(a)\ge0$ are the \emph{singular values} of $A$, such that $s(a)\neq s(b)$ if $a\neq b$, and $\{u(a,b)\}_{a,b}\subset\calY$,
$\{v(a,b)\}_{a,b}\subset\calX$ are orthonormal collections of vectors.
Here $\Sigma$ is a finite register with $\lvert\Sigma\rvert\le \rank(A)$ equal to the number of distinct singular values of $A$ (\emph{i.e.} to the geometric multiplicity of $AA^\dagger$), and $\sum_a m_a=\rank(A)$.


\section{Notable examples}

\begin{example}
	Consider the trace map, $\Phi\in\rmT(\calX,\CC)$, define by $\Phi(X)=\Tr(X)$ for all $X\in\Lin(\calX)$.
	It is worth noting that $\Lin(\CC)\simeq \CC$, via the isomorphism
	$\Lin(\CC)\ni f\mapsto f(1)\in\CC$.
	Therefore the trace map, as any other map whose codomain is one-dimensional, is effectively a functional $\Lin(\calX)\to\CC$.

	The trace map is clearly Hermitian- and trace-preserving, as well as completely positive.

	Its natural and Choi representations are
	\begin{equation}
		K(\Tr) = e_+^\dagger,
		\qquad J(\Tr) = I_{\calX}.
	\end{equation}
	For the Kraus representation, given any orthonormal basis $\{x_a\}\subseteq\calX$, we can choose $A_a=x_a^\dagger\in\Lin(\calX,\CC)$:
	\begin{equation}
		\sum_a x_a^\dagger X x_a = \Tr(X),
		\qquad X\in\Lin(\calX).
	\end{equation}
	The Stinespring isometry is the $V\in\rmU(\calX,\calW)$ defined by
	\begin{equation}
		V \equiv \sum_a e_a A_a = \sum_a e_a x_a^\dagger.
	\end{equation}
	The adjoint is the map $\Tr^\dagger:\CC\to\Lin(\calX)$ such that
	\begin{equation}
		\bar\alpha \Tr(X)
		= \langle \alpha,\Tr(X)\rangle
		= \langle \Tr^\dagger(\alpha), X\rangle
		= \Tr[(\Tr^\dagger(\alpha))^\dagger X],
	\end{equation}
	for all $X\in\Lin(\calX)$ and $\alpha\in\CC$.
	Therefore $\Tr^\dagger(\alpha)=\alpha I_\calX$.
\end{example}

\begin{example}
	Consider the \emph{partial trace} map:
	$\Tr_\calY\in\rmT(\calX\otimes\calY,\calX)$, with
	\begin{equation}
		\Tr_\calY \equiv I_{\Lin(\calX)}\otimes\Tr.
	\end{equation}
	Its adjoint is then
	\begin{equation}
		(\Tr_\calY)^\dagger(X) = X\otimes I_\calY.
	\end{equation}
\end{example}

\begin{example}
	Consider the \emph{replacement map} $\Phi\in\rmT(\calX,\calY)$ defined by
	\begin{equation}
		\Phi(\rho)=\langle X, \rho \rangle Y
		\equiv \Tr(X^\dagger \rho) Y,
	\end{equation}
	for $X\in\Lin(\calX),Y\in\Lin(\calY)$ and $\rho\in\Lin(\calX)$.
	We have
	\begin{equation}
		\Phi(\rho^\dagger) = \overline{\Tr(X\rho)} Y,
		\qquad
		\Phi(\rho)^\dagger = \overline{\Tr(X^\dagger\rho)} Y^\dagger.
	\end{equation}
	It follows that the replacement map $\Phi$ is \emph{Hermitian preserving} if and only if $X\in\Herm(\calX)$ and $Y\in\Herm(\calY)$.
	Moreover,
	\begin{equation}
		K(\Phi) = \on{vec}(Y)\on{vec}(X)^\dagger,
		\qquad
		J(\Phi) = Y\otimes \bar X.
	\end{equation}
	From the Choi, we see immediately that $\Phi$ is CP \emph{iff} $X$ and $Y$ are positive semidefinite.
	Moreover, $\Phi$ is trace-preserving \emph{iff} $X=I_\calX$.

	The Kraus representation is obtained from the eigenvectors of $J(\Phi)$.
	If $\Phi$ is Hermitian preserving, and thus $X,Y$ are Hermitian,
	then we can write
	$Y=\sum_a y_a y_a'^\dagger$ and $X=\sum_a x_a x_a'^\dagger$
	for some orthogonal collections $\{x_a\}_a,\{x_a'\}_a\subset\calX$ and $\{y_a\}_a,\{y_a'\}_a\subset\calY$.
	Therefore
	\begin{equation}
		J(\Phi) = \sum_{a,b} (y_a\otimes \bar x_b)(y_a'^\dagger\otimes\bar x_b'^\dagger).
	\end{equation}
	It follows that there are operators
	$A_{(ab)} = y_a x_b^\dagger$ and
	$A'_{(ab)} = y'_a x_b'^\dagger$ such that
	\begin{equation}
		\Phi(\rho) = \sum_{ab} A_{(ab)} \rho A_{(ab)}'^\dagger.
	\end{equation}
	When $X\in\Pos(\calX)$ and $Y\in\Pos(\calY)$, we can choose $x_a'=x_a$ and $y_a'=y_a$.

	For the Stinespring representation, define the isometries $U\in\rmU($
	\begin{equation}
		U \psi = \sum_{ab} sedurftgyuhl
	\end{equation}
\end{example}


\subsection{Quantum-to-classical channels}

Discussed in~\parencite{horodecki2003entanglement,watrous2018theory}.

\begin{defn}
	A channel $\Phi\in\rmC(\calX,\calY)$ is said to be \emph{entanglement breaking} if there is $\{\mu(a):a\in\Sigma\}\in\on{POVM}(\calX)$
	and states $\{Y_a:a\in\Sigma\}\subset\rmD(\calY)$ such that,
	for all $X\in\Lin(\calX)$,
	\begin{equation}
		\Phi(X) = \sum_{a\in\Sigma} \langle \mu(a),X\rangle Y_a.
	\end{equation}
\end{defn}

\begin{prop}
	A map $\Phi\in\rmC(\calX,\calY)$ is entanglement breaking \emph{iff} $(\Phi\otimes I_{\calZ})\Gamma$ is separable for any $\Gamma\in\rmD(\calX\otimes\calZ)$.
\end{prop}

\begin{defn}
	A channel $\Phi\in\rmC(\calX,\calY)$ is said to be \emph{quantum-to-classical} if $\Phi=\Phi_{\rm cd}\circ\Phi$,
	where $\Phi_{\rm cd}\in\rmC(\calY)$ is the \emph{completely dephasing channel}.
\end{defn}


\begin{prop}
	A channel $\Phi\in\rmC(\calX,\calY)$ is quantum-to-classical \emph{iff}
	$\Phi(\rho)\in\rmD(\calY)$ is diagonal for all $\rho\in\rmD(\calX)$.
\end{prop}

\begin{prop}
	A channel $\Phi\in\rmC(\calX,\calY), \calY\equiv \CC^\Sigma$ is quantum-to-classical \emph{iff}
	there is a POVM $\mu:\Sigma\to\Pos(\calX)$ such that, for all $X\in\Lin(\calX)$,
	\begin{equation}
		\Phi(X) = \sum_{a\in\Sigma} \langle\mu(a),X\rangle E_{a,a}.
	\end{equation}
	Moreover, $\mu$ is uniquely determined by $\Phi$.
	In other words, there is a bijection between quantum-to-classical channels and POVMs.
	\highlight{(can we replace the $E_{a,a}$ with an arbitrary set of orthogonal operators?)}
\end{prop}

\subsection{Mixed-unitary channels}

\begin{prop}\label{prop:rank_of_ptrace_equals_schmidt_rank}
	The partial trace of a pure operator is pure \emph{iff} it is a product operator.
	In other words, for any $\psi\in\calX_1\otimes\calX_2$, we have
	$\Tr_{\calX_2}(\psi\psi^\dagger)=\phi\phi^\dagger$ for some $\phi\in\calX_1$ \emph{iff} $\psi=\psi_1\otimes\psi_2$ for some $\psi_i\in\calX_i$.
\end{prop}
\begin{proof}
	If $\psi=\psi_1\otimes\psi_2$ the conclusion is immediate.
	Let us on the other hand suppose that
	$\Tr_{\calX_2}(\psi\psi^\dagger)=\phi\phi^\dagger$.
	Write the SVD decomposition of $\psi$ as
	\begin{equation}
		\psi = \sum_{a\in\Sigma} \psi_1(a)\otimes\psi_2(a),
	\end{equation}
	for some $\{\psi_i(a): a\in\Sigma\}\subset\calX_i$ such that
	$\langle \psi_i(a),\psi_i(b)\rangle = c_i(a) \delta_{ab}$.
	Then
	\begin{equation}
		\Tr_{\calX_2}(\psi\psi^\dagger)
		= \sum_{a\in\Sigma} \|\psi_2(a)\|^2 \PP[\psi_1(a)].
	\end{equation}
	Because $\{\psi_1(a):a\in\Sigma\}$ is a collection of \emph{orthogonal} vectors, this is a sum over rank-one orthogonal (non-zero) orthoprojections.
	It follows that
	\begin{equation}
		\rank(\Tr_{\calX_2}(\psi\psi^\dagger)) = \lvert\Sigma\rvert.
	\end{equation}
	This implies, in particular, that if the partial trace is pure, then $\psi$ must have a single nonzero singular value, that is, it must be a product state.
\end{proof}

\begin{prop}
	Let $\Phi\in\rmT_{\rm CP}(\calX,\calY)$ be a completely positive map with unitary representation
	\begin{equation}
		\Phi(X) = \Tr_\calW[U(X\otimes uu^\dagger)U^\dagger],
	\end{equation}
	where $U\in\rmU(\calX\otimes\calZ,\calY\otimes\calW)$ and $u\in\calZ$.
	Then $\Phi$ is a isometric channel, $\Phi(X)=VXV^\dagger$ for some $V\in\rmU(\calX,\calY)$, \emph{iff} ???
\end{prop}
\begin{proof}
	if $\Phi$ is isometric, then for any $x\in\calX$,
	\begin{equation}
		\Phi(xx^\dagger) = \Tr_\calW(\PP[U(x\otimes u)])
		= \PP[V x].
	\end{equation}
	Using~\cref{prop:rank_of_ptrace_equals_schmidt_rank}, this implies that $U(x\otimes u)$ must be a product state, that is, there must be $y_x\in\calY$ and $z_x\in\calZ$ such that $U(x\otimes u)=y_x\otimes z_x$.
	\highlight{(OAISJDOAISJDOAISJDOAISJDOAISJD)}
\end{proof}

\begin{prop}
	Let $\Phi\in\rmC(\calX)$ be a channel, and let $\Psi\in\rmT_{\rm CP}(\calX)$ be a \emph{completely positive} map such that
	$\Phi\circ\Psi=I_{\Lin(\calX)}$.
	Then there exists a unitary operator $U\in\rmU(\calX)$ such that
	\begin{equation}
		\Phi(X) = U^\dagger X U,
		\qquad
		\Psi(X) = UXU^\dagger,
	\end{equation}
	for all $X\in\Lin(\calX)$.
\end{prop}

\subsection{Dephasing channel}

This is the channel
\begin{equation}
	\Phi(\rho) = \frac{2-p}{2} \rho + \frac{p}{2} Z\rho Z.
\end{equation}
Its Kraus operators are thus
\begin{equation}
	A_0 = \sqrt{\frac{2-p}{2}} I,
	\qquad
	A_1 = \sqrt{\frac{p}{2}} Z.
\end{equation}
In vectorised form, these are
\begin{equation}
	\on{vec}(A_0) = \sqrt{\frac{2-p}{2}} (e_0\otimes e_0 + e_1\otimes e_1),
	\quad
	\on{vec}(A_1) = \sqrt{\frac{p}{2}} (e_0\otimes e_0 - e_1\otimes e_1),
\end{equation}
and thus the Choi representation reads
\begin{equation}
	J(\Phi) =
	\left(\frac{2-p}{2}\right) 2\PP[\ket*{\Phi^+}]
	+ \left(\frac{p}{2}\right) 2\PP[\ket*{\Phi^-}],
\end{equation}
where $\sqrt2\ket*{\Phi^\pm}\equiv \ket{00}\pm\ket{11}$.
As a matrix, this is represented by
\begin{equation}
	J(\Phi) \doteq
	\begin{pmatrix}
		1 & 0 & 0 & 1- p \\
		0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 \\
		1 - p & 0 & 0 & 1
	\end{pmatrix}.
\end{equation}
Its Stinespring representation is obtained via the isometry
\begin{equation}
	U \psi = 
	\sqrt{\frac{2-p}{2}} (\psi\otimes e_0)
	+ \sqrt{\frac{p}{2}} [(Z\psi)\otimes e_1].
	% U = \begin{pmatrix}	
	% \end{pmatrix}
\end{equation}

\subsection{??}

\begin{itemize}
	\item Isometric channels: $\Phi(X)=UXU^\dagger$ for some isometry $U$.
	\item Replacement channels;
	\item Entanglement-breaking channels. Channels $\Phi\in\rmC(\CC^n,\calY)$ such that for any bipartite $\rho\in\rmD(\calX\otimes\calH)$, the state $(\Phi\otimes I_{\Lin(\calH)})(\rho)\in\rmD(\calY\otimes\calH)$ is separable. We know that $\Phi\in\mathrm{EB}$ \emph{\textbf{iff}} its Choi is separable.
	An alternative characterisation is that $\Phi\in\mathrm{EB}$ \emph{\textbf{iff}} it can be written as $\Phi(\rho)=\sum_a \langle \mu(a),\rho\rangle \rho_a$, for some POVM $\{\mu(a)\}_a$ and collection of states $\{\rho_a\}_a$.
	\item Extreme channels;
	\item Pinching channels: $\Phi(X) =\sum_a \Pi_a X\Pi_a$ with $\{\Pi_a\}$ a projective measurement. The completely dephasing channel is an example of a pinching channel;
	\item Mixed-unitary channels;
\end{itemize}


\section{Adjoint channel}

Given a map $\Phi\in\rmT(\calX,\calY)$, we can always define its \emph{adjoint} $\Phi^*$. This is the unique map $\Phi^*\in\rmT(\calY,\calX)$ such that $\langle \Phi^*(Y),X\rangle=\langle Y,\Phi(X)\rangle$ for all $X\in\Lin(\calX)$ and $Y\in\Lin(\calY)$.

The adjoint of a channel is also sometimes referred to as the \emph{Heisenberg-picture representation} of the channel. This originates from the observation that performing a measurement $\mu\in\Pos(\calY)$ on a state $\rho\in\rmD(\calX)$ after evolution through $\Phi\in\rmC(\calX,\calY)$, amounts to performing the measurement $\Phi^*(\mu)$ on $\rho$:
\begin{equation}
	\langle \mu,\Phi(\rho) \rangle = \langle \Phi^*(\mu),\rho\rangle.
\end{equation}


\section{Interesting properties and facts}

\begin{itemize}
	\item $\rmC(\calX,\calY)$ is \emph{convex};
	\item A channel is invertible \emph{\textbf{iff}} it is a unitary channel;
\end{itemize}

\begin{thm}
	Suppose $\Phi,\Psi\in\rmC(\calX,\calY)$ are such that $\Phi\circ\Psi=\Id$. If their Kraus operators are $\{A_a\}_a$ and $\{B_b\}_b$, respectively, then $A_a B_b=\lambda_{ab} I$ for some $\lambda_{ab}\in\CC$.
\end{thm}
\begin{proof}
	See Preskill notes.
\end{proof}

\section{Norms, distances, etc}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$ be a positive, unital map. Then
	\begin{equation}
		\|\Phi(X)\| \le \|X\|,
	\end{equation}
	for all $X\in\Lin(\calX)$.
\end{prop}
\begin{proof}
	From Watrous.
	Sketch: operator norm is max of inner product $\langle Y,\Phi(X)\rangle$ over ops $Y$ such that $\|Y\|=1$. To finish.
\end{proof}

\begin{defn}[Induced trace norm]
	Let $\Phi\in\rmT(\calX,\calY)$. Its \emph{induced trace norm} is
	\begin{equation}
		\|\Phi\|_1 \equiv \max\left\{\|\Phi(X)\|_1 :
		\,\, X\in\Lin(\calX) \text{ with } \|X\|_1\le 1
		\right\}.
	\end{equation}
	In words, this is the trace norm of $\Phi(X)$ maximised over all possible operators $X$ with trace norm less than $1$.
\end{defn}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$ be a positive, unital map. Then
	\begin{equation}
		\|\Phi\|_1 = \max_{U\in\rmU(\calY)}\|\Phi^\dagger(U)\|.
	\end{equation}
\end{prop}
\begin{proof}
	We know that, for every $X\in\Lin(\calX)$, the trace norm satisfies
	\begin{equation}
		\|X\|_1 = \max\left\{
		\lvert\langle U,X\rangle\rvert : \,\, U\in\rmU(\calX)
		\right\}.
	\end{equation}

\end{proof}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$ be a map. Then
	\begin{equation}
		\|\Phi\|_1 = \max_{u,v\in\calS(\calX)}\|\Phi(uv^\dagger)\|_1.
	\end{equation}
\end{prop}
\begin{proof}
	The trace norm is a convex function, and every $X\in\Lin(\calX)$ is a convex combination of rank-one operators $uv^\dagger$.
	Similarly, every $X\in\Lin(\calX)$ with unit trace-norm is a convex combination of rank-one operators $uv^\dagger$ with $u,v\in\calS(\calX)$.
\end{proof}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$ be a \emph{positive} map. Then
	\begin{equation}
		\|\Phi\|_1 = \max_{u\in\calS(\calX)}\Tr[\Phi(uu^*)].
	\end{equation}
\end{prop}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$ be positive and trace-preserving. Then
	$\|\Phi\|_1=1$.
\end{prop}

\begin{prop}
	Consider the channels
	$\Phi_0,\Psi_0\in\rmC(\calX,\calY)$ and
	$\Phi_1,\Psi_1\in\rmC(\calY,\calZ)$.
	Then
	\begin{equation}
		\|\Psi_1\circ\Psi_0 - \Phi_1\circ\Phi_0\|_1
		\le
		\|\Psi_0 - \Phi_0\|_1 +
		\|\Psi_1 - \Phi_1\|_1.
	\end{equation}
\end{prop}

\begin{prop}
	Let $\Phi\in\rmT(\calX,\calY)$. Let
	$U_0,V_0\in\rmU(\calX)$ and
	$U_1,V_1\in\rmU(\calY)$ be unitaries. Define $\Psi\in\rmT(\calX,\calY)$ as
	\begin{equation}
		\Psi(X) \equiv U_1 \Phi(U_0 X V_0) V_1.
	\end{equation}
	Then $\|\Psi\|_1 = \|\Phi\|_1$.
\end{prop}

\section{Quantum state discrimination (to move?)}

Suppose Alice sends Bob a state. She can decide to send one of $\rho_0,\rho_1\in\rmD(\calX)$. Bob's goal is to determine whether the state he received is $\rho_0$ or $\rho_1$.

\hl{(does Bob need to know the distribution with which $\rho_0$ and $\rho_1$ are chosen?)}

How would Bob manage such a feat? He will need to find some \emph{binary measurement} $\mu:\{0,1\}\to\Pos(\calX)$ which, when used on the received state $\rho$, returns a result which with good probability corresponds to the state Alice sent in the first place.
If Bob knows that Alice will send him either the state $\rho_i$ with some probability $\lambda_i$, he will describe the state send by Alice as
$\rho = \sum_{i=0,1} \lambda_i \rho_i$.

Suppose Bob performs a measurement $\mu$ and finds some result $b\in\{0,1\}$.
If the received state is $\rho_i$, the probability of observing such event is
\begin{equation}
	\Prob(\text{measured }b | \rho=\rho_i)
	\equiv p(b | \rho_i,\mu)
	= \langle \mu(b), \rho_i\rangle.
\end{equation}
Therefore, the \emph{prior} probability of the state being $\rho_i$, knowing that the measurement resulted in the outcome $b$, is
\begin{equation}
	\Prob(\rho_i | b)
	= \frac{\lambda_i \langle \mu(b),\rho_i\rangle}{\Prob(b)}
	= \frac{\lambda_i \langle \mu(b),\rho_i\rangle}{\sum_i \lambda_i \langle \mu(b),\rho_i\rangle}.
	\label{eq:prob_rhoi_given_measuredb}
\end{equation}
Of course, from Bob's perspective, this is the prior probability only if he knows the probabilities $\lambda_i$ with which each $\rho_i$ is picked by Alice.
From~\cref{eq:prob_rhoi_given_measuredb}, we see that Bob's best guess about which state he received is
\begin{equation}
	i_{\rm opt}(\mu,b) = \argmax_i \lambda_i \langle\mu(b),\rho_i\rangle.
\end{equation}
The probability of this guess being correct is
$\max_i\Prob(\rho_i|b)$.
The overall probability of guessing correctly using this strategy (that is, the probability of successfully finding $i$ estimated before knowing the measurement outcome $b$) is thus
\begin{equation}
	p_{\rm success}(\mu)
	= \sum_{b\in\{0,1\}}
	\Prob(b)
	\max_i \Prob(\rho_i | b)
	= \sum_{b\in\{0,1\}}
	\max_{i\in\{0,1\}}
	\lambda_i \langle\mu(b),\rho_i\rangle.
\end{equation}


\begin{example}
	Suppose we want to discriminate between $\rho_0=\PP_0$ and $\rho_1=\PP_+\equiv \frac12\sum_{i,j\in\{0,1\}}E_{ij}$, and we use the measurement $\mu$ with $\mu(0)=\PP_0, \mu(1)=\PP_1$.
	The outcome probabilities then read
	\begin{equation}
		\langle \mu(0),\rho_0\rangle = 1,
		\quad
		\langle \mu(1),\rho_0\rangle = 0,
		\quad
		\langle \mu(0),\rho_1\rangle = \frac12,
		\quad
		\langle \mu(1),\rho_1\rangle = \frac12.
	\end{equation}
	Therefore we have
	$\Prob(b=0) = (1+\lambda_0)/2$,
	$\Prob(b=1) = (1-\lambda_0)/2$, and 
	\begin{equation}
	\begin{gathered}
		\Prob(\rho_0|b=0) = \frac{2\lambda_0}{1+\lambda_0},
		\qquad\qquad
		\Prob(\rho_1|b=0) = \frac{1-\lambda_0}{1+\lambda_0}, \\
		\Prob(\rho_0|b=1) = 0,\qquad\qquad\,\,
		\qquad\qquad
		\Prob(\rho_1|b=1) = 1.
	\end{gathered}
	\end{equation}
	% \begin{equation}
	% 	\Prob(\rho_0|b=0) = \frac{2\lambda_0}{1+\lambda_0},\quad
	% 	\Prob(\rho_1|b=0) = \frac{1-\lambda_0}{1+\lambda_0},\quad
	% 	\Prob(\rho_0|b=1) = 0,\quad
	% 	\Prob(\rho_1|b=1) = 1.
	% \end{equation}
	Then, $i_{\rm opt}(\mu,0)= \max(2\lambda_0,1-\lambda_0)$
	and $i_{\rm opt}(\mu,1)= 1$, which means that when $b=0$ is measured, Bob's best guess is $\rho_0$ when $\lambda_0\ge1/3$, whereas when $b=1$ is found, Bob's best guess is always $\rho_1$.
	This is of course to be expected: $b=1$ is possible only if $\rho=\rho_1$.

	The overall success probability with this strategy is
	$p_{\rm success}(\mu) = \max(\lambda_0,\frac{1-\lambda_0}{2})+\frac{1-\lambda_0}{2}$.

	Let us consider now a more general strategy for the same qubit states.
	Suppose $\mu(0)= \alpha(I+\bs v\cdot\bs\sigma)$, where $0<\alpha\le1$ and $\|\bs v\|\le \frac{1-\alpha}{\alpha}$. Every effect can be written in such a way, thus we can parametrise every one-qubit POVM changing $\alpha$ and $\bs v\in\RR^3$. With this POVM we have
	\begin{equation}
		\langle\mu(0),\rho_0\rangle = \alpha(1+v_3),
		\qquad
		\langle\mu(0),\rho_1\rangle = \alpha(1+v_1),
	\end{equation}
	and thus
	\begin{equation}
		\Prob(b=0) = \alpha[\lambda_0(1+v_3) + \lambda_1(1+v_1)]
		= \alpha[(1+v_1) + \lambda_0 (v_3-v_1)].
	\end{equation}
	The success probabilities reads
	\begin{equation}
		p_{\rm success}(\mu)
		= \max(
		\lambda\alpha(1+v_3),
		(1-\lambda)\alpha(1+v_1)
		) +
		\max(
		\lambda(1-\alpha-\alpha v_3),
		(1-\lambda)(1-\alpha-\alpha v_1)
		).
		\label{eq:QST:success_prob_in_example_general_POVM}
	\end{equation}
	Note how this success probability is consistent with the one derived for the special case of $\mu(0)=\PP_0$, which corresponds to the choice of parameters $\alpha=1/2$ and $\bs v=(0,0,1)^T$.
\end{example}


% Another way to get to an expression for $p_{\rm success}(\mu)$ is to notice that 
We can now use the identity $2\max(a,b)=(a+b)+|a-b|$, valid for any $a,b\in\RR$. This tells us that
\begin{equation}
	p_{\rm success}(\mu)
	= \frac12 + \frac12
	\sum_{b\in\{0,1\}} \left\lvert
	\langle\mu(b), \lambda_0 \rho_0 - \lambda_1\rho_1\rangle
	\right\rvert.
\end{equation}
The natural next question is then: what is the measurement $\mu$ that maximises the success probability $p_{\rm success}(\mu)$?
To answer this we observe that
\begin{equation}
	\sup_{\mu\in\on{POVM}}
	\sum_{b\in\{0,1\}} \left\lvert
	\langle\mu(b), \lambda_0 \rho_0 - \lambda_1\rho_1\rangle
	\right\rvert
	= \|\lambda_0 \rho_0 - \lambda_1\rho_1\|_1.
\end{equation}
This follows as a special case of~\cref{prop:sum_of_traceswithPos_less_than_trace_norm}.

\begin{prop}
	Let $\calX$ be a finite-dimensional complex space, and let $A\in\Herm(\calX)$.
	Let $P,Q\in\Pos(\calX)$ such that $P+Q=I$. Then
	\begin{equation}
		\lvert \Tr(PA)\rvert
		+ \lvert \Tr(QA)\rvert \le \|A\|_1.
	\end{equation}
	Moreover, the equality is achieved choosing $P$ as the projector onto the positive eigenvectors of $A$.
	\label{prop:sum_of_traceswithPos_less_than_trace_norm}
\end{prop}


\begin{prop}
	Let $u\in\CC^\Sigma$ be a vector,
	and $\{P_a: a\in\Sigma\}\subset\Pos(\calX)$ a collection of positive semidefinite operators.
	Then
	\begin{equation}
		\left\|\sum_{a\in\Sigma} u(a)P_a\right\|
		\le \| u\|_\infty \left\| \sum_{a\in\Sigma}P_a\right\|.
	\end{equation}
\end{prop}
\begin{proof}
	Define $A\in\Lin(\calX,\calX\otimes\CC^\Sigma)$ as the operator
	\begin{equation}
		A = \sum_{a\in\Sigma} \sqrt{P_a}\otimes e_a
		: \calX\ni v\mapsto \sum_{a\in\Sigma} (\sqrt{P_a}v)\otimes e_a.
	\end{equation}
	Componentwise, this amounts to $A_{ia,j} = (\sqrt{P_a})_{ij}$.
	Its Hermitian conjugate $A^\dagger$ is the operator satisfying
	% \begin{equation}
		$A^\dagger(v\otimes e_a)
				= \sqrt{P_a}v$.
	% \end{equation}
	Using $A$, we can write $P_a$ as
	\begin{equation}
		P_a = A^\dagger (I_{\calX}\otimes E_{aa})A,
		\qquad E_{aa}\equiv e_a e_a^\dagger.
	\end{equation}
	We then have
	\begin{equation}
	\begin{gathered}
		\left\|\sum_{a\in\Sigma} u(a) P_a\right\|
		= \left\|\sum_{a\in\Sigma} u(a) A^\dagger(I_{\calX}\otimes E_{aa})A\right\| \\
		\le \|A^\dagger\|
		\left\|\sum_{a\in\Sigma} u(a) (I_{\calX}\otimes E_{aa})\right\|
		\|A\|
		= \|u\|_\infty \left\|\sum_{a\in\Sigma}P_a\right\|.
	\end{gathered}
	\end{equation}
\end{proof}
\begin{proof}
	If $B$ is Hermitian, the operator norm satisfies
	$\|B\|=\max_{x:\|x\|=1} \lvert\langle x,Bx\rangle\rvert$.
	Therefore
	\begin{equation}
		\left\|\sum_{a\in\Sigma} u(a) P_a\right\|
		= \max_{x:\|x\|=1} \left\lvert
		\sum_{a\in\Sigma}
		u(a) \langle x, P_a x\rangle
		\right\rvert
		\le \|u\|_\infty
		\underbrace{
			\max_{x:\,\|x\|=1}
			\sum_{a\in\Sigma}\langle x,P_a x\rangle
		}_{=\|\sum_a P_a\|},
	\end{equation}
	where we used the fact that $\langle x,Px\rangle\ge0$ for all $x$.
\end{proof}



\section{Channel discrimination}

Let $\Phi_0,\Phi_1\in\rmC(\calX,\calY)$ be channels, and $\lambda\in[0,1]$ be a real number. Let $\rho\in\rmD(\calX)$ be some state.
Suppose Alice can send Bob a quantum state obtained from evolving $\rho$ through either $\Phi_0$ or $\Phi_1$.
Bob's goal is, given $\Phi_i(\rho)$, to figure out whether $i=0$ or $i=1$.

\chapter{Measurements}

Let $\calX$ be a finite-dimensional complex vector space.

\chapter{Foundations}

\section{Mixed facts}

\begin{defn}[Effects]
	Denote with $\on{Eff}(\calH)\subset\Herm(\calH)$ the set of \emph{effects} on $\calH$, that is, the set of positive operators $E$ such that $0\le E \le I$. If $\dim(\calH)=d$, we write this set as $\on{Eff}_d(\calH)$. When clear from the context, the dependence on $\calH$ might be omitted.
\end{defn}
Note that $\on{Eff}_d$ is equivalently characterised as the set of Hermitian operators with eigenvalues in the unit interval.
Note that POVMs are subsets of $\on{Eff}$ that sum to the identity operator.

\begin{prop}\label{prop:expval_of_projections_same_as_orthogonal_vec}
	Let $\calH$ be a finite-dimensional Hilbert space, and let $\Pi\in\Proj(\calH)$ be an orthoprojection in $\calH$.
	Then $\langle v,\Pi v\rangle\equiv\langle \Pi\rangle_v=0$ \emph{iff} $\Pi v=0$.
\end{prop}
\begin{proof}
	Observe that $\langle v,\Pi v\rangle=\|\Pi v\|^2$ is the norm of the projection of $v$ onto the support of $\Pi$.
\end{proof}

\begin{prop}
	The set of effects, $\on{Eff}(\calH)$, is convex. Its extreme points are the projection operators.
\end{prop}
\begin{proof}
	Proof adapted from~\parencite{caves2004gleasontype}.
	Let $E\in\on{Eff}_d$ be an effect in a finite-dimensional space.
	Write its spectral decomposition as
	$E = \sum_k \lambda_k \PP_k$, where $\lambda_k\in[0,1]$ are the eigenvalues of $E$ and $\PP_k$ projectors onto the corresponding eigenvectors.
	We will prove that we can always write $E$ as
	\begin{equation}
		E = \lambda_1 \Pi_1 + \sum_{m=2}^d (\lambda_m-\lambda_{m-1})\Pi_m + (1-\lambda_d) 0,
	\end{equation}
	where $\Pi_k=\sum_{j=k}^d\PP_k$, with $\bs 0$ denoting the null operator (that is, the operator sending every vector to $0\in\calH$).
	Because every such $\Pi_k$ is a projector, this proves that every element of $\on{Eff}_d$ can be written as the convex combination of a finite set of projectors.
	The statement follows from writing
	\begin{equation}
	\begin{gathered}
		\lambda_1\sum_{j=1}^d \PP_j
		+ \sum_{m=2}^d(\lambda_m-\lambda_{m-1})\sum_{j=m}^d \PP_j
		+ (1-\lambda_d)0 \\
		= \sum_{j=1}^d \left(\lambda_1 + \sum_{m=2}^j (\lambda_m-\lambda_{m-1})\right) \PP_j
		+ (1-\lambda_d) 0
		= \underbrace{\sum_{j=1}^d \lambda_j \PP_j}_{=E} + (1-\lambda_d) 0 = E.
	\end{gathered}
	\end{equation}
	This shows that every $E$ is convex combination of projectors.

	We now have to show that every projector is an extreme point, that is, cannot be written as a nontrivial convex combination of other projectors.
	For the purpose, observe that for a generic projector $\Pi\in\calP(\calH)$ we have $v\in\on{supp}(\Pi)\Longleftrightarrow \Pi v=v$.
	Suppose $\Pi=a\Pi_1+(1-a)\Pi_2$ for some $\Pi_1,\Pi_2\in\calP(\calH)$ and $a\in(0,1)$.
	% Then clearly $\on{supp}(\Pi_1)\cap\on{supp}(\Pi_2)\subseteq\on{supp}(\Pi)$.
	Then $\ker(\Pi)\subseteq\ker(\Pi_1)\cap\ker(\Pi_2)\subseteq\ker(\Pi_i)$ for $i=1,2$.
	Indeed, if $\Pi v=0$, then
	\begin{equation}
		\langle \Pi\rangle_v=
		a \langle \Pi_1\rangle_v + (1-a) \langle \Pi_2\rangle_v = 0.
	\end{equation}
	But $\langle \Pi\rangle_w\ge0$ for all $w$, and thus this is only possible if $\langle \Pi_i\rangle_v=0$,
	\emph{i.e.} $\Pi_i v=0$, as follows from~\cref{prop:expval_of_projections_same_as_orthogonal_vec}.
	Thus, taking the orthogonal spaces, we get $\on{supp}(\Pi_i)\subseteq\on{supp}(\Pi)$.
	On the other hand, $\on{supp}(\Pi)\subseteq\on{supp}(\Pi_i)$, because for any $v\in\on{supp}(\Pi)$ we have
	\begin{equation}
		1 = \mel{v}{\Pi}{v}
		= a\mel{v}{\Pi_1}{v} + (1-a) \mel{v}{\Pi_2}{v},
	\end{equation}
	which can only be verified if $\mel{v}{\Pi_1}{v}=\mel{v}{\Pi_2}{v}=1$.
	We conclude that $\on{supp}(\Pi_i)=\on{supp}(\Pi)$, and thus $\Pi=\Pi_1=\Pi_2$.
\end{proof}

\begin{prop}
	Let $A\in\on{Eff}(\calH)$ and $B\in\Lin(\calH)$ such that $A+B\le I$.
	Then $B\in\on{Eff}(\calH)$.
\end{prop}
\begin{proof}
	We need to prove that (1) $B\in\Herm(\calH)$, and (2) that $0\le B\le I$.
	\begin{itemize}
		\item The first follows from writing $A+B+(I-A-B)=I$, which implies that $B=I - A - (I-A-B)$ is Hermitian by virtue of being a linear combination of Hermitian operators.
		\item Taking expectation values on arbitrary vectors $v\in\calH$, we see that
		\begin{equation}
			\expval{B}_v \le 1 - \expval{A}_v.
		\end{equation}
		This implies that $\sigma(B)\in[0,1]$, and thus that $0\le B\le I$.
	\end{itemize}
\end{proof}

\begin{prop}
	Let $A,B\in\on{Eff}(\calH)$ with $A\le B$. Then $B-A\in\on{Eff}(\calH)$.
	\label{prop:diff_of_effects_is_effect}
\end{prop}
\begin{proof}
	By definition $0\le A,B\le I$ and $B-A\ge 0$.
	But also, for any $v\in\calH$, we have
	$\mel{v}{B-A}{v}=\mel{v}{B}{v}-\mel{v}{A}{v}\in[-1,1]$.
	We conclude that $B-A\le I$, and thus $B-A\in\on{Eff}(\calH)$.
\end{proof}


\section{Gleason's theorem}

\begin{defn}[States as probability measures]
	Let $\calP(\calH)$ be the set of projection operators on an Hilbert space $\calH$. A \emph{quantum state} can be defined as a generalised probability measure on $\calP(\calH)$, that is, as a function $v:\calP(\calH)\to[0,1]$ such that $v(I)=1$ and
	$v(\sum_k E_k) = \sum_k v(E_k)$
	for any set of projectors $\{E_k\}\subset\calP(\calH)$ such that $\sum_k E_k\le I$.
\end{defn}
The requirement that $\sum_k E_k\in\calP(\calH)$ automatically implies mutual orthogonality of the $E_k$ (proof), thus $v$ is by definition additive on orthogonal projectors.
Gleason's theorem says that any such state $v$, when $\dim(\calH)\ge3$, can be represented as $v(E)=\Tr(\rho E)$ for some $\rho\in\Herm(\calH)$.
A generalisation of this result to \emph{effects} is given in~\parencite{busch2003quantum,caves2004gleasontype}.

\begin{defn}[Frame functions]
	Let $\calH$ be a separable Hilbert space.
	We say that $f$ is a \emph{frame function} for $\calH$ if, for any orthonormal basis $\{x_i\}\subset\calH$, it satisfies
	$\sum_i f(x_i) = 1$.
	A frame function $f$ is said to be \emph{regular} if there is some $\rho\in\Herm(\calH)$ such that $f(x)=\mel{x}{\rho}{x}$.
\end{defn}

\begin{defn}[Frame functions]
	\emph{Frame functions} are sometimes defined in a slightly different way, as functions $f:\on{Eff}_d\to[0,1]$ such that $\sum_k f(E_k)=1$ for all sets $\{E_k\}\subset\on{Eff}_d$ such that $\sum_k E_k=I$.
\end{defn}
These two definitions should be mostly equivalent. The equivalence is probably proven observing that $\on{Eff}_d$ is the convex hull of projection operators, and that there is a bijection between trace-1 projection operators and unit vectors.

We will use the latter definition for now.

\begin{example}
	Two possible frame functions in $\RR^2$ are $f(\bs n_\theta)=1/2$ and $f(\bs n_\theta)=\cos(2\pi \theta)$.
\end{example}
\begin{proof}
	These are ``frame functions'' in the sense of functions defined on $\calS(\calH)$ (the unit sphere).

	The orthonormal bases in $\RR^2$ are pairs of unit vectors in the directions $\theta$ and $\theta+\pi/2$, for any $\theta\in[0,2\pi]$.
	Denote with $v_1, v_2\in\RR^2$ one such orthonormal basis, and let $f(\bs n_\theta)\equiv \cos(n\theta)$. Then,
	\begin{equation}
	\begin{gathered}
		f(\bs n_\theta) + f(\bs n_{\theta+\pi/2})
		= \cos(n \theta) + \cos(n(\theta+\pi/2)) \\
		= \cos(n\theta)(1 + \cos(n\pi/2)) - \sin(n\theta) \sin(n\pi/2).
	\end{gathered}
	\end{equation}
	Therefore, for $f(\bs n_\theta) + f(\bs n_{\theta+\pi/2})$ to have constant value for all $\theta$, we must have $\sin(n\pi/2)=0$, \emph{i.e.} $n\in 2\ZZ$, which in turn implies $1+\cos(n\pi/2)\in\{0,2\}$ depending on whether $n=0\pmod 4$ or ${n=2\pmod 4}$.
	We conclude that $f(\bs n_\theta) + f(\bs n_{\theta+\pi/2})=1$ if $f(\bs n_\theta)=1/2$, and
	$f(\bs n_\theta) + f(\bs n_{\theta+\pi/2})=0$ for $f(\bs n_\theta)=\cos(2\theta)$.
\end{proof}
\wondering{are these "regular" frames?}

\begin{prop}
	Any frame function satisfies $f(I)=1$.
\end{prop}

\begin{prop}
	Frame functions are \emph{additive} on the set of effects:
	$f(E_1+E_2)=f(E_1)+f(E_2)$ for any $E_1,E_2\in\on{Eff}(\calH)$.
	\label{prop:frame_functions_are_additive}
\end{prop}
\begin{proof}
	Let $E_1,E_2\in\on{Eff}(\calH)$ be such that $E_1+E_2\le I$, and thus $E_1+E_2\in\on{Eff}(\calH)$.
	Then $\{E_1,E_2,I-E_1-E_2\},\{E_1+E_2,I-E_1-E_2\}\in\on{POVM}(\calH)$.
	Therefore
	\begin{equation}
		f(E_1 + E_2 + (I-E_1-E_2))
		= f(E_1) + f(E_2) + f(I-E_1-E_2)
		= f(E_1 + E_2) + f(I-E_1-E_2).
	\end{equation}
	It follows that $f(E_1)+f(E_2)=f(E_1+E_2)$.

	For the more general case where $E_1+E_2\le I$ is not necessarily the case, we observe that $E_1+E_2\le 2I$, and thus $(E_1+E_2)/2\in\on{Eff}(\calH)$ for any $E_1,E_2\in\on{Eff}(\calH)$.
	But we also see that for any $E\in\on{Eff}(\calH)$, if $2E\le I$ then $f(2E)=2f(E)$ and thus for any $E\in\on{Eff}(\calH)$ we must have $f(E/2)=f(E)/2$.
	Therefore
	\begin{equation}
		f(E_1+E_2)/2 = f((E_1+E_2)/2)
		= f(E_1/2) + f(E_2/2)
		= f(E_1)/2 + f(E_2)/2.
	\end{equation}
\end{proof}

\begin{prop}
	Frame functions are homogeneous with respect to nonnegative rationals.
\end{prop}
\begin{proof}
	For any $E\in\on{Eff}(\calH)$ and $k\in\NN\cap[0,\infty]$, we have $E/k\in\on{Eff}(\calH)$.
	Therefore
	\begin{equation}
		f(E) = f(\underbrace{(E/k)+...+(E/k)}_k)
		= k f(E/k).
	\end{equation}
	It follows that $f(E/k)=f(E)/k$.
	Moreover, from~\cref{prop:frame_functions_are_additive}, we know that
	$f(kE)=kf(E)$ for all $E\in\on{Eff}(\calH)$ and positive integers $k$.
	We conclude that $f(qE)=qf(E)$ for any positive rational $q\in\QQ$.
\end{proof}


\begin{prop}
	Let $f$ be a frame function on $\calH$, and $A,B\in\on{Eff}(\calH)$ with $A\le B$. Then $f(A)\le f(B)$.
	\label{prop:frame_functions_are_monotone}
\end{prop}
\begin{proof}
	We know by~\cref{prop:diff_of_effects_is_effect} that $B-A\in\on{eff}(\calH)$.
	Therefore
	\begin{equation}
		f(B) = f((B-A)+A) = f(A) + f(B-A) \ge f(A).
	\end{equation}
\end{proof}

\begin{prop}
	Frame functions are \emph{positively homogeneous}, that is, $f(\alpha E)=\alpha f(E)$ for any $E\in\on{Eff}(\calH)$ and $\alpha\ge0$.
\end{prop}
\begin{proof}
	Let $(p_k)_k$ and $(q_k)_k$ be increasing and decreasing sequences converging to $\alpha$. Then from~\cref{prop:frame_functions_are_monotone} we see that
	\begin{equation}
		p_k f(E) \le f(\alpha E) \le q_k f(E).
	\end{equation}
	Going to the limit we conclude that $\alpha f(E)=f(\alpha E)$.
\end{proof}

\begin{prop}
	Frame functions can be uniquely extended linearly to $\Lin(\calH)$.
\end{prop}
\begin{proof}
	For any $A\in\Herm(\calH)$, we have a decomposition $A=\alpha A_1 - \beta A_2$ with $A_1,A_2\in\on{Eff}(\calH)$ and $\alpha,\beta\ge0$.
	We then just define
	\begin{equation}
		\tilde f(A) = \alpha f(A_1) - \beta f(A_2).
	\end{equation}
	Given an arbitrary $A\in\Lin(\calH)$, we can the define $\tilde f(A)\equiv \tilde f(A_H) + i f(A_S)$, where $A_H\equiv (A+A^\dagger)/2$ and $A_S\equiv (A-A^\dagger)/2i$ are Hermitian and skew-Hermitian components of $A$.
\end{proof}

\begin{prop}
	As was shown with the previous results, any frame function $f:\on{Eff}(\calH)\to[0,1]$ extends linearly to a linear functional $\tilde f\in\Lin(\calH)\to\RR$. Any such functional can be represented as
	\begin{equation}
		\tilde f(A) = \Tr(A \rho_f)
	\end{equation}
	for some $\rho_f\in\Herm(\calH)$.
\end{prop}
\begin{proof}
	Let $\{\sigma_k\}\subset\Lin(\calH)$ be an operatorial basis for $\Lin(\calH)$.
	In this basis, we have the decomposition $A=\sum_k \sigma_k \langle \sigma_k, A\rangle$, where $\langle \sigma_k,A\rangle\equiv\Tr(\sigma_k^\dagger A)$.
	We then write
	\begin{equation}
		\tilde f(A) = \sum_k \langle \sigma_k,A\rangle \tilde f(\sigma_k).
	\end{equation}
	Our goal is to find $\rho_f$ such that $\tilde f(A)=\langle \rho_f,A\rangle = \sum_k \langle\sigma_k,A\rangle \langle\rho_f,\sigma_k\rangle$.
	We thus need $\rho_f$ to satisfy the conditions
	$\langle\rho_f,\sigma_k\rangle = \tilde f(\sigma_k)$.
	We conclude that $\rho_f$ must have the form $\rho_f=\sum_k \tilde f(\sigma_k)^* \sigma_k$.
\end{proof}

\section{Kochen-Specker theorem}

\begin{defn}[Noncontextual assignments]
	Let $v:\Herm(\calH)\to\RR$ be a function that is an \emph{algebra homomorphism on compatible observables}, that is, such that $v(A+B)=v(A)+v(B)$ and $v(AB)=v(A)v(B)$ whenever $[A,B]=0$.
	Let us refer to such a function as a \emph{noncontextual assignment}.
\end{defn}

\begin{prop}
	$v(\alpha A)=\alpha v(A)$ for any $v\in \CC$. This might actually require some continuity assumption to jump from homogeneity with respect to $\QQ$ to the general case, but we will assume this to be satisfied.
\end{prop}

\begin{prop}
	For any observable $A\in\Herm(\calH)$, we have $v(A)\in\sigma(A)$, with $\sigma(A)$ denoting the spectrum of $A$. In particular, $v(P)\in[0,1]$ for any projection operator $P\in\on{Proj}(\calH)$, and $v(I)=1$.
\end{prop}
\begin{proof}
	We can always write $A=\sum_k\lambda_k \PP_{v_k}$ for some orthonormal basis $\{v_k\}_k\subset\calH$, hence the conclusion from observing that only one element of $\{v(\PP_{v_k})\}$ can be nonvanishing.
\end{proof}

Kochen-Specker theorem, as reported \emph{e.g.} in~\parencite{moretti2019fundamental}, states the following:
\begin{prop}[KS theorem]
	Let $\calH$ be a finite-dimensional Hilbert space with $\dim(\calH)\ge3$.
	Consider a function $v:\Herm(\calH)\to\RR$.
	Then the constraints:
	\begin{itemize}
		\item for all $A,B\in\Herm(\calH)$, if $[A,B]=0$ then $v(A+B)=v(A)+v(B)$,
		\item for all $A,B\in\Herm(\calH)$, if $[A,B]=0$ then $v(AB)=v(A)v(B)$,
	\end{itemize}
	are incompatible.
\end{prop}
In other words, Kochen-Specker's theorem states that there cannot be functionals of observables that are both additive and multiplicative.

In~\parencite{cabello1994simple} they prove that such noncontextual assignments are not possible for $\on{dim}(\calH)\ge3$. The proof proceeds via a grueling exploration of how $v$ must act on different trines of projections.

\section{Fine's theorem}

Consider~\parencite{fine1982hidden}.


Reading from~\parencite{kunjwal2015fine}.
We say that a given conditional probability distribution (corresponding to measurement outcomes on spatially separated parties yada yada yada) is \emph{factorisable} (with respect to a given LHV model I suppose?) if
\begin{equation}
	p(x_1,...,x_n|y_1,...,y_n;\lambda)
	= \prod_{i=1}^n p(x_i|y_i;\lambda).
\end{equation}
There is probably a more rigorous way to formalise this in terms of registers etc.
We also might refer to these conditional probability distributions as \emph{response functions}.
We say these are \emph{outcome-indeterministic} when $p\notin\{0,1\}$.

It is worth noting that outcome-determinism implies factorisability, but not \emph{vice-versa}.

\chapter{Information theory}



\section{Entropy and co}

Define the function $\theta(\alpha,\beta)\equiv \alpha\log(\alpha/\beta)$. Then fuck you very much.

\section{Classical source coding}

\begin{prop}
	Let $p\in[0,1]$ and $n,N\in\NN$ be positive integers with $n\le N$.
	Then,
	\begin{equation}
		p^{np} (1-p)^{N-np}
		= 2^{-nH(p)},
	\end{equation}
	where $H(p)\equiv -p\log_2(p)-(1-p)\log_2(1-p)$ is the Shannon entropy of the corresponding probability distribution.
\end{prop}

Consider a source emitting symbols taken from some finite alphabet $X$, and some probability vector $p\in \calP(X)$.
Denote with $X^n$ the set of strings of length $n$ with elements in $X$.

\begin{defn}[Typical strings]
	A string $a_1\cdots a_n\in X^n$ is said to be $\epsilon$-typical with respect to $p$ if
	\begin{equation}
		2^{-n(H(p)+\epsilon)} < p(a_1)\cdots p(a_n) < 2^{-n(H(p)-\epsilon)}.
	\end{equation}
	Denote with $T_{n,\epsilon}(p)\subset X^n$ the set of all such strings.
\end{defn}

The above definition makes precise the idea of typical strings corresponding to probabilities $p(\bs a)\sim 2^{-nH(p)}$.
The definition can be equivalently expressed as requiring that
\begin{equation}
	\left\lvert \frac{1}{n}\log p(\bs a) + H(p) \right\rvert < \epsilon
\end{equation}

\chapter{Continuous variables}
\minitoc

\section{Literature on nonclassicality etc}

\heading{Nonclassicality via the $P$-representation}
\begin{itemize}
	\item \textcite{richter2002nonclassicality} derive nonclassicality criteria via the (double) Fourier transform of $P$. More specifically, they show, leveraging Bochner's theorem, that $P$ is a probability measure \emph{iff} its Fourier transform is ``of positive type''.
	They then provide a hierarchy of conditions that are equivalent to $P$-nonclassicality.
	\item \textcite{wunsche2004about} points out problems in the singularity and behaviour of $P$-functions for bimodal thermal states.
	\item \textcite{korbicz2005hilbert} derive criteria for the negativity of the $P$-function, and relate these to Hilbert's 17th problem.
	They build upon the result of~\parencite{richter2002nonclassicality}, and derive what they claim to be essentially \emph{nonclassicality witnesses} for the $P$ function.
	They end up with the following criterion: \emph{a state $\rho$ with $P$-representation $P$ is nonclassical iff, for every polynomial $v\in\RR[x,y]$, $\int d^2\alpha P v^2\ge0$}.
	They note how their approach is inherently geometrical: their criterion amounts to a characterisation of the classical set via its supporting hyperplanes.
	\item \textcite{miranowicz2015statistical} apply different nonclassicality measures to several classes of states. They also give a nice review of measures of nonclassicality.
	\item \textcite{albarelli2016nonlinearity} show the relations between entropic nonlinearity and ground state nonclassicality, using Wigner and $P$ functions.
\end{itemize}


\heading{Wigner distributions and quantum advantage}
\textcite{galvao2005discrete} characterise a class of discrete Wigner functions, and relate this to stabiliser states.
Similar results are given in~\parencite{cormick2006classicality}.
\textcite{veitch2012negative} show that, in odd-dimensional systems, input states with positive discrete Wigner representation can be simulated efficiently. They also show that the set of states with positive discrete Wigner function strictly contains the set of (convex combinations of) stabilizer states.
\textcite{mari2012positive} show that quantum circuits where the initial state and all the following quantum operations can be represented by positive Wigner functions can be classically efficiently simulated. The result holds both for continuous-variable and finite odd-dimensional systems.
Their result generalises the Gottesman-Knill theorem.

\heading{Detecting nonclassicality via the Wigner function}
\textcite{kenfack2004negativity} mention several methods to detect nonclassicality via distance measures. Then criteria involving phase-space functions.
\textcite{mari2011directly} measure nonclassicality operationally in terms of the distance from a state with positive Wigner function.

\heading{Resource theories of nonclassicality}
\textcite{sperling2015convex} introduce a framework to discuss nonclassicality, and quantify ``quantumness'', in generic scenarios. This is a resource-theory-like formalism, using convexity of classical set etc.

\section{Quadratures and co.}

\begin{defn}[Mode operators]
	Annihilation and creation operators are defined as the operators $a,a^\dagger$ such that $[a,a^\dagger]=1$.
	If multiple modes are considered, we have $[a_j,a_k^\dagger]=\delta_{jk}$ and $[a_j,a_k]=[a_j^\dagger,a_k^\dagger]=0$.
\end{defn}

\begin{prop}
	We have $[a^\dagger a, a] = -a$ and
	$[a^\dagger a, a^\dagger] = a^\dagger$.
\end{prop}

\begin{defn}
	Position and momentum operators are defined as the Hermitian and skew-Hermitian components of $a$, respectively:
	\begin{equation}
		x = \frac{a + a^\dagger}{\sqrt2},
		\qquad
		p = \frac{a - a^\dagger}{\sqrt2 i}.
	\end{equation}
\end{defn}
With these definitions, the operators satisfy $[x,p]=i$, and $a$ is retrieved as
$a = \frac{x + ip}{\sqrt2}$.

It is worth noting that $a,a^\dagger$ cannot actually be represented as operators in some underlying Hilbert space $\calH$. One way to see this is to take the trace of the relation $[a,a^\dagger]=1$.
The RHS is the identity operator in the underlying space, whose trace equals the dimension of the space itself, whereas $\Tr([A,B])=0$ for any $A,B$.

\begin{defn}[General quadratures]
	More generally, we define the \emph{quadrature operators} $q(\phi)$ from the mode operators --- or equivalently, from the position and momentum operators --- as
	\begin{equation}
		q(\phi) = \cos(\phi) x + \sin(\phi) p
		= \frac{1}{\sqrt2}(a e^{-i\phi} + a^\dagger e^{i\phi}).
	\end{equation}
	The definition is straightforwardly generalised to multimode spaces upon replacing $q\to q_k, a\to a_k, x\to x_k,p\to p_k$.
\end{defn}

\begin{prop}
	The quadrature operators $q(\phi)$ satisfy
	\begin{equation}
		[q(\theta), q(\phi)] = i \sin(\phi-\theta).
	\end{equation}
\end{prop}

\subsection{Multimode quadratures}

One way to handle multimode operators is to bundle them together, defining the column vector
$\bs r \equiv (x_1, p_1, x_2, p_2, ..., x_n, p_n)^T$.
More explicitly, we define $\bs r$ as a vector of $2n$ operators with components $r_{2k}=x_k$ and $r_{2k+1}=p_k$.
With this definition, we have the commutation relations:
\begin{prop}
	$[r_k, r_\ell] = i\Omega_{k\ell},$
	or more briefly, $[\bs r,\bs r] = i\Omega$,
	where the \emph{symplectic matrix} $\Omega$ is
	\begin{equation}
		\Omega = \bigoplus_{k=1}^{n} (i\sigma_y)
		= I_n \otimes (i\sigma_y),
		\qquad
		\text{where }
		\qquad
		i\sigma_y\equiv \begin{pmatrix}
			0 & 1\\-1& 0
		\end{pmatrix}.
	\end{equation}
\end{prop}
\begin{proof}
	The only nonvanishing commutators between elements of $\bs r$ are
	\begin{equation}
		[r_{2k}, r_{2k+1}] = [x_k, p_k] = i.
	\end{equation}
\end{proof}


\section{Covariance matrices and co}

\begin{defn}[Covariance matrix]
	Given a state $\rho\in\rmD(\calH)$,
	we define its \emph{covariance matrix} $\sigma(\rho)$ as
	\begin{equation}
		\sigma(\rho)_{k\ell}
		= \frac12 \langle \{ r_k, r_\ell\}\rangle_\rho
		- \langle r_k\rangle_\rho \langle r_\ell\rangle_\rho.
	\end{equation}
\end{defn}

\begin{prop}
	The diagonal elements of $\sigma(\rho)$ are the variances of the quadrature operators.
\end{prop}
\begin{proof}
	% \begin{equation}
	% 	\sigma(\rho)_{kk}
	% 	= \langle r_k^2\rangle_\rho - \langle r_k\rangle_\rho^2.
	% \end{equation}
	$\sigma(\rho)_{kk} = \langle r_k^2\rangle_\rho - \langle r_k\rangle_\rho^2.$
\end{proof}

\begin{prop}
	The covariance matrix is \emph{real, symmetric, positive definite}, and such that $\sigma+\frac{i}{2}\Omega\ge0$.
\end{prop}

\section{Wigner functions}

\begin{defn}
	Given $\rho\in\rmD(\calH^{\otimes n})$ (an $n$-mode state), its \emph{Wigner function} is the map $W_\rho:\RR^{n}\times\RR^n\to \RR$ defined as
	\begin{equation}
		W_\rho(\bs x,\bs p)
		= \int_{\RR^n} \frac{d\bs q}{(2\pi)^n}
		\mel{\bs x+\frac12\bs q}{\rho}{\bs x-\frac12 \bs q}
		e^{i \bs p\cdot\bs q},
	\end{equation}
	where $d\bs q\equiv \prod_{k=1}^n dq_k$.
\end{defn}

\begin{defn}
	The \emph{characteristic function} of $\rho$ is defined as
	\begin{equation}
		\chi_\rho(\bs\alpha)
		= \expval{D(\bs\alpha)}_\rho
		\equiv \Tr[\rho D(\bs\alpha)],
	\end{equation}
	where $D(\bs\alpha)\equiv \otimes_{k=1}^n D_k(\alpha_k)$.
\end{defn}

The characteristic function can be defined more generally as
\begin{equation}
	\chi_\rho^s(\bs\xi) = \Tr[\rho \hat D(\bs\xi) e^{\frac{s}{2}|\bs\xi|^2}].
\end{equation}
The Wigner then corresponds to some Fourier transform of $\chi_\rho^s$ for $s=0$, or something like that.

\section{Some formulas}

\begin{equation}
	(a^\dagger)^n \ket m 
	= \sqrt{\frac{(m+n)!}{m!}} \ket{m+n}.
\end{equation}

\begin{equation}
	a^n \ket m = \sqrt{\frac{m!}{(m-n)!}} \ket{m-n},\quad n\le m.
\end{equation}

\section{Displacement operators}

\begin{defn}[Displacement operators]
	$\hat D(\alpha) \equiv \exp(\alpha \hat a^\dagger-\bar\alpha \hat a)$ for $\alpha\in\CC$.
\end{defn}
Observe that $\hat D^\dagger(\alpha)=\hat D(-\alpha)$.

\begin{prop}
	\begin{equation}
		D(\alpha)
		= e^{-\lvert\alpha\rvert^2/2}
			\exp(\alpha a^\dagger)\exp(-\bar\alpha a).
	\end{equation}
\end{prop}

\begin{prop}
	In terms of position and momentum operators, the displacement operator reads
	\begin{equation}
		\hat D(\alpha)
		= \exp[\sqrt2 i(\alpha_I \hat x - \alpha_R \hat p)],
	\end{equation}
	where $\alpha\equiv \alpha_R+i\alpha_I$.
	In matrix notation, this can also be written as
	\begin{equation}
		\hat D(\alpha)
		= \exp\left[\sqrt2 i \begin{pmatrix}
			\hat x & \hat p
		\end{pmatrix}
		\begin{pmatrix}
			0 & 1 \\ -1 & 0
		\end{pmatrix}
		\begin{pmatrix}
			\alpha_R \\ \alpha_I
		\end{pmatrix}\right]
		\equiv \exp(\sqrt2 i\, \hat z^T \Omega \vec\alpha),
	\end{equation}
	where $\Omega\equiv i\sigma_y$ is the \emph{symplectic matrix}, we defined the vector of operators $\hat z\equiv (\hat x,\hat p)^T$, and we denotes with $\vec \alpha$ the length-$2$ vector with components real and imaginary parts of $\alpha\in\CC$.
\end{prop}
\begin{proof}
	Notice that
	\begin{equation}
		\alpha a^\dagger - \bar\alpha a
		= \alpha \left(\frac{\hat x-i\hat p}{\sqrt2}\right)
		- \bar\alpha\left(\frac{\hat x + i\hat p}{\sqrt2}\right)
		= \left(\frac{\alpha-\bar\alpha}{\sqrt2}\right) \hat x
		+ \left(\frac{\alpha+\bar\alpha}{\sqrt2 i}\right) \hat p.
	\end{equation}
\end{proof}

\begin{prop}
	Displacement operators acting on the vacuum state generate coherent states:
	\begin{equation}
		D(\alpha)\ket0 = e^{-|\alpha|^2/2}\exp(\alpha a^\dagger)\ket0
		= \ket\alpha
		\equiv e^{-|\alpha|^2/2} \sum_{k=0}^\infty
			\frac{\alpha^k}{\sqrt{k!}} \ket k.
	\end{equation}
\end{prop}

\subsection{Multimode displacement operators}

\begin{defn}
	For multimode states, we defined the \emph{single-mode} displacement operators as
	$D_k(\alpha)\equiv\exp(\alpha a^\dagger_k-\bar\alpha a_k)$,
	and the \emph{joint} displacement operator on $n$ modes as
	\begin{equation}
		D(\bs\alpha)
		= D_1(\alpha_1)\otimes\cdots\otimes D_n(\alpha_n),
		\qquad \bs\alpha\equiv (\alpha_1,...,\alpha_n).
	\end{equation}
\end{defn}

\subsection{Displacement operators on Fock states}

\begin{prop}
	% \begin{equation}
		$D(\alpha) a D^\dagger(\alpha) = a - \alpha,
				\qquad D^\dagger(\alpha) a D(\alpha) = a + \alpha$
	% \end{equation}
\end{prop}
\begin{proof}
	Write $D(\alpha)=\exp(\calD)$ with $\calD\equiv \alpha a^\dagger-\bar\alpha a$.
	Then
	\begin{equation}
		D(\alpha) a D^\dagger(\alpha)
		= e^\calD a e^{-\calD}
		= \exp(\on{ad}(\calD)) a
		= a + [\calD,a] = a-\alpha,
	\end{equation}
	where we used the fact that $[\calD,a]=-\alpha$, and thus any other commutator term arising from $\exp(\on{ad}(\calD))a$ vanishes.
\end{proof}

\begin{prop}
	\begin{equation}
	\begin{gathered}
		D(\alpha)\ket1
		% = e^{-|\alpha|^2/2}
		% 	\sum_{k=0}^\infty \frac{\alpha^{k-1}}{\sqrt{k!}}\left(
		% 	k - |\alpha|^2
		% 	\right) \ket k \\
		= e^{-|\alpha|^2/2}
			\sum_{k=0}^\infty \frac{\alpha^{k}}{\sqrt{k!}}
			\left( \frac{ k - |\alpha|^2 }{ \alpha } \right) \ket k
	\end{gathered}
	\end{equation}
	\label{prop:displacement_action_on_1}
\end{prop}
\begin{proof}
	We have
	\begin{equation}
	\begin{gathered}
		D(\alpha) \ket 1
		= e^{-|\alpha|^2/2}
		  \exp(\alpha a^\dagger) (\ket1 - \bar\alpha \ket0) \\
		= e^{-|\alpha|^2/2}
		  \sum_{k=0}^\infty \frac{\alpha^k}{k!} \left[
		  	\sqrt{(k+1)!}\ket{k+1}
		  	- \bar\alpha \sqrt{k!}\ket k
		  \right]
	\end{gathered}
	\end{equation}
	and therefore
	\begin{equation}
		D(\alpha)\ket1
		= e^{-|\alpha|^2/2} \left[
			-\bar\alpha \ket 0
			+ \sum_{k=1}^\infty \frac{\alpha^{k-1}}{\sqrt{k!}}\left(
			k - |\alpha|^2
			\right) \ket k
		\right],
	\end{equation}
	which can be can also be written as in the proposition.
\end{proof}
\begin{proof}
	We can also observe that $D(\alpha)\ket1=D(\alpha) a^\dagger D(\alpha)\ket\alpha=(a^\dagger-\bar\alpha)\ket\alpha$, which leads to the same result.
\end{proof}

\begin{prop}
	\begin{equation}
		D(\alpha) \ket \ell
		= e^{-|\alpha|^2/2}
		\sum_{s=0}^\infty \frac{\alpha^{s-\ell}}{\sqrt{\ell!s!}}
		\left(
		\sum_{k=0}^{\min(s,\ell)}
		(-1)^{\ell-k}|\alpha|^{2(\ell-k)} k!\binom{\ell}{k}\binom{s}{k}
		\right)
		\ket{s}
	\end{equation}
\end{prop}

\begin{proof}
	\begin{equation}
	\begin{gathered}
		D(\alpha) \ket \ell
		= e^{-|\alpha|^2/2}
		\exp(\alpha a^\dagger) 
		\sum_{k=0}^\ell \frac{(-\bar\alpha)^k}{k!} a^k \ket\ell \\
		= e^{-|\alpha|^2/2}
		\exp(\alpha a^\dagger) 
		\sum_{k=0}^\ell \frac{(-\bar\alpha)^k}{k!} \sqrt{\frac{\ell!}{(\ell-k)!}} \ket{\ell-k} \\
		= e^{-|\alpha|^2/2}
		\exp(\alpha a^\dagger) 
		\sum_{k=0}^\ell
		\frac{(-\bar\alpha)^{\ell-k}}{(\ell-k)!} \sqrt{\frac{\ell!}{k!}} \ket{k} \\
		= e^{-|\alpha|^2/2}
		\sum_{k=0}^\ell\sum_{j=0}^\infty
		\frac{(-\bar\alpha)^{\ell-k} \alpha^j}{j!(\ell-k)!} \sqrt{\frac{\ell!(k+j)!}{k!^2}} \ket{k+j} \\
		= e^{-|\alpha|^2/2}
		\sum_{s=0}^\infty \left(
			\sum_{k=0}^{\min(s,\ell)}
			\frac{(-\bar\alpha)^{\ell-k} \alpha^{s-k}}{(s-k)!(\ell-k)!} \sqrt{\frac{\ell!s!}{k!^2}} 
		\right) \ket{s} \\
		= e^{-|\alpha|^2/2}
		\sum_{s=0}^\infty \frac{\alpha^{s-\ell}}{\sqrt{\ell!s!}}
		\left(
		\sum_{k=0}^{\min(s,\ell)}
		(-1)^{\ell-k}|\alpha|^{2(\ell-k)} k!\binom{\ell}{k}\binom{s}{k}
		\right)
		\ket{s} 
	\end{gathered}
	\end{equation}
\end{proof}
\begin{proof}
	Same result is obtained observing that
	$D(\alpha)\ket\ell=\frac{1}{\sqrt{\ell!}}(a^\dagger-\bar\alpha)^\ell \ket\alpha$.
\end{proof}

\begin{example}
	\begin{equation}
		D(\alpha) \ket2
		= e^{-\mu/2} \sum_{s=0}^\infty
		\frac{\alpha^{s-2}}{\sqrt{2 s!}}
		\left(
		\mu^2 -2s \mu + 2\binom{s}{2}
		\right) \ket s,
	\end{equation}
	where $\mu\equiv\lvert\alpha\rvert^2$ and $\binom{s}{2}$ is taken to be $0$ when $s<2$.
\end{example}

\begin{prop}
	\begin{equation}
		D(\alpha)\PP_0 D^\dagger(\alpha) = \ketbra\alpha \equiv \PP_\alpha.
	\end{equation}
	\begin{equation}
		D(\alpha)\PP_1 D^\dagger(\alpha)
		= ...
	\end{equation}
\end{prop}

\section{Linear evolution}

\begin{prop}
	Consider evolution through a linear interferometer described by the unitary $\calU$. We then have
	\begin{equation}
		\ket k \equiv \frac{(a^\dagger)^k}{\sqrt{k!}} \ket 0
		\xrightarrow{\calU} \sum_{\calJ\in[0...k]^n : |\calJ|=k}
		\binom{k}{\calJ_1\cdots \calJ_n}^{1/2}
		\bs c_{\calJ}
		\ket{\calJ_1\cdots \calJ_n}.
	\end{equation}
\end{prop}

\begin{proof}
	\begin{equation}
	\begin{gathered}
		\varphi(\calU) \ket k
		= \frac{1}{\sqrt{k!}}(\calU a^\dagger \calU^\dagger)^k \ket0
		= \frac{1}{\sqrt{k!}}
		\left(
			\sum_{j=1}^n c_j a_j^\dagger
		\right)^k \ket 0 \\
		= \frac{1}{\sqrt{k!}}
		\sum_{J\in [n]^k} \left(\prod_{\ell=1}^k c_{J_\ell} a_{J_\ell}^\dagger\right) \ket0 \\
		= \frac{1}{\sqrt{k!}}
		\sum_{\calJ\in[0...k]^n : |\calJ|=k}
		\binom{k}{\calJ_1\cdots \calJ_n}
		\prod_{\ell=1}^n (c_\ell a_{\ell}^\dagger)^{\calJ_\ell} \ket0 \\
		= \sum_{\calJ\in[0...k]^n : |\calJ|=k}
		\binom{k}{\calJ_1\cdots \calJ_n}^{1/2}
		\bs c_\calJ \ket{\calJ_1\cdots \calJ_n}.
	\end{gathered}
	\end{equation}
\end{proof}

\begin{example}
	For a two-output interferometer (\emph{i.e.} a beamsplitter),
	\begin{equation}
		\ket k \to
		\sum_{j=0}^k \binom{k}{j}^{1/2} t^j r^{k-j} \ket{j,k-j}.
	\end{equation}
	Looking only at the first output port (that is, partial tracing the second mode), we get
	\begin{equation}
		\ket k \to
		\sum_{j=0}^k \binom{k}{j} T^j (1-T)^{k-j} \PP_j.
	\end{equation}
\end{example}

\begin{example}
	Denote with $\calE_t$ the channel describing linear evolution through a beamsplitter with transmissivity $t$ followed by a partial trace with respect to the second output mode. We have
	\begin{equation}
		\calE_t(\ketbra{j}{j+1})
		= \sum_{k=0}^j \binom{j}{k} \frac{j+1}{k+1}
		\bar t T^k (1-T)^{j-k} \ketbra{k}{k+1}.
	\end{equation}
	This follows from
	\begin{equation}
		\ketbra{j}{k} \to
		\sum_{m=0}^j \sum_{n=0}^k
		\binom{j}{m}^{1/2} \binom{k}{n}^{1/2}
		t^{m} \bar t^n r^{j-m} \bar r^{k-n}
		\ketbra{m,j-m}{n,k-n},
	\end{equation}
	and thus
	\begin{equation}
		\calE_t(\ketbra{j}{k})
		= \sum_{m=0}^j \binom{j}{m}^{1/2} \binom{k}{k+m-j}^{1/2}
		T^m R^{j-m} \bar t^{k-j}
		\ketbra{m}{m+k-j}.
	\end{equation}
\end{example}

\begin{example}
	% \begin{equation}
		$\calE_t(\ketbra{0}{n})
				= \bar t^n \ketbra{0}{n}.$
	% \end{equation}
\end{example}

\section{Thermalised states}

\begin{defn}
	Given a state $\rho$, we define the corresponding \emph{thermalised state with average occupation number} $\bar n$ as
	\begin{equation}
		\Phi_{\bar n}^{\rm th}(\rho)
		\equiv \int d\mu \frac{e^{-\mu/\bar n}}{\bar n}
		\int_0^{2\pi} \frac{d\phi}{2\pi}
		D(\sqrt\mu e^{i\phi})\rho D^\dagger(\sqrt\mu e^{i\phi}).
	\end{equation}
\end{defn}

\begin{prop}\label{prop:integration_formula_for_thermalisation}
	\begin{equation}
		\int_0^\infty d\mu \,\, \frac{e^{-\mu/\bar n}}{\bar n}
		(e^{-\mu}\mu^j)
		= \frac{\bar n^j}{(\bar n+1)^{j+1}} j!
	\end{equation}
\end{prop}

\begin{prop}\label{prop:thermalisation_applied_to_00}
	\begin{equation}
		\Phi^{\rm th}_{\bar n}(\PP_0)
		= \sum_{j=0}^\infty \frac{\bar n^j}{(1+\bar n)^{1+j}} \PP_j.
		% = \sum_{j,k=0}^\infty
		% 	\frac{((j+k)/2)!}{\sqrt{j!k!}}
		% 	\frac{1}{\bar n} \left(\frac{\bar n}{\bar n+1}\right)^{1+(j+k)/2}
		% 	\ketbra{j}{k}
	\end{equation}
\end{prop}
\begin{proof}
	The displacement operators act on the vacuum state as
	\begin{equation}
		\hat D(\sqrt\mu e^{i\phi})\ketbra0 \hat D^\dagger(\sqrt\mu e^{i\phi})
		= e^{-\mu} \sum_{j,k=0}^\infty \frac{\mu^{(j+k)/2}}{\sqrt{j!k!}} e^{i\phi(j-k)} \ketbra{j}{k}.
	\end{equation}
	Integrating over the phases $\phi$, using $\int_0^{2\pi}d\phi e^{i\phi j}=2\pi \delta_{j,0}$, we get
	\begin{equation}
		\int_0^{2\pi} \frac{d\phi}{2\pi}
		\hat D(\sqrt\mu e^{i\phi})\ketbra0 \hat D^\dagger(\sqrt\mu e^{i\phi})
		= e^{-\mu} \sum_{j=0}^\infty \frac{\mu^j}{j!} \ketbra j.
	\end{equation}
	Integrating over $\mu$ and introducing the additional factors $e^{-\mu/\bar n}/\bar n$, we then reach the conclusion.
\end{proof}

\begin{prop}\label{prop:thermalisation_applied_to_11}
	\begin{equation}
		\Phi^{\rm th}_{\bar n}(\PP_1)
		= \sum_{j=0}^\infty
			\frac{ \bar n^j }{ (\bar n+1)^{j+1} }
			\frac{ (j+ \bar n^2) }{ \bar n(\bar n + 1) }
			\PP_j.
	\end{equation}
	\label{prop:thermalisation_of_P1}
\end{prop}
\begin{proof}
	The action of $\hat D$ on $\ket1$ gives
	\begin{equation}
		\hat D(\sqrt\mu e^{i\phi})\ketbra1 \hat D^\dagger(\sqrt\mu e^{i\phi})
		= e^{-\mu} \sum_{j,k=0}^\infty \frac{\mu^{(j+k)/2}}{\sqrt{j!k!}} e^{i\phi(j-k)}
		\left(\frac{j-\mu}{\sqrt\mu e^{i\phi}}\right)
		\left(\frac{k-\mu}{\sqrt\mu e^{-i\phi}}\right)
		\ketbra{j}{k}.
	\end{equation}
	Integrating over the phases we get
	\begin{equation}
		\int_0^{2\pi} \frac{d\phi}{2\pi}
		\hat D(\sqrt\mu e^{i\phi})\ketbra1 \hat D^\dagger(\sqrt\mu e^{i\phi})
		= e^{-\mu} \sum_{j=0}^\infty \frac{\mu^{j-1}}{j!} (j-\mu)^2 \PP_j.
	\end{equation}
	Therefore,
	\begin{equation}
	\begin{gathered}
		\Phi_{\bar n}^{\rm th}(\PP_1)
		= \sum_{j=0}^\infty \frac{1}{j!\bar n}\PP_j \int_0^\infty d\mu
		e^{-\mu} \mu^{j-1} (j-\mu)^2 e^{-\mu/\bar n} \\
		= \sum_{j=0}^\infty \frac{1}{j!\bar n}\PP_j
		C^{-j-2} \int_0^\infty d\mu
		e^{-\mu} \mu^{j-1} (C j-\mu)^2 \\
		= \sum_{j=0}^\infty \frac{1}{j!\bar n}\PP_j
		C^{-j-2} \left[
			C^2 j^2 (j-1)! - 2C j j! + (j+1)!
		\right],
	\end{gathered}
	\end{equation}
	where $C\equiv 1+1/\bar n$.
	We reach the conclusion by simply inserting the explicit formula for $C$ in the above expression.
\end{proof}

\begin{prop}
	\begin{equation}
		\Phi_{\bar n}^{\rm th}(\PP_2)
		= \sum_{j=0}^\infty
		\frac{\bar n^j}{(\bar n + 1)^{j+1}}
		\frac{
			2\bar n^4 + 4j \bar n^2 + j(j-1)
		}{2 \bar n^2 (\bar n + 1)^2}
		\,\, \PP_j
	\end{equation}
\end{prop}
\begin{proof}
	We have
	\begin{equation}
		\int_0^{2\pi} \frac{d\phi}{2\pi}
		\hat D(\sqrt\mu e^{i\phi})\ketbra2 \hat D^\dagger(\sqrt\mu e^{i\phi})
		= e^{-\mu}
		\sum_{j=0}^\infty \frac{\mu^{j-2}}{2j!}
		(\mu^2 -2j \mu + j(j-1))^2
		\,\,\PP_j.
	\end{equation}
	Performing the average over $\mu$ we then obtain the result.
	% \begin{equation}
	% 	\frac{1}{2j!} \left(
	% 	\frac{\bar n^j}{(\bar n+1)^{j+1}} j!
	% 	- 2j \frac{\bar n^{j-1}}{(\bar n+1)^{j}} (j-1)!
	% 	+ j(j-1) \frac{\bar n^{j-2}}{(\bar n+1)^{j-1}} (j-2)!
	% 	\right)
	% \end{equation}
	% \begin{equation}
	% \begin{gathered}
	% 	\sum_{j=0}^\infty
	% 	\frac{\bar n^j}{(\bar n + 1)^{j+1}}
	% 	\frac{
	% 		2\bar n^4 + 4j \bar n^2 + j(j-1)
	% 	}{2 \bar n^2 (\bar n + 1)^2}
	% 	\,\, \PP j
	% \end{gathered}
	% \end{equation}
\end{proof}

\begin{prop}
	\begin{equation}
		\Phi_{\bar n}^{\rm th}(\PP_\ell) = ?
	\end{equation}
\end{prop}
\begin{proof}
	We can write
	\begin{equation}
		\hat D(\alpha) \ket\ell = e^{-\mu/2}
		\sum_{s=0}^\infty \alpha^{s-\ell}
		\sum_{k=0}^{\min(s,\ell)} \mu^{\ell-k} c_{s,\ell,k},
	\end{equation}
	where
	\begin{equation}
		c_{s,\ell,k} \equiv
		\frac{1}{\sqrt{\ell! s!}} (-1)^{\ell-k}
		k! \binom{\ell}{k} \binom{s}{k}.
	\end{equation}
	Therefore
	\begin{equation}
	\begin{gathered}
		\int_0^{2\pi}\frac{d\phi}{2\pi} \,
		\hat D\ketbra{\ell}{\ell} \hat D^\dagger =
		e^{-\mu} \sum_{s=0}^\infty \mu^{s-\ell}
		\left(
			\sum_{k=0}^{\min(s,\ell)} \mu^{\ell-k} c_{s,\ell,k}
		\right)^2
		\ketbra{s}{s}.
	\end{gathered}
	\end{equation}
\end{proof}

\begin{prop}
	\begin{equation}
		\Phi_{\bar n}^{\rm th}(\ketbra{j}{k}) = ?
	\end{equation}
\end{prop}
\begin{proof}
	Let us write the action of $\hat D\equiv \hat D(\alpha)$ on $\ket\ell$ as
	\begin{equation}
		\hat D\ket \ell = e^{-\mu/2}\sum_{s=0}^\infty
		\alpha^{s-\ell} c_{s,\ell,\mu} \ket s,
	\end{equation}
	where
	\begin{equation}
		c_{s,\ell,\mu} \equiv \frac{1}{\sqrt{\ell! s!}}
		\sum_{k=0}^{\min(s,\ell)} (-\mu)^{\ell-k}
		k! \binom{\ell}{k} \binom{s}{k}.
	\end{equation}
	This expression has the advantage of isolating the phase dependence in the $\alpha^{s-\ell}$ terms. We can then readily compute the phase average we need:
	\begin{equation}
		\int_0^{2\pi} \frac{d\phi}{2\pi}
		\hat D\ketbra{j}{k} \hat D^\dagger =
		e^{-\mu} \sum_{s=0}^\infty \mu^{s-j}
		c_{s,j,\mu} c_{s+k-j,k,\mu}
		\ketbra{s}{s+(k-j)}.
	\end{equation}
	To isolate the $\mu$ dependence, write the coefficients as
	\begin{equation}
		c_{s,\ell,\mu} = \sum_{k=0}^{\min(s,\ell)} \mu^{\ell-k} C_{s,\ell,k},
		\qquad
		C_{s,\ell,k} \equiv (-1)^{\ell -k}
		\frac{k!}{\sqrt{\ell!s!}} \binom{\ell}{k} \binom{s}{k}.
	\end{equation}
	We then have
	\begin{equation}
		\ketbra{j}{k} \to 
		e^{-\mu} \sum_{s=0}^\infty
		\sum_{m=0}^{\min(s,j)} \sum_{n=0}^{\min(s+k-j,k)}
		\mu^{s+k - m - n} C_{s,j,m} C_{s+k-j,k,n}
		\ketbra{s}{s+k-j}.
	\end{equation}
\end{proof}

\begin{prop}\label{prop:thermalisation_applied_to_01}
	\begin{equation}
		\Phi^{\rm th}_{\bar n}(\ketbra{0}{1})
		= \sum_{j=0}^\infty
		% \frac{\bar n^{j}}{(1+\bar n)^{j+2}} \sqrt{j+1}
		% \ketbra{j}{j+1}.
		\frac{\bar n^{j}}{(1+\bar n)^{j+1}}
		\frac{\sqrt{j+1}}{\bar n + 1}
		\ketbra{j}{j+1}.
	\end{equation}
\end{prop}

\begin{proof}
	Using~\cref{prop:displacement_action_on_1}, we see that
	\begin{equation}
		\hat D(\sqrt\mu e^{i\phi})\ketbra{0}{1} \hat D^\dagger(\sqrt\mu e^{i\phi})
		= e^{-\mu} \sum_{j,k=0}^\infty
		\frac{\mu^{(j+k-1)/2}}{\sqrt{j!k!}}
		e^{i\phi(j-k+1)} (k-\mu) \ketbra{j}{k}.
	\end{equation}
	Integrating over the phases, we get a term $\delta_{k,j+1}$, which results in
	\begin{equation}
		\int_0^{2\pi} \frac{d\phi}{2\pi}
		\hat D(\sqrt\mu e^{i\phi})\ketbra{0}{1} \hat D^\dagger(\sqrt\mu e^{i\phi})
		= e^{-\mu} \sum_{j=0}^\infty
		\frac{\mu^j}{j!\sqrt{j+1}} (j+1-\mu) \ketbra{j}{j+1}.
	\end{equation}
	And finally,
	\begin{equation}
	\begin{gathered}
		\Phi^{\rm th}_{\bar n}(\ketbra{0}{1})
		= \sum_{j=0}^\infty \frac{1}{j!\sqrt{j+1}} \ketbra{j}{j+1} \frac{1}{\bar n}
		\int_0^\infty d\mu
		e^{-\mu/\bar n} e^{-\mu} \mu^j (j+1-\mu) \\
		= \sum_j \ketbra{j}{j+1} \frac{C^{-j-2}}{j!\sqrt{j+1}\bar n}
		\int d\mu \, e^{-\mu} \mu^j (C(j+1) - \mu) \\
		= \sum_{j=0}^\infty \ketbra{j}{j+1} \frac{C^{-j-2}}{j!\sqrt{j+1}\bar n}
		(C j! (j+1) - (j+1)!) \\
		= \sum_{j=0}^\infty \ketbra{j}{j+1} \frac{C^{-j-2}}{\bar n} \sqrt{j+1} (C-1)
		= \sum_{j=0}^\infty \ketbra{j}{j+1}
			\frac{\bar n^{j}}{(1+\bar n)^{j+2}} \sqrt{j+1}.
	\end{gathered}
	\end{equation}
\end{proof}

\begin{prop}\label{prop:thermalisation_applied_to_02}
	\begin{equation}
		\Phi_{\bar n}^{\rm th}(\ketbra{0}{2})
		= \sum_{j=0}^\infty
		\frac{\bar n^j}{(\bar n + 1)^{j+1}}
		\binom{j+2}{2}^{1/2}
		\frac{1}{(\bar n + 1)^2}
		\ketbra{j}{j+2}.
	\end{equation}
\end{prop}
\begin{proof}
	Remember that
	\begin{equation}\scalebox{0.92}{$\displaystyle
		\hat D\ket0 = e^{-\mu/2} \sum_{s=0}^\infty \frac{\alpha^s}{\sqrt{s!}} \ket s,
		\qquad
		\hat D\ket2 = e^{-\mu/2} \sum_{s=0}^\infty \frac{\alpha^{s-2}}{\sqrt2\sqrt{s!}}
		\left(\mu^2 - 2s \mu + s(s-1)\right) \ket s.
	$}\end{equation}
	\begin{equation}
	\begin{gathered}
		\int_0^{2\pi} \frac{d\phi}{2\pi}
		\hat D \ketbra{0}{2} \hat D^\dagger
		= e^{-\mu} \sum_{j=0}^\infty
		% \frac{\mu^j}{\sqrt{2(j+2)!}}
		% (\mu^2 - 2 (j+2) \mu + (j+2)(j+1))
		\mu^j\frac{(\mu^2 - 2 (j+2) \mu + (j+2)(j+1))}{\sqrt{2\,\,j!(j+2)!}}
		\ketbra{j}{j+2}.
	\end{gathered}
	\end{equation}
	The result is obtained integrating over $\mu$, using~\cref{prop:integration_formula_for_thermalisation}.
\end{proof}

\begin{prop}\label{prop:thermalisation_applied_to_0ell}
	\begin{equation}
		\Phi_{\bar n}^{\rm th}(\ketbra{0}{\ell}) =
		\sum_{s=0}^\infty
		\frac{\bar n^s}{(\bar n + 1)^{s+1}}
		\binom{\ell+s}{s}^{1/2}
		\frac{1}{(\bar n + 1)^\ell}
		\,\,\ketbra{s}{s+\ell}.
	\end{equation}
\end{prop}
\begin{proof}
	We have the general relation
	\begin{equation}
	\begin{gathered}
		\int_0^{2\pi} \frac{d\phi}{2\pi}
		\hat D \ketbra{0}{\ell} \hat D^\dagger =
		e^{-\mu} \sum_{s=0}^\infty
		\sum_{k=0}^\ell \mu^{s+\ell-k} c_{s,\ell,k}
		\ketbra{s}{s+\ell},
	\end{gathered}
	\end{equation}
	where
	\begin{equation}
		c_{s,\ell,k} \equiv
		\frac{1}{\sqrt{s!\ell!(s+\ell)!}} (-1)^{\ell-k}
		k! \binom{\ell}{k} \binom{s+\ell}{k}.
	\end{equation}
	Integrating over $\mu$ we thus get, using~\cref{prop:integration_formula_for_thermalisation},
	\begin{equation}
	\begin{gathered}
		\Phi_{\bar n}^{\rm th}(\ketbra{0}{\ell}) =
		\sum_{s=0}^\infty \sum_{k=0}^\ell
		\frac{\bar n^{s+\ell-k}}{(\bar n + 1)^{s+\ell-k+1}} (s+\ell-k)! c_{s,\ell,k} \ketbra{s}{s+\ell} \\
		= \sum_{s=0}^\infty
		\frac{\bar n^s}{(\bar n + 1)^{s+1}}
		\binom{\ell}{s}^{1/2}
		\frac{1}{(\bar n + 1)^\ell}.
	\end{gathered}
	\end{equation}
\end{proof}


% ----- old shitty (and wrong) "proof"
% \begin{proof}
% 	\begin{equation}
% 	\begin{gathered}
% 		\Phi^{\rm th}_{\bar n}(\ketbra{0}{1})
% 		= \int_0^\infty d\mu \frac{e^{-\mu/\bar n}}{\bar n}
% 		\sum_{j,k=0}^\infty \ketbra{j}{k}
% 		e^{-\mu} \frac{1}{\sqrt{j!k!}}
% 		\mu^{\frac{j+k}{2} - \frac12} (k-\mu) \\
% 		= \sum_{j,k=0}^\infty \ketbra{j}{k}
% 		\frac{C^{(-\frac{j+k}{2}-\frac32)}}{\bar n\sqrt{j!k!}}
% 		\int_0^\infty dx
% 			e^{-x} x^{(\frac{j+k}{2}-\frac12)}
% 			(Ck - x) \\
% 		= \sum_{j,k=0}^\infty \ketbra{j}{k}
% 		\frac{1}{\bar n\sqrt{j!k!}}
% 		\left(\frac{\bar n}{\bar n + 1}\right)^{(j+k+3)/2}
% 		\Gamma\left(\frac{j+k+1}{2}\right)
% 		\left(
% 			Ck - \frac{j+k+1}{2}
% 		\right) \\
% 		= \sum_{j,k=0}^\infty \ketbra{j}{k}
% 		\frac{1}{\bar n\sqrt{j!k!}}
% 		\left(\frac{\bar n}{\bar n + 1}\right)^{(j+k+3)/2}
% 		\Gamma\left(\frac{j+k+1}{2}\right)
% 		\left(
% 			Ck - \frac{j+k+1}{2}
% 		\right) \\
% 		= \sum_{j,k=0}^\infty \ketbra{j}{k}
% 		\frac{1}{\bar n\sqrt{j!k!}}
% 		\left(\frac{\bar n}{\bar n + 1}\right)^{(j+k+3)/2}
% 		\Gamma\left(\frac{j+k+1}{2}\right)
% 		\left(
% 			\frac{ \bar n(k- j) + 2k - \bar n }{ 2\bar n }
% 		\right).
% 	\end{gathered}
% 	\end{equation}
% 	Thus in particular
% 	\begin{equation}
% 	\begin{gathered}
% 		\langle j| \Phi^{\rm th}_{\bar n}(\ketbra{0}{1}) |j\rangle
% 		= \frac{1}{\bar n j!}
% 		\left(\frac{ \bar n }{ \bar n + 1 }\right)^{j + 3/2}
% 		(j+1/2)!
% 		\left( \frac{ 2j - \bar n }{ 2\bar n } \right) \\
% 		=
% 		\frac{ \bar n^{ j - 1/2 } }{ (\bar n + 1)^{ j + 3/2 } }
% 		\frac{ (j+1/2)! }{ j! }
% 		\left( \frac{ 2j - \bar n }{ 2 } \right)
% 	\end{gathered}
% 	\end{equation}
% \end{proof}

\subsection{Attenuation and then thermalisation}

\begin{prop}
	Let $\Phi_t$ be the channel describing attenuation through a beamsplitter with transmissivity $t$, and $\Phi_{\bar n}^{\rm th}$ be the thermalisation operation. Then
	\begin{equation}
	\Phi^{\rm th}_{\bar n}(\Phi_t(\PP_0))
	= \Phi^{\rm th}_{\bar n}(\PP_0)
	= \rho^{\rm th}_{\bar n}
	\equiv \sum_{j=0}^\infty \frac{\bar n^j}{(\bar n + 1)^{j+1}} \PP_j.
	\end{equation}
\end{prop}

\begin{prop}
	Let $\Phi_t$ the channel corresponding to attenuation through a beamsplitter with transmissivity $t$, and $\Phi_{\bar n}^{\rm th}$ the thermalisation operation. Then
	\begin{equation}
		\Phi_t(\PP_1)
		= (1-T) \PP_0 + T \PP_1,
	\end{equation}
	where $T\equiv|t|^2$, and
	\begin{equation}
		\Phi_{\bar n}^{\rm th}(\Phi_t(\PP_1))
		= \sum_{k=0}^\infty
		\PP_j
		\frac{\bar n^j}{(1+\bar n)^{j+1}}
		\frac{\bar n(\bar n+1) + T(j-\bar n)}{\bar n(1+\bar n)}
	\end{equation}
	\label{prop:attenuation_and_then_thermalisation_focks}
\end{prop}
\begin{proof}
	We have, using~\cref{prop:thermalisation_of_P1},
	\begin{equation}
	\begin{gathered}
		\Phi_{\bar n}^{\rm th}(\Phi_t(\PP_1))
		= \sum_{j=0}^\infty \PP_j
		\frac{\bar n^j}{(1+\bar n)^{j+1}} \left[
		(1-T) + T \frac{j+\bar n^2}{\bar n(1+\bar n)}
		\right] \\
		= \sum_{j=0}^\infty \PP_j
		\frac{\bar n^j}{(1+\bar n)^{j+1}}
		\frac{\bar n(\bar n+1)+ T(j-\bar n)}{\bar n(1+\bar n)}
	\end{gathered}
	\end{equation}
\end{proof}

\begin{prop}
	\begin{equation}
	\begin{gathered}
		\Phi_{\bar n}^{\rm th}(\Phi_t(\PP_2)) =
		\sum_{j=0}^\infty \PP_j \frac{\bar n^j}{(\bar n + 1)^{j+1}}
		\left[
		T^2 c_0
		+ 2TR c_1
		+R^2
		\right]
	\end{gathered}
	\end{equation}
	where
	\begin{equation}
	\begin{gathered}
		c_0 \equiv
			\frac{2\bar n^4 + 4j \bar n^2 + j(j-1)}{2\bar n^2 (\bar n+1)^2},
		\qquad
		c_1 \equiv
			\frac{\bar n^2 + j}{\bar n(\bar n+1)}.
	\end{gathered}
	\end{equation}
\end{prop}

\begin{prop}
	\begin{equation}
		\frac{1}{2}\Phi_{\bar n}^{\rm th}(\Phi_t(\PP[(\ket0+\ket2)])) = ?
	\end{equation}
\end{prop}
\begin{proof}
	We have
	\begin{equation}
		\Phi_t(\PP[\ket0+\ket2]) = 
		\left(1+R^2\right) \PP_0
		+ 2TR \, \PP_1
		+ T^2 \PP_2
		+ \left(
			t^2 \ketbra{2}{0} + \bar t^2 \ketbra{0}{2}
		\right).
	\end{equation}
	Thus, for the full state,
	\begin{equation}\scalebox{0.95}{$\displaystyle
	\begin{gathered}
		\frac12\sum_{j=0}^\infty
		\frac{\bar n^j}{(\bar n + 1)^{j+1}}
		\left[
		\left((1+R^2)
		+ 2TR \frac{j+\bar n^2}{\bar n(\bar n + 1)}
		+ T^2 \frac{2\bar n^4+4j \bar n^2 + j(j-1)}{2\bar n^2(\bar n + 1)^2}
		\right) \PP_j
		\right. \\
		\left. + \binom{j+2}{2}^{1/2} \frac{1}{(\bar n + 1)^2}
		(t^2 \ketbra{j+2}{j} + \bar t^2 \ketbra{j}{j+2})
		\right].
	\end{gathered}
	$}\end{equation}
\end{proof}

\begin{prop}
	The state $\ket0+\ket1$ evolving through a beamsplitter with transmissivity $t$, looking only at the transmitted output branch, gives
	\begin{equation}
		\rho_t
		\equiv
		\frac{2-T}{2} \PP_0
		+ \frac{T}{2} \PP_1
		+ \frac12 (t \ketbra{1}{0} + \mathrm{h.c.}).
	\end{equation}
	Applying $\Phi^{\rm th}_{\bar n}$ to this state gives
	\begin{equation}
	\begin{gathered}
		\langle j|\Phi^{\rm th}_{\bar n}(\rho_t)|j\rangle
		= \frac{\bar n^j}{(\bar n + 1)^{j+1}}
		\frac{[2\bar n(\bar n+1) + T(j-\bar n)]}{2\bar n(\bar n+1)},
	\end{gathered}
	\end{equation}
	and
	\begin{equation}
		\Tr[X_{j}\langle j|\Phi^{\rm th}_{\bar n}(\rho_t)|j\rangle]
		= \frac{\bar n^j}{(\bar n+1)^{j+1}} \frac{\sqrt{j+1}}{\bar n+ 1}
		\Re(t).
	\end{equation}
\end{prop}
\begin{proof}
	We get, using~\cref{prop:thermalisation_applied_to_00,prop:thermalisation_applied_to_11,prop:thermalisation_applied_to_01},
	\begin{equation}
	\begin{gathered}
		\Phi_{\bar n}^{\rm th}(\rho_t)
		= \sum_{j=0}^\infty
		\frac{\bar n^j}{2\bar n(\bar n + 1)^{j+2}}
		[2\bar n(\bar n+1) + T(j-\bar n)]
		\PP_j \\
		+ \frac{1}{2}\sum_{j=0}^\infty
		\frac{\bar n^j}{(\bar n + 1)^{j+2}} \sqrt{j+1} \,\,
		(t\ketbra{j+1}{j} + \bar t \ketbra{j}{j+1}),
	\end{gathered}
	\end{equation}
	where $X_j \equiv \ketbra{j}{j+1} + \ketbra{j+1}{j}$.
	% \begin{equation}
	% \begin{gathered}
	% 	% P_j^{\mathrm{th};\bar n}
	% 	\langle j|\Phi^{\rm th}_{\bar n}(\rho_t)|j\rangle
	% 	= \frac{\bar n^j}{(\bar n + 1)^{j+1}}
	% 	\left(
	% 		\frac{2-T}{2}
	% 		+ \frac{T}{2} \frac{j+\bar n^2}{\bar n(\bar n + 1)}
	% 		% + \frac{\Re(t)}{\sqrt{\bar n(\bar n + 1)}}
	% 		% \frac{(j+1/2)!}{j!}
	% 		% \left( \frac{ 2j - \bar n }{ 2 } \right)
	% 	\right) \\
	% 	= \frac{\bar n^j}{(\bar n + 1)^{j+1}}
	% 	\frac{1}{2\bar n(\bar n+1)}
	% 	% [(2-T)\bar n(\bar n+1) + T(j+\bar n^2)]
	% 	[2\bar n(\bar n+1) + T(j-\bar n)]
	% \end{gathered}
	% \end{equation}
\end{proof}


Let $\ket\psi$ be written in terms of mode operators as
$\ket\psi=f_\psi(\hat a^\dagger)\ket0$.
Then, using the relation $\hat D(\alpha) \hat a^\dagger \hat D^\dagger(\alpha)=\hat a^\dagger -\bar\alpha$, we see that
$\hat D(\alpha)\ket\psi=f_\psi(\hat a^\dagger-\bar\alpha)\ket\alpha$.
We can therefore write the thermalisation operation $\Phi^{\rm th}_{\bar n}$ as a statistical mixture of the form
\begin{equation}
	\Phi^{\rm th}_{\bar n}(\PP[\ket\psi])
	= \int d\mu(\alpha)
	\PP[f_\psi(\hat a^\dagger-\bar\alpha)\ket\alpha],
\end{equation}
where $d\mu(\alpha)=d(|\alpha|^2)\frac{d\phi}{2\pi} \frac{e^{-|\alpha|^2/\bar n}}{\bar n}$.
Evolving through a beamsplitter thus gives
\begin{equation}
	\Phi_{{\rm BS},t}(\Phi_{\bar n}^{\rm th}(\PP[\ket\psi]))
	= \int d\mu(\alpha)
	\PP[f_\psi(t \hat a_1^\dagger + r \hat a_2^\dagger-\bar\alpha)
	(\ket{t\alpha}\otimes\ket{r\alpha})
	].
\end{equation}
On the other hand, if we pass $\ket\psi$ through the beamsplitter and \emph{then} thermalise, we get
\begin{equation}
	\Phi_{{\rm BS},t}(\PP[\ket\psi])
	= \PP[f_{\psi}(t \hat a_1^\dagger + r\hat a_2^\dagger)
	(\ket{\rm vac}\otimes \ket{\rm vac})
	],
\end{equation}
and then, applying the thermalisation operation to the first mode, we get
\begin{equation}
	\Phi_{\bar n}^{\rm th}(\Phi_{{\rm BS},t}(\PP[\ket\psi]))
	= \int d\mu(\alpha)\,
	\PP[
	f_\psi(t (\hat a_1^\dagger-\bar\alpha) + r\hat a_2^\dagger)
	(\ket\alpha\otimes\ket{\rm vac})
	]
\end{equation}


\subsection{Thermalisation and then attenuation}

\begin{prop}\label{eq:generalised_geometric_series}
	\begin{equation}
		\sum_{k=n}^\infty \binom{k}{n} x^k
		= \frac{x^n}{(1-x)^{n+1}}
	\end{equation}
\end{prop}
\begin{proof}
	Differentiating the geometric series we get
	\begin{equation}
		D^n \frac{1}{1-x} = \frac{n!}{(1-x)^{n+1}},
	\end{equation}
	and on the other side
	\begin{equation}
		D^n \sum_{k=0}^\infty x^k
		= \sum_{k=n}^\infty \frac{k!}{(k-n)!} x^{k-n}.
	\end{equation}
\end{proof}


\begin{prop}
	Consider a thermal state $\rho^{\rm th}_{\bar n}$, evolving through a beamsplitter with transmissivity $T\equiv |t|^2$, of which we only consider the first output port. We then have
	\begin{equation}
	\begin{gathered}
		\rho^{\rm th}_{\bar n}
		\equiv \sum_{k=0}^\infty \frac{\bar n^k}{(\bar n+1)^{k+1}}\PP_k
		\longrightarrow
		\sum_{k=0}^\infty \frac{\bar n^k}{(\bar n+1)^{k+1}}
		\sum_{j=0}^k \binom{k}{j} T^j (1-T)^{k-j} \PP_j \\
		= \sum_{j=0}^\infty \PP_j \left[
			\sum_{k=j}^\infty 
			\frac{\bar n^k}{(\bar n+1)^{k+1}}
			\binom{k}{j}
			T^j (1-T)^{k-j}
		\right]
		= \sum_{j=0}^\infty \PP_j \left[
			\frac{T^j}{(1+\bar n)(1-T)^j}
			\sum_{k=j}^\infty \binom{k}{j} A^k
		\right],
	\end{gathered}
	\end{equation}
	where in the last step we defined $A\equiv \frac{\bar n}{1+\bar n}(1-T)$.
	Using~\cref{eq:generalised_geometric_series} we thus conclude that
	% We now make use of the formula
	% \begin{equation}
	% 	\sum_{k=j}^\infty \binom{k}{j} A^k
	% 	= \frac{A^j}{(1-A)^{j+1}},
	% \end{equation}
	% to conclude that
	\begin{equation}
	\begin{gathered}
		\rho^{\rm th}_{\bar n} \to
		\sum_{j=0}^\infty \PP_j
		\left[
			\frac{T^j}{(1+\bar n)(1-T)^j}
			\left(\frac{\bar n}{1+\bar n}(1-T)\right)^j
			\left(\frac{1+\bar n}{1+\bar n T}\right)^{j+1}
		\right] \\
		= \sum_{j=0}^\infty \PP_j
		\frac{(\bar n T)^j}{(1+\bar nT)^{j+1}}.
	\end{gathered}
	\end{equation}
\end{prop}

\begin{prop}
	Denote with $\calE_t$ the channel describing attenuation through a beamsplitter with transmissivity $t$. Then
	\begin{equation}
		\calE_t(\Phi_{\bar n}^{\rm th}(\PP_0))
		= \sum_{k=0}^\infty \PP_j
		\frac{(\bar n T)^j}{(1+\bar n T)^{j+1}},
	\end{equation}
	\begin{equation}
		\calE_t(\Phi_{\bar n}^{\rm th}(\PP_1))
		= \sum_{j=0}^\infty \PP_j \left[
		\frac{(\bar n T)^j}{(1+\bar n T)^{j+1}}
		\frac{(j + \bar n + (\bar n-1)\bar nT)}{\bar n(1+\bar nT)}
		\right].
	\end{equation}
\end{prop}
\begin{proof}
	\begin{equation}
	\begin{gathered}
		\calE_t(\Phi_{\bar n}^{\rm th}(\PP_1))
		= \sum_{k=0}^\infty
		\frac{\bar n^k}{(1+\bar n)^{k+1}}
		\frac{(k+\bar n^2)}{\bar n(\bar n+1)}
		\sum_{j=0}^k
		\binom{k}{j} T^j (1-T)^{k-j} \PP_j \\
		= \sum_{j=0}^\infty \PP_j
		\left[
		\sum_{k=j}^\infty 
		\frac{\bar n^k}{(1+\bar n)^{k+1}}
		\frac{(k+\bar n^2)}{\bar n(\bar n+1)}
		\binom{k}{j} T^j (1-T)^{k-j}
		\right] \\
		= \sum_{j=0}^\infty \PP_j \left[
		T^j \frac{\bar n^{j-1}}{(1+\bar n T)^{j+2}}
		(j + \bar n + (\bar n-1)\bar nT)
		\right] \\
		= \sum_{j=0}^\infty \PP_j \left[
		\frac{(\bar n T)^j}{(1+\bar n T)^{j+1}}
		\frac{(j + \bar n + (\bar n-1)\bar nT)}{\bar n(1+\bar nT)}
		\right]
	\end{gathered}
	\end{equation}
	% double checked
	Note that, following~\cref{prop:attenuation_and_then_thermalisation_focks}, we have
	\begin{equation}
		\calE_t(\Phi_{\bar n}^{\rm th}(\PP_i))
		= \Phi^{\rm th}_{\bar nT}(\calE_t(\PP_i)).
	\end{equation}
	This relation holds more generally for any input state \highlight{(I think? prove it)}
	% Sum[
	%   Binomial[k, j] (n/(1 + n) (1 - t))^k (k + n^2),
	%   {k, j, Infinity}
	%   ] // FullSimplify[#, n \[Element] Integers && n >= 0 && t >= 0] &
\end{proof}

% \begin{prop}
% 	% Denote with $\calE_t$ the channel describing attenuation through a beamsplitter with transmissivity $t$.
% 	\begin{equation}
% 		\langle j|
% 		\Phi_{\bar n}^{\rm th}\left(\PP\left(\frac{1}{\sqrt2}(\ket0+\ket1)\right)\right)
% 		|j\rangle
% 		= \frac{(\bar nT)^{j}}{(1+\bar nT)^{j+1}}
% 		\frac{\bar nT(1+\bar nT)+j+\bar n(1+\bar nT-T)}{2(\bar nT)(1+\bar nT)}
% 	\end{equation}
% 	CHEEEEECK
% \end{prop}
% \begin{proof}
% 	\begin{equation}
% 	\begin{gathered}
% 		\frac12 \sum_{j=0}^\infty
% 		\frac{\bar n^j}{(1+\bar n)^{j+1}}
% 		\left[
% 			\left(
% 			1 + \frac{(j+\bar n^2)}{\bar n(1+\bar n)}
% 			\right) \PP_j
% 			+ \frac{\sqrt{j+1}}{1+\bar n}
% 			(\ketbra{j}{j+1}+\ketbra{j+1}{j})
% 		\right]
% 	\end{gathered}
% 	\end{equation}
% \end{proof}

\chapter{Bell nonlocality}

Review on Bell nonlocality~\autocite{brunner2014bell}.
Kochen-Specker theorem~\autocite{kochen1967problem}.
Fine's theorem~\autocite{fine1982joint,fine1982hidden}.

Consider a standard Bell's inequalities scenario: two parties, $A$ and $B$, sharing a bipartite state $\ket\Psi$. $A$ and $B$ can perform measurements on their respective parts of $\ket\Psi$. We will denote with $A$ and $B$ the random variables corresponding to their outcomes, and with $X$ and $Y$ the random variables corresponding to their choices of measurements.
The most general type of correlations that can be observed is of the form $p(ab|xy)$.

We want to understand whether the correlations between the measurements performed by $A$ and $B$ are compatible with a hidden variable theory.
By \emph{hidden variable theory} we mean a theory which tries to explain the correlations $p(ab|xy)$ under the assumptions of \emph{locality} and \emph{realism}.
Mathematically, this is expressed in the following assumption over $p$:
\begin{equation}
    p(ab|xy) = \sum_\lambda p(\lambda) p(a|x\lambda)p(b|y\lambda).
    \label{eq:prob_lhv_decomposition}
\end{equation}
The gist of Bell's theorem, in the CHSH form, is that~\cref{eq:prob_lhv_decomposition} is not compatible with \ac{QM}. In other words, the correlations produced by quantum mechanics cannot be explained via a probability $p(ab|xy)$ which satisfies~\cref{eq:prob_lhv_decomposition}.
The reason is that if~\cref{eq:prob_lhv_decomposition} is verified, then the outcome probabilities are a convex combination of \emph{factorised} outcome probabilities,
meaning that the expectation value of factorized functions $f(A,B)=f_1(A)f_2(B)$ over the outcomes gives
\begin{equation}
    \expval{f(A,B)}_{x,y} =
    \sum_\lambda p(\lambda) \expval{f(A,B)}_{x,y,\lambda} =
    \sum_\lambda p(\lambda) \expval{f_1(A)}_{x,\lambda} \expval{f_2(B)}_{y,\lambda}.
\end{equation}
A way to find a Bell inequality is then to find coefficients $\alpha_{x,y}\in\RR$ such that $\lvert \calS_\lambda\rvert\le C$ for all $\lambda$, with
\begin{equation}
    \calS_\lambda \equiv \Big\lvert
        \sum_{x,y} \alpha_{x,y} \expval{f(A,B)}_{x,y,\lambda}
    \Big\rvert.
\end{equation}
for this would directly imply that $\calS\equiv\sum_\lambda p_\lambda \calS_\lambda$ satisfies $|\calS|\le C$.


\begin{depthbox}[]{CHSH inequalities}
    Assume that $A$ and $B$ have two measurement choices, and two measaurement outcomes. We will conventionally denote with $x,y=\pm1$ and $a,b=\pm1$ the two possible measurement choices and outcomes, respectively.
    Let us define $E(x,y)$ as the expectation value of the product of the measurement outcomes measured by $A$ and $B$, when the measurement choices are $x$ and $y$:
    \begin{equation}
        E(x,y) \equiv \sum_{a,b} ab \, p(ab|xy).
    \end{equation}
    If the correlations satisfy~\cref{eq:prob_lhv_decomposition}, then we can expand this to
    \begin{equation}
    \begin{gathered}
        E(x,y) = \sum_\lambda p(\lambda) E_\lambda(x,y), \\
        E_\lambda(x,y) = \sum_a a\,p(a|x\lambda)\sum_b b\,p(b|y\lambda).
    \end{gathered}
    \end{equation}
    This means that $E(x,y)$ is really a convex combination of the expectation values $E_\lambda(x, y)$, each one of which is the expectation value of $AB$ \emph{for $A$ and $B$ uncorrelated with each other}:
    \begin{equation}
        E_\lambda(x,y) = \expval{A}_{x,\lambda} \expval{B}_{y,\lambda}.
    \end{equation}
    If $y,y'$ are two different measurement choices, then $E_\lambda(x,y)+E_\lambda(x,y')$ becomes
    \begin{equation}
        E_\lambda(x,y) + E_\lambda(x,y') =
        \sum_a a\,p(a|x\lambda) \sum_b b[p(b|y\lambda) + p(b|y'\lambda)].
    \end{equation}
    We will write this using the shorthand notation:
    \begin{equation}
        \expval{a_0 b_0}_\lambda + \expval{a_0 b_1}_\lambda =
        \expval{a_0}_\lambda (\expval{b_0}_\lambda + \expval{b_1}_\lambda).
    \end{equation}
    Let us now define the quantity $S_\lambda$, defined as
    \begin{equation}
    \begin{aligned}
        S_\lambda &\equiv
            E_\lambda(x,y) + E_\lambda(x,y') + E_\lambda(x',y) - E_\lambda(x',y') \\
        &= \expval{a_0}_\lambda (\expval{b_0}_\lambda + \expval{b_1}_\lambda)
        + \expval{a_1}_\lambda (\expval{b_0}_\lambda - \expval{b_1}_\lambda).
    \end{aligned}
    \end{equation}
    We then have
    \begin{equation}
        |S_\lambda| \le \lvert\expval{b_0} + \expval{b_1}\rvert
                      + \lvert\expval{b_0} - \expval{b_1}\rvert.
    \end{equation}
    Remembering that $\expval{a_i},\expval{b_i}$ are reals and lesser than $2$ in modulus, it easily follows that
    \begin{equation}
        \lvert S_\lambda\rvert \le 2.
    \end{equation}
    This is for example immediate in the case $\expval{a_i},\expval{b_i}\ge 0$.
\end{depthbox}

\section{Space of behaviors}
\newcommand{\NS}{\mathcal{N}\mathcal{S}}

Consider the set of all possible Bell correlation scenarios $\{p(ab|xy)\}_{abxy}$.
Let $\mathcal P$ denote the set of all such vectors.
If there are $m$ possible measurement choices, and $\Delta$ possible measurement outcomes, then $\mathcal P$ is the subspace of $\RR^{\Delta^2 m^2}$ containing all vectors $\mathbf p=\{p(ab|xy)\}_{abxy}$ such that
\begin{equation}
    p(ab|xy)\ge0\quad\text{ and }\quad \sum_{ab}p(ab|xy)=1.
\end{equation}
It follows that $\dim\mathcal P=(\Delta^2-1)m^2$.

A first limitation to $\mathcal P$ is to restrict to only the set of \emph{no-signaling} behaviors, which we will denote with $\NS$.
$\NS\subset\mathcal P$ is the set of behaviors whose marginals are independent of the other party's measurement settings:
\begin{equation}
    p(a|x)\equiv p(a|xy)\equiv\sum_b p(ab|xy),
\end{equation}
and similarly for $p(b|y)$.
It is shown in~\autocite{pironio2005lifting} that $\dim\NS=2(\Delta-1)m+(\Delta-1)^2m^2$.

A more restrictive constraint is represented by the set $\mathcal L$ of \emph{local behaviors}, defined as the element of $\mathcal P$ such that
\begin{equation}
    p(ab|xy)=\sum_\lambda q(\lambda) p(a|x,\lambda)p(b|y,\lambda).
\end{equation}
One can show that $\mathcal L\subset \NS\subset \mathcal P$.
\begin{prop}
    Nonlocal correlations $\mathbf p\notin \mathcal L$ can only exist if $\Delta\ge2$ \emph{and} $m\ge2$.
\end{prop}
\begin{proof}
    See~\href{https://physics.stackexchange.com/a/421622/58382}{physics.stackexchange.com/a/421622}.
    The $\Delta=1$ case is trivial. For the $m=1$ case, we want to prove that any probability distribution $p(a,b)$ can be written as
    \begin{equation}
        p(a,b)=\sum_\lambda q(\lambda)p(a|\lambda)p(b|\lambda).
    \end{equation}
    Consider a hidden variable $\lambda_{a,b}$ with $\Delta^2$ possible values, and a probability distribution $q(\lambda_{a,b})=p(a,b)$.
    Then we have the thesis if $p(a'|\lambda_{a,b})=\delta_{a,a'}$ and $p(b'|\lambda_{a,b})=\delta_{b,b'}$.
    Indeed, this proves not only that any $p(a,b)$ can be thought of as ``induced'' by a hidden variable, but that such hidden variable can always be chosen to \emph{deterministically} explain the probability distribution.
\end{proof}

\begin{prop}[\cite{fine1982hidden,brunner2014bell}]
    The set $\mathcal L$ of local correlations can always be expressed in terms of \emph{deterministic} local hidden-variable models.
\end{prop}
\begin{proof}
    \large{\color{red}Why?}
\end{proof}

\begin{prop}[\cite{brunner2014bell}]
    asd
\end{prop}


\chapter{Quantum resources}

Main sources: \cite{streltsov2016maximal}.

A number of different quantum features are considered as important resources for applications of quantum information theory.
Among these \emph{entanglement}, \emph{quantum discord}, and \emph{quantum coherence} have been identified as necessary ingredients for the successful implemntation of tasks such as quantum cryptography, quantum algorithms, and quantum metrology.
Quantum resources can be formally classified in the framework of resource theories,
where the state space is divided into free states and resource states.

\section{Resource theory of quantum coherence}
\newcommand{\setIncoherentStates}{\mathcal{I}}
Sources: \cite{aberg2006quantifying,baumgratz2014quantifying,winter2016operational,marvian2016how,streltsov2016quantum}.

The free states of this resource theory are called \emph{incoherent states},
that is, states which are diagonal in a fixed basis $\{\ket i\}$, like $\sigma = \sum_i p_i \ketbra{i}$.
Let us denote the set of such states with $\setIncoherentStates{}$.
The \emph{free operations} are the operations that cannot turn a free state into a resource state, so in this case the operations which cannot turn a mixed state into a coherent state.
There are several approaches to define free operations for mixed states:
\begin{enumerate}
	\item The historically first and most general approach, suggested in \cite{aberg2006quantifying}, is to define the set of \acp{MIO}: operations which cannot create coherence:
	\begin{equation}
	 	\Lambda_{\on{MIO}}(\sigma) \in \setIncoherentStates{}, \quad \forall \sigma \in \setIncoherentStates{}.
	 \end{equation}
	 \item Another important family is the set of \acp{IO}~\cite{baumgratz2014quantifying}.
	 These are operations which admit a Kraus decomposition $\Lambda_{\on{IO}}(\rho) = \sum_i K_i \rho K_i^\dagger$ with incoherent Kraus operators $K_i$, that is, $K_i \ket m \sim \ket n$.
	 \item See~\cite{streltsov2016maximal} for other definitions and extensions to multipartite systems.
\end{enumerate}
The \emph{amount of coherence} in a given state can be quantified via \emph{coherence monotones}.
These are nonnegative functions $\mathcal C$ which do not increase under the corresponding set of free operations.
A \ac{MIO} monotone is a nonnegative function $\mathcal C$ which does not increase under $\ac{MIO}$ operations:
$\mathcal C(\Lambda_{\on{MIO}}(\rho)) \le \mathcal C(\rho)$.
An important example are distance-based coherence monotones:
$\mathcal C(\rho) \equiv \inf_{\sigma\in\setIncoherentStates{}} D(\rho,\sigma)$,
with $D$ a suitable distance on the space of quantum states.
The most prominent example of distance-based measure is the \emph{relative entropy of coherence}
$\mathcal C_r(\rho) \equiv \min_{\sigma\in\setIncoherentStates{}} S(\rho \| \sigma)$
with the relative entropy
$S(\rho \| \sigma) \equiv \Tr(\rho \log_2\rho) - \Tr(\sigma\log_2\sigma)$.

\paragraph{Maximally coherent mixed states~\cite{streltsov2016maximal}}
Since coherence is a basis-dependent concept, a unitary operation will in general change the amount of coherence in a given state.
It is therefore natural to ask the question: \emph{which unitary maximizes the coherence of a given state $\rho$?}.
We therefore define the corresponding figure of merit
$\mathcal C_{\on{max}}(\rho) \equiv \max_U \mathcal C(U\rho U^\dagger)$.
While it is reasonable to believe that this maximization is out of reach, it turns out that the maximum can be evaluated in a large number of relevant scenarios, and that there exists a \emph{universal} maximally coherent mixed state, which does not depend on the particular choice of coherence monotone.
\begin{thm}[\cite{streltsov2016maximal}]
	For a fixed spectrum $\{ p_n \}$, the state
	\begin{equation}
		\rho_{\on{max}} = \sum_{n=1}^d p_n \ketbra{n_+},
	\end{equation}
	is a maximally coherent mixed state with respect to any \ac{MIO} monotone.
	Here $\{ \ket{n_+} \}$ denotes a \ac{MUB} with respect to the incoherent basis $\{ \ket{i} \}$,
	that is, $\lvert \braket{i}{n_+}\rvert^2 = 1/d$, where $d$ is the dimension of the Hilbert space.
\end{thm}
\begin{thm}[\cite{streltsov2016maximal}]
	For any distance-based coherence monotone with a contractive distance $D$,
	the following equality holds:
	$\mathcal C_{\on{max}} = \mathcal C(\rho_{\on{max}}) = D(\rho, \mathds{1} / d)$.
\end{thm}

\subsection{Coherence VS multipartite entanglement}
\parencite{regula2017converting,killoran2016converting}

In~\cite{regula2017converting} is presented a general formalism for the conversion of nonclassicality into multipartite entanglement.
They show that a faithful reversible transformation between the two resources is always possible, within a precise resource-theoretic framework.

Despite the common origin, entanglement and superposition can be formalized according to different resource theories:
the former being tied to the paradigm of spatially separated laboratories which can only implement local operations and classical communication for free,
while the second specified by the inability to create superpositions of the classical states for free.
Consequently, these two resources, like two currencies, enjoy different uses in quantum technologies.

\cite{regula2017converting} show that the presence of $k$-level nonclassicality in the state of a single $d$-level system is necessary and sufficient to create $(k+1)$-partite entanglement between the system and $k$ ancillas.
This builds on the result of \cite{killoran2016converting} that there always exists an isometry, consisting of adding an ancilla and applying a global unitary, which maps each pure state of nonclassical rank $k$ into a bipartite entangled pure state of Schmidt rank $k$.

\begin{thm}[\cite{regula2017converting}]
	\renewcommand{\DH}{\mathcal D(\calH{})}
	\newcommand{\calHanc}{\calH_{\on{anc}}}
	Let $\calH$ be a $d$-dimensional Hilbert space, $\DH{}$ the corresponding set of density operators,
	and $\calHanc{} \cong \calH{}$ the Hilbert space of an ancillary system.
	Then if the classical pure states $\{ \ket{\chi_i} \}_{i=1}^d$ form a linearly independent set spanning $\calH{}$,
	there exists an isometry $W: \calH{} \to \calH{} \otimes \calHanc{}$ such that for any state $\rho \in \DH{}$ we have
	$N_N(\rho) = N_S(W \rho W^\dagger)$.
\end{thm}
 
Following~\cite{ghne2005multipartite}, we define a pure state to be $k$-producible if it can be written as
$\ket\psi = \ket{\psi_1} \otimes ... \otimes \ket{\psi_m}$ with each $\ket{\psi_j}$ pertaining to at most $k$ parties,
and a mixed state $\rho$ to be $k$-producible if it can be written as a convex combination of $k$-producible pure states.
We call a state $\rho$ \emph{genuinely $k$-partite entangled} if it is $k$-producible but not $(k-1)$-producible.
Equivalently, under such conditions, we say that $\rho$ has \emph{entanglement depth} $D_E(\rho) = k$.
A 1-producible state $\rho$ has $D_E(\rho)=1$ and is fully separable.

\begin{thm}
	\renewcommand{\DH}{\mathcal D(\calH{})}
	\newcommand{\aHanc}{\mathcal H_{\on{anc}}}
	Let $\calH$ be a $d$-dimensional Hilbert space, and $\aHanc$ the Hilbert space of an ancillary system.
	Then if the classical pure states $\{ \ket{\chi_i} \}_{i=1}^d \in \calH$ form a linearly independent set spanning $\calH$, there exists an isometry $V: \calH\to\calH\otimes\aHanc^{\otimes d}$
	such that for any state $\rho\in\DH$ with nonclassical number $N_N(\rho)=k$,
	$V \rho V^\dagger$ is genuinely $(k+1)$-partite entangled iff $\rho$ is nonclassical ($2\le k\le d$)
	and $V\rho V^\dagger$ is fully separable iff $\rho$ is classical ($k=1$).
\end{thm}

\chapter{Analytical mechanics}

\paragraph{Euler-Lagrange equations.}
The equations of motion corresponding to the \emph{Lagrangian function} $L=L(\bs q,\dot{\bs q},t)$ are
\begin{equation}
	\frac{d}{dt}\left(\frac{\partial L}{\partial \dot q_i}\right) -
	\frac{\partial L}{\partial q_i} = 0.
\end{equation}
More precisely, given a \emph{chart function} $\sharp_{\bs q}:t\mapsto (\bs q(t),\partial_t \bs q(t),t)$, the Euler-Lagrange equations read
\begin{equation}
	((\partial_{\dot q_i}L)\circ\sharp_{\bs q})' - (\partial_{q_i}L)\circ\sharp_{\bs q} = 0.
\end{equation}
Physical systems follow trajectories $t\mapsto\bs q(t)$ satisfying this equation.

\paragraph{Principle of least action.}
Trajectories that satisfy the Euler-Lagrange equations are also extrema of the \emph{action} $\calS\equiv \calS[\bs q]$ defined as
\begin{equation}
	\calS[\bs q]
	= \int_{t_1}^{t_2} L \circ \sharp_{\bs q}
	= \int_{t_1}^{t_2} dt \,\, L(\bs q(t),\partial_t \bs q(t),t).
\end{equation}
More precisely, EL equations can be derived imposing $t\mapsto\bs q(t)$ is an extremum for $\calS$ in the set of trajectories with fixed initial and final points.

\paragraph{Hamiltonian from Lagrangian.}
Given the Lagrangian function $L=L(\bs q,\dot{\bs q},t)$, the Hamiltonian reads
\begin{equation}
	H(\bs p, \bs q, t) \equiv
	\bs p\cdot\dot{\bs q}(\bs p, \bs q, t)
	- L(\bs q, \dot{\bs q}(\bs p, \bs q, t), t),
\end{equation}
where $\dot{\bs q}(\bs p, \bs q, t)$ is the function defined implicitly via
\begin{equation}
	\partial_{\dot{\bs q}}L(\bs q, \dot{\bs q}(\bs p, \bs q, t), t)
	= \bs p.
\end{equation}
In other words, $\dot{\bs q}(\bs p,\bs q, t)$ is the inverse with respect to $\dot{\bs q}$ of the function
\begin{equation}
	\bs p(\bs q,\dot{\bs q}, t) = \nabla_{\dot{\bs q}}L(\bs q, \dot{\bs q}, t).
\end{equation}
Similarly, the Lagrangian can be written in terms of the Hamiltonian as
\begin{equation}
	L(\bs q,\dot{\bs q}, t) =
	\partial_{\dot{\bs q}}
\end{equation}
This construction can be seen to arise from taking the \emph{Legendre transform} of the Lagrangian. In other words, Hamiltonian and Lagrangian are Legendre transforms one of the other.
When the exact functional dependences are clear, one can briefly write the relation between $H$ and $L$ as $H=p\dot q - L$.

\paragraph{Hamilton equations from least action on Lagrangian.}
Taking the partial derivative of the Hamiltonian function, we get
\begin{equation}
	\partial_{\bs p}H=\dot{\bs q}+\bs p\cdot\partial_{\bs p}\dot{\bs q} - \partial_{\dot{\bs q}}L \cdot \partial_{\bs p}\dot{\bs q},
\end{equation}
or more explicitly,
\begin{equation}
	\partial_{\bs p}H(\bs p,\bs q, t) =
	\dot{\bs q}(\bs p, \bs q, t) +
	\bs p\cdot \partial_{\bs p}\dot{\bs q}(\bs p,\bs q, t) -
	\partial_{\dot{\bs q}}L(\bs q, \dot{\bs q}(\bs p,\bs q, t), t)
	\cdot \partial_{\bs p}\dot{\bs q}(\bs p, \bs q, t).
\end{equation}
Either way, we find that
$\partial_{\bs p}H(\bs p, \bs q, t) = \dot{\bs q}(\bs p, \bs q, t)$.
A similar calculation leads us to
\begin{equation}
	\partial_{\bs q}H(\bs p, \bs q, t) =
	- \partial_{\bs q}L(\bs q, \dot{\bs q}(\bs p, \bs q, t), t).
\end{equation}
If we consider these equations \emph{on shell}, and remember that the Euler-Lagrange equations prescribe that
\begin{equation}
	(\partial_{\bs q}L)\circ\sharp_{\bs q} =
	\partial_t((\partial_{\dot{\bs q}}L)\circ\sharp_{\bs q})
	= \dot{\bs p}.
\end{equation}
We conclude that, if $t\mapsto \bs q(t)$ satisfies EL equations, then
the corresponding phase-space trajectory $t\mapsto (\bs q(t),\bs p(t))$ satisfies the \emph{Hamilton equations}
\begin{equation}
	\begin{cases}
		\dot{\bs q}(t) = \partial_{\bs p}H(\bs p(t),\bs q(t), t), \\
		\dot{\bs p}(t) = -\partial_{\bs q}H(\bs p(t),\bs q(t), t).
	\end{cases}
\end{equation}
We also have $\partial_t H = -\partial_t L$.

\paragraph{Conservation of the Hamiltonian.}
Define $\tilde\sharp(t)\equiv (\bs p(t), \bs q(t),t)$, for some trajectory $(\bs p(t), \bs q(t))$ satisfying Hamilton equations.
Then
\begin{equation}
	\partial_t(H\circ\tilde\sharp) =
	(\partial_t H)\circ\tilde\sharp.
\end{equation}
It is worth stressing that on the LHS we have only a function of time, and thus the derivative is a ``total derivative'', whereas on the RHS, $\partial_t H$ is a partial derivative of $H$ with respect to its time argument.

\paragraph{Matrix form of Hamilton equations.}
We can write Hamilton equations in a more concise and symmetric form by writing the phase-space coordinates as the $2n$-dimensional vector $\bs x\equiv (\bs q,\bs p)$. Hamilton equations then read
\begin{equation}
	\dot{\bs x}(t) = \EE \,\nabla_{\bs x}H(\bs x(t), t)
	\equiv \EE \partial_{\bs x}H(\bs x(t), t),
\end{equation}
where
\begin{equation}
	\EE \equiv \begin{pmatrix}
		0 & I_n \\ -I_n & 0
	\end{pmatrix}
\end{equation}
is the $2n\times 2n$ \emph{symplectic matrix}.

\begin{prop}[Liouville's theorem]
	The phase-space distribution function is constant along trajectories of the system.
	Another way to state this is to say that infinitesimal phase-space volumes remain constant in time.
\end{prop}
\begin{proof}
	Consider the set of trajectories in phase-space corresponding to a given Hamiltonian $H$. Upon an infinitesimal-time transformation, we have
	\begin{equation}
		\bs q \to \bs q + dt \partial_{\bs p}H,
		\qquad
		\bs p \to \bs p - dt \partial_{\bs q}H.
	\end{equation}
	Consider now two different trajectories, starting at $\bs x\equiv(\bs q,\bs p)$ and $\bs x'\equiv(\bs q',\bs p')$.
	We then have
	\begin{equation}
	\begin{gathered}
		d\bs q(t) \equiv \bs q'(t) - \bs q(t)
		\to d\bs q(t) + d\bs q(t)dt\partial_{\bs q}\partial_{\bs p}H(\bs x(t)), \\
		d\bs p(t) \to d\bs p(t)[1 - dt \partial_{\bs p}\partial_{\bs q}H(\bs x(t))].
	\end{gathered}
	\end{equation}
	We conclude that $d\bs q(t)d\bs p(t) \to d\bs q(t)d\bs p(t) + \calO(dt^2)$, that is, infinitesimal phase-space volumes are constant along Hamiltonian trajectories.
\end{proof}

\section{Canonical transformations}

Consider a phase-space coordinate transformation of the form $\bs x\mapsto \bs y(\bs x)$. If the trajectory $t\mapsto\bs x(t)$ satisfies Hamilton equations, then
\begin{equation}
	\dot{\bs y}(t) =
	(\partial_{\bs x}\bs y)(\bs x(t))
	\cdot \dot{\bs x}(t).
\end{equation}
Define a \emph{transformed Hamiltonian} $\tilde H$ such that
$\tilde H(\tilde{\bs y}(\tilde{\bs x})) = H(\tilde{\bs x})$.
Here we are using the shorthand notation $\tilde{\bs x}\equiv(\bs x,t)$.
Then
\begin{equation}
	\dot{\bs y}(t) = J(\bs x(t)) \EE J^T(\bs x(t))
	\,\, \partial_{\bs y}\tilde H(\tilde{\bs y}(\tilde{\bs x}(t))),
\end{equation}
where $J(\bs x(t))$ is the \emph{Jacobian} of the transformation $\bs x\mapsto \bs y(\bs x)$.
It follows that the transformed variable follow the Hamilton equations with transformed Hamiltonian $\tilde H$ if and only if the Jacobian of the transformation is \emph{symplectic}, that is, is such that
\begin{equation}
	J(\bs x)\EE J^T(\bs x) = \EE.
\end{equation}

\begin{example}
	Consider a change of coordinates of the form $q\mapsto Q(q)$.
	What is the corresponding function $(q,p)\mapsto P(q,p)$ making the transformation canonical.
	For the transformation to be canonical we require the Jacobian to be symplectic. For $2\times2$ matrices, this is equivalent to the matrix having unit determinant. The Jacobian reads
	\begin{equation}
		J(q,p) = \begin{pmatrix}
			\partial_q Q & \partial_p Q \\
			\partial_q P & \partial_p P
		\end{pmatrix} =
		\begin{pmatrix}
			Q' & 0 \\
			\partial_q P & \partial_p P
		\end{pmatrix}.
	\end{equation}
	it follows that the transformation is canonical \emph{iff} $\partial_p P=1/\partial_q Q$, that is, $P(q,p) = p / Q'(q)$.

	For example, if $Q(q)=q^2$, then we must have $P(q,p) = \frac{p}{2q}$.
\end{example}

\paragraph{Infinitesimal canonical transformations.}
We can also consider change of coordinates parametrised by some real parameter $\alpha$:
\begin{equation}
\begin{aligned}
	\bs Q(\bs q,\bs p) &= \bs q + \alpha \bs F(\bs q, \bs p), \\
	\bs P(\bs q,\bs p) &= \bs p + \alpha \bs E(\bs q, \bs p).
\end{aligned}
\end{equation}
For these to be canonical, the Jacobian $J(\bs q, \bs p,\alpha)$ of the transformation must be symplectic. We have
\begin{equation}
	J(\bs q, \bs p, \alpha) =
	\begin{pmatrix}
		I + \alpha \partial_{\bs q}\bs F & 
		\alpha \partial_{\bs p}\bs F \\
		\alpha \partial_{\bs q}\bs E &
		I + \alpha \partial_{\bs p}\bs E
	\end{pmatrix}.
\end{equation}

\section{Hamiltonian formalism}

Consider an Hamiltonian $H$.
If the coordinates $q_i$ are \emph{cyclic} then $\dot p_i=0$.
If $H$ is conserved then $H=H(\bs\alpha)$ is also constant in time, and the remaining equations of motion are
\begin{equation}
	\dot q_i = \frac{\partial H}{\partial\alpha_i} = \omega_i(\alpha)
	\Longrightarrow
	q_i = \omega_i t + \delta_i.
\end{equation}

The Euler-Lagrange equations are invariant when
\begin{enumerate}
	\item a point transformation occurs: $Q=Q(q,t)$ with $L[q,t]=L'[Q,t]$;
	\item a total derivative is summed to the Lagrangian: $L'=L+\frac{dF[q,t]}{dt}$.
\end{enumerate}

\paragraph{Point transformations for a Hamiltonian.}
These are phase-space transformations of the form
\begin{equation}
	Q_i = Q_i(q,p, t),\qquad
	P_i = P_i(q,p,t),
\end{equation}
such that the Hamilton equations are still satisfied with some modified Hamiltonian function (I think?).

\paragraph{Canonical transformations.}
A transformation 

\chapter{Making sense of things}

\section{What does it mean to \emph{observe}?}

An \emph{observation} is the process of determining which one of a series of possible outcomes is realised. By definition, this entails a choice of \emph{measurement setting}, that is, a choice of interaction used to probe the unknown system.
Once a measurement has been chosen (say, \emph{look at the coin after flipping it}), the set of all possible states the system could be in is partitioned according to the possible outcomes. This is the distinction between \emph{macro} and \emph{micro} states.
But a system is not fully characterised by the way it behaves upon a given measurement: multiple different states might behave identically with respect to a fixed measurement. Therefore, knowledge of the outcomes in different measurement setups might be required for a full characterisation.
A question that follows naturally is then: \emph{are there relations between the outcomes in these possible choices of measurement?}
More specifically, we wonder whether the \emph{uncertainties} associated to such different measurements need to be somehow related.

QM teaches us that this is the case: there cannot be an arbitrary number of \emph{independent} measurements for a fixed system.
With ``\emph{independent}'' we mean here that the output probabilities corresponding to different measurement setups have zero mutual information, \emph{i.e.} knowing the outcome of one measurement gives us zero information about the outcome of another.
In reality, the situation in QM is even stronger: not only the measurement outcomes are independent, but they are also \emph{incompatible}, meaning that knowledge of an outcome in one measurement \emph{destroys} all knowledge we might have had regarding the outcomes in the other measurement.

From the QM formalism we know that such \emph{maximally incompatible} measurements produce probabilities satisfying $\sum_i p_i (1-p_i)\ge1/2$, where $i=1,2,3$ for a qubit system.
``Statistically unbiased'' means here that $p_i q_i=0$ implies full uncertainty with respect to the other measurement choices. The big question is: \textcolor{RedOrange}{\emph{\textbf{why not more than three unbiased measurements?}}}

Suppose knowing the outcome of $M_1$, we have full uncertainty on the outcome of $M_2$. This means the measurements correspond to ``incompatible quantities''.
One could try to understand this in terms of macro- and micro-states, but this approach misses the point. In particular, it makes the mistake of assuming that it makes sense to list faithfully the full set of states a system can be found in.
This is not the case when we consider situations in which the act of measurement itself perturbs the system.
In such scenario, it is not possible to make a list of all possible ``states'' of a system. Instead, it becomes necessary to identify the state by the way it behaves under several different measurement setups.

It must be stressed that such setup inherently assumes a separation between the system that is being probe and the rest of the universe.

\section{Decomplexifying QM}
A pure \emph{quantum state} $\psi$ is an element of a projective space $\mathbb{CP}^n\simeq S^{2n+1}/U(1)$.
Equivalently, it is an equivalence class of vectors in $\CC^{n}$ under the equivalence relation $\psi\sim\phi\Longleftrightarrow \psi=\lambda\phi$ for some $\lambda\in\CC$.
Using only real numbers, we can describe states as equivalence classes of vectors of $\psi\in\RR^{2n}\simeq \RR^n\otimes\RR^2$: $\psi\in\RR^{2n}\!/\!\sim$ where $\psi\sim\phi\Longleftrightarrow \psi=\alpha(I\otimes R)\phi$ for some $\alpha\in\RR$ and $R\in SO(2)$.
Equivalently, these are characterisable as orbits in the hypersphere $S^{2n-1}$.

In this notation, a linear map acting on a complex state, $A^c\psi^c$, translates into the action
\begin{equation}
	\psi\equiv \begin{pmatrix}
		\psi_R \\ \psi_I
	\end{pmatrix}
	\mapsto
	\begin{pmatrix}
		A_R \psi_R - A_I \psi_I \\
		A_R \psi_I + A_I \psi_R
	\end{pmatrix} \equiv A\psi \in \RR^n\otimes\RR^2.
\end{equation}
In other words, the linear transformation $A^c$ is represented as
\begin{equation}
	A^c \to A \equiv \begin{pmatrix}
		A_R & -A_I \\ A_I & A_R
	\end{pmatrix}.
\end{equation}
If $A^c$ is unitary, $(A^c)^\dagger A^c=A(A^c)^\dagger=I$, then $A_R^T A_R+A_I^T A_I = A_R A_R^T+A_I A_I^T = I$ and $A_R^T A_I = A_I^T A_R$.
From this we can see that $A^T A=AA^T=I_{2n}$ and $\det(A)=\lvert\det(A^c)\rvert^2=1$, that is, $A\in SO(2n)$.
Moreover, thus implies that $A_R$ and $A_I$ have the same left and right singular vectors, and with singular values summing to $1$:
\begin{equation}
	A_R = \sum_k \sqrt{s_k} \ketbra{u_k}{v_k},
	\qquad
	A_I = \sum_k \sqrt{1-s_k} \ketbra{u_k}{v_k}.
\end{equation}
Note the difference between these operators and unitary matrices: unitary matrices' singular values are all equal to $1$, and therefore unitaries send any orthonormal basis into another orthonormal basis. In contrast, here, different vectors $\ket{v_k}$ are treated differently, in that they correspond to different singular values.

One can also verify that $A^c$ unitary implies $A\in \on{Sp}(2n,\RR)$:
\begin{equation}\scalebox{0.9}{$\displaystyle
	A^T \begin{pmatrix}
		0 & I \\ -I & 0
	\end{pmatrix} A
	= \begin{pmatrix}
		A_R^T & A_I^T \\ -A_I^T & A_R^T
	\end{pmatrix}
	\begin{pmatrix}
		0 & I \\ -I & 0
	\end{pmatrix}
	\begin{pmatrix}
		A_R & -A_I \\ A_I & A_R
	\end{pmatrix} =
	\begin{pmatrix}
		-A_I^T A_R + A_R^T A_I & A_I^T A_I + A_R^T A_R \\
		-A_R^T A_R - A_I^T A_I & A_R^T A_I - A_I^T A_R
	\end{pmatrix}
$}\end{equation}

\subsection{Purely probabilistic route}

At a fundamental level, a \emph{qubit state} is something that always returns a binary answer when measured.
It is however crucial to observe that the outcome probabilities associated with a single measurement setup are not sufficient to fully characterise such a state: knowing that the probabilities in the $Z$ basis are $(0.5,0.5)$ does not tell us what the probabilities are in the $X$ basis.
Nonetheless, sometimes knowing the probabilities in one basis \emph{is} sufficient for a full characterisation. More specifically, knowing that the outcome is deterministic in a given basis, say $Z$, implies it having maximal entropy in \emph{mutually unbiased} bases.

For any state, there is always a measurement choice with a deterministic outcome, which alone would be sufficient to characterise the state. Can there be more than one? Sure, but that would probably amount to not be thinking about the state as a \emph{qubit} anymore.

Suppose there is a measurement $M$ which, performed on the state $\psi$, results in outcome probabilities $\calP(\psi,M)=(1,0)$.
There might be different a measurement $M'$ such that $\calP(\psi,M')=(0.5,0.5)$, and some other $M''$ such that $\calP(\psi,M'')=(3/4,1/4)$. Can we find a subset of measurements $\{M\}$ which allow us to optimally describe any possible outcome distribution resulting from a given state?
We would then also need to understand how changing $\psi$ would affect the results.

\begin{itemize}
	\item Such an optimal description would also come with a way to write measurements in terms of other measurements. What exactly would that correspond to, from a physical standpoint? Measuring $\alpha\sigma_x+\beta\sigma_y$ can be reduced to measuring $\sigma_z$ after evolving the state through an appropriate basis-changing unitary.
	This would correspond to saying that for any $M'$, there is some $\psi'$ such that $\calP(\psi,M)=\calP(\psi',M')$.
	Question is: is it necessary to discuss \emph{evolution} to only describe possible measurement outcomes?
	\item I guess the problem is to characterise what are the possible functional relationships $(\psi,M)\mapsto \calP(\psi,M)$. QM would have us think we need to parametrise $\psi$ as a complex vector (more specifically, $\psi\in\mathbb{CP}^n$) and $M$ as a Hermitian operator, and then $\calP(\psi,M)=\mel{\psi}{M}{\psi}$, but is this type of description really the only possible one?
	\item How can we inject into $M\mapsto \calP(\psi,M)$ the fact that there is some underlying ``thing'' (the state $\psi$) that is being measured in different ways? If $\psi$ is only characterised by $\calP(\psi,M_\psi)=(1,0)$ for some $M_\psi$, then there probably is no way to distinguish between different $M_\perp$ such that $\calP(\psi,M_\perp)=(0.5,0.5)$.
	At the same time, QM suggests that there \emph{cannot} be arbitrarily many such $M_\perp$. On the contrary, for a qubit, there are two and only two such ``unbiased'' measurements.
	Physically, that would mean that there cannot be more than two measurement setups leading to maximally entropic outcomes.
	There is probably some assumption on what kinds of ``measurements'' are accepted here, as one can imagine there being many ways to completely destroy the state in order to get unpredictable outcomes.
	\item One can imagine there being an underlying \emph{master distribution} which fully determines all outcomes (Bell be damned).
	Such distribution would have to also ``cause'' the choices of measurements, thus rendering the concepts of conditional probabilities somewhat non-fundamental.
	Note that we cannot have any ``hidden variable'' or similar parameter capable of predicting measurement outcomes. This is because we are not really describing the state \emph{per se}, but rather its interaction with its surroundings/measurement apparatus.
\end{itemize}

Suppose we have binary random variables $X_i$ which are all \emph{mutually unbiased}, meaning that $\Pr(X_i=x_i|X_j=x_j)=1/2$ for all $i,j$ and $x_i,x_j\in\{0,1\}$. I guess an important aspect here is that we are assuming that the system that is being measured is always the same. There must therefore be some bound on the total amount of information that can be stored in it.
At the same time, the random variables correspond to different measurements, \emph{i.e.} different ways in which we can interact with the system. Each one of these thus \emph{inject} some new information into the system, to some degree.
That is why we need to talk of a probability assignment of the form $\calP(\psi,M)$, which depends both on state \emph{and} measurement.

\section{What is this ``\emph{QM}'' you keep blabbering about?}

\section{Mach-Zender setup}

\heading{Summary of section}
In this section we revisit the standard Mach-Zender setup, but focusing on the relation between different probabilities.
Can everything be described exclusively in terms of probabilities?

\heading{The setting}
Consider the simple Mach-Zender setup, with a single photon passing through two BSs, with the inputs of the second equal to the inputs of the first one. We can write the corresponding evolution as
$a_1^\dagger\to a_1^\dagger + a_2^\dagger \to a_1^\dagger$, or equivalently $\ket0\to H\ket0\to H^2\ket0=\ket0$.

\heading{The paradox}
Measuring between the two BSs, the photon can be found in one of the two possible positions with equal probability. We can therefore meaningfully talk of the corresponding probability distribution $p(\on{left})=p(\on{right})=1/2$.
We can also talk about \emph{conditional} probabilities of observing the photon in, say, the first output of the second BS, when the photon has previously been (measured and) found in the $i$-th branch of the apparatus.
This will read $p(\text{out}_1|\text{left})=p(\text{out}_1|\text{right})=1/2$.
Classically, we should expect from these two facts that the final probability is given by
\begin{equation}
	p(\text{out}_1) =
	p(\text{out}_1|\text{left}) p(\text{left})
	+ p(\text{out}_1|\text{right}) p(\text{right}) = 1/2.
	\label{MZ:eq:conditional_probabilities_classical_law}
\end{equation}
This is \emph{not} what is observed, nor what QM predicts, which is $p(\text{out}_1)=1$.
Why is this so? What went wrong in the above argument?

\heading{The underlying wrong assumption}
The misguided assumptions in the above argument is the existence of a global probability distribution $p(x,y)$, with $x$ ``final position of the photon'' and $y$ ``position the photon is found in between the BSs''.
If there is such a probability distribution, then we can always write
\begin{equation}
	p(x) = p(x|Y=0) p(Y=0) + p(x|Y=1) p(Y=1).
\end{equation}
Note that the above is nothing but a rewriting of the very definition of marginal and conditional probabilities.
That this basic property of probability distributions fails in our MZ, means that it was wrong of us to assume the well-definedness of an underlying $p(x,y)$.
The existence of such an object, physically, means the possibility to, at least in principle, predict the value of $x$ and $y$.

\heading{``Measuring'' is itself an event to take into account}
The next natural question is ``why should $p(x,y)$ be ill-defined?''.
The answer lies in the observation that the \emph{act of measuring itself}, in this context, affects the possible outcomes of the apparatus. If no measurement is performed $p(\text{out}_1)=1$, but if a measurement is performed in between the BSs, we have $p(\text{out}_1)=1/2$ instead.
In other words, in~\cref{MZ:eq:conditional_probabilities_classical_law}, we did not account for the third possibility of no measurement performed between the BSs.
Doing that, we realise that the correct statement is quite trivial, because $p(\text{out}_1)=1$ only when no measurement is performed, which is an event incompatible with measuring the photon left or right (which are both sub-cases of the "measuring" instance).


\section{States to density matrices}

Let $\psi\in\calS(\CC^N)$ be a state, that is, a complex vector with unit norm.
Denote with $\psi_\RR\in\RR^{2N}$ its \emph{realification}, defined by
$\psi_\RR\equiv(\psi_R,\psi_I)^T$, where $\psi_R,\psi_I\in\RR^N$ are real and imaginary parts of $\psi$, respectively. We have
\begin{equation}
	\langle \psi_\RR,\psi_\RR\rangle_{\RR^{2N}}
	= \|\psi_\RR\|^2 = \|\psi_R\|^2 + \|\psi_I\|^2
	= \|\psi\|^2 \equiv \langle \psi,\psi\rangle_{\CC^N}.
\end{equation}
For any $\phi\in\calS(\CC^N)$, we have
\begin{equation}
	\langle \psi,\phi\rangle_{\CC^N}
	\equiv \psi^\dagger \phi
	= \sum_{k=1}^N \bar\psi_k \phi_k
	= (\psi_R^\dagger\phi_R + \psi_I^\dagger \phi_I)
	+ i(\psi_R^\dagger \phi_I - \psi_I^\dagger\phi_R),
\end{equation}
and therefore
\begin{equation}
	\lvert\langle \psi,\phi\rangle\rvert^2
	= \lvert \langle \psi_\RR,\phi_\RR\rangle \rvert^2
	+ \lvert \langle \psi_\RR,\EE\phi_\RR\rangle \rvert^2,
\end{equation}
% \begin{equation}
% 	\lvert \psi^\dagger \phi\rvert^2
% 	= \lvert\psi_\RR^\dagger \phi_\RR\rvert^2
% 	+ \lvert\psi_\RR^\dagger \EE \phi_\RR \rvert^2,
% \end{equation}
where $\EE\equiv \begin{pmatrix}0 & I \\ -I & 0\end{pmatrix}$ is the symplectic matrix.

Given $\psi\in\calS(\CC^N)$, we wish to characterise
\begin{equation}
	\{ \lvert\langle \phi,\psi\rangle\rvert^2 : \,\,\phi\in\calS(\CC^N) \}
	\subseteq [0,1].
\end{equation}

\section{Linear optics}

The general interaction Hamiltonian in a \emph{linear optical} setup has the form
\begin{equation}
	H = \sum_{jk} A_{jk} a_j^\dagger a_k.
\end{equation}


\section{QRC}

\subsection{``\emph{Training QRCs}'' vs POVM reconstruction}
\emph{Summary:}
\begin{enumerate}
	\item The ``quantum reservoir computing'' setup amounts to some type of POVM;
	\item The training data is sufficient to effectively characterise the POVM corresponding to the apparatus;
	\item Training of the reservoir works if and only if the training dataset is sufficient to fully characterise the corresponding POVM;
	\item When the POVM corresponding to the reservoir is known, characterising the retrievable information, and retrieving any retrievable observable, is be done easily without any ML-like algorithm.
\end{enumerate}

\textcolor{red}{\emph{Do ``standard applications'' of quantum reservoir computing (QRC) manage to use information more efficiently than approaches which require knowledge of the underlying channel (\emph{i.e.} require to perform process tomography at some point)?}}
No. To see this, observe that the QRC scheme can, when all is said and done, be described as some measure (POVM) $\{\mu(a)\}_{a=1}^m$ applied to a set of states $\rho_i$. This POVM incorporates knowledge of the full dynamics and whatever measurement is performed on the final reservoir state.
The reason I think in terms of a POVM, rather than of a channel followed by a measurement, is that this more closely describes the situation we are faced: neither the channel nor the measurement on the reservoir change throughout the protocol, thus there is not much of a point to distinguish them at this stage, and we can reason in terms of the overall effective measurement the setup amounts to.

The ``training data'' the algorithm is fed is a set of the form
\begin{equation}
	\calC \equiv \{ (\rho_i, \bs p_i) \}_{i=1}^N,
\end{equation}
where $\rho_i$ are the input states in the training dataset, and $\bs p_i\in\RR^m$ the probabilities corresponding to whatever measurement was performed on $\rho_i$.
The POVM corresponding to the underlying dynamics is assumed to be ``uncontrolled'': unknown at first, but fixed throughout the experiment.

Observe that knowing $\calC$ amounts to a partial reconstruction of the measurement $\{\mu(a)\}_a$, that is, of the reservoir. To see this, note that, as long as the vectors $\bs p_i$ in $\calC$ come from some measurement of $\rho_i$, then $\bs p_i= (\Tr(\mu(a)\rho_i))_{a=1}^m$.
Therefore, there must be some matrix $M$ such that
\begin{equation}
	\tilde p = M \tilde \rho,
\end{equation}
where $\tilde\rho\equiv \sum_i \on{vec}(\rho_i)\bs e_i^\dagger$ is the matrix whose $i$-th column is (a vectorised form of) $\rho_i$, and $\tilde p\equiv \sum_i \bs p_i \bs e_i^\dagger$ is the matrix whose $i$-th column is $\bs p_i$.
Here, the matrix $M$ describes the POVM corresponding to the setup.
More precisely, its $a$-th row is the (vectorised form of the) $a$-th POVM element $\mu(a)$:
$M= \sum_a \bs e_i \on{vec}(\mu(a))^\dagger$.
Knowing $\tilde\rho$ and $\tilde p$ we thus recover $M$ as
\begin{equation}\label{eq:QRC_POVM_from_trainingdataset}
	M = \tilde p \tilde\rho^+.
\end{equation}
This solution is unique as long as the rank of $\tilde\rho$ is equal to the dimension of the underlying state space. If the number of linearly independent states used in $\calC$ is smaller than this, then $M$ --- and thus $\{\mu(a)\}_a$ --- is only partially determined (in a way that can be characterised precisely).

The gist of this is that, as long as the training dataset is made of states (which we assume to be fully characterised) paired with associated probabilities resulting from \emph{some} measurement, then knowing the training dataset amounts to knowledge of the measurement describing the reservoir dynamics.
This is akin to how, in the context of complex media, people characterise the transmission matrix of their (uncontrolled but fixed) complex medium, except here we are not restricting to linear optical systems.

\textcolor{red}{\emph{(Generalisation capabilities of a QRC scheme)}}
Suppose we used our training dataset $\calC$ to estimate the effective POVM of the apparatus using~\cref{eq:QRC_POVM_from_trainingdataset}.
We thus obtained a description for the POVM $\{\mu(a)\}_{a=1}^m$, via the matrix $M$.
If the number of training states is not too large, $M$ is only determined modulo a few translations.
Suppose we are now given a new, previously unseen, state $\rho$.
Can we predict the resulting probabilities $\bs p$?

The answer is clearly yes when the states in the training dataset are a basis for the corresponding space of Hermitian operators.
More generally, each POVM operator $\mu(a)$ is only determined modulo the vector space orthogonal to the span of the states $\{\rho_i\}_i$ in the training dataset. This is because each $\mu(a)$ is only determined by the relations
% \begin{equation}
	$\langle \mu(a), \rho_i \rangle = (\bs p_i)_a$,
% \end{equation}
which are left invariant by translations of the form
$\mu(a)\to \mu(a) + \sigma$ for any $\sigma\in\on{span}_\RR(\{\rho_i\}_i)_\perp$.
Further constraints can probably be imposed to enforce these operators to sum to the identity.

Therefore, given $\rho$, the possible probabilities $\bs p$ have the form
\begin{equation}
	(\bs p(\rho))_a \in \langle \mu(a), \rho\rangle + \langle \sigma(a),\rho\rangle,
\end{equation}
where $\sigma(a)\in\on{span}_\RR(\{\rho_i\}_i)_\perp$.
It is worth noting that this is really a straightforward result: we are simply saying that the POVM is only determined by the way it acts on the input states used in the training, and thus that we will not be able to predict outcome probabilities for input states that are orthogonal to the training ones.
The operators $\sigma(a)$ are the ``pieces of the POVM'' which are invisible to us due to a restricted training dataset.

\textcolor{red}{\emph{But why isn't the reservoir more efficient to extract specific properties?}}
Suppose the goal is to reconstruct a target observable $\calO$.
This means that we want, from $\calC$, to find a vector $\bs w$ such that, for all input states $\rho$, we have
$\langle \bs w, \bs p(\rho)\rangle = \Tr(\calO\rho)$,
where $\bs p(\rho)$ is determined by the apparatus in use.
But such a $\bs w$ exists \emph{iff} $\on{span}_\RR(\{\mu(a)\}_a)$ contains the span of the eigenstates of $\calO$.
Moreover, assuming existence of $\bs w$, we need to have information about the way the POVM acts on all input states, that is, we need the $\rho_i$ in $\calC$ to span the full state space, for if that was not the case, then there would be no knowledge about $\bs p(\rho')$ for some $\rho'$, and it wouldn't be possible to find a suitable $\bs w$ to handle such cases.

In other words, this is saying that to extract \emph{any} observable using the QRC scheme, we need the training set $\calC$ to contain a \emph{basis} of states. But when that is the case, as we already argued, we also have full knowledge of the POVM describing the apparatus, and we can thus fully characterise which observables can be retrieved from the measurements, and which linear maps $\bs w$ to use to do it.

\textcolor{red}{\emph{(Cost of QRC scheme vs process tomography)}}
Quantum process tomography of a channel $\Phi\in\rmC(\calX,\calY)$ requires characterisation of $\langle\sigma_i,\Phi(\rho_j)\rangle\equiv\Tr(\sigma_i\Phi(\rho_j))$ for some bases $\{\sigma_i\}_i\subset\rmD(\calY)$ and $\{\rho_j\}_j\subset\rmD(\calX)$.
This requires collecting statistics for $\dim(\calX)^2\dim(\calY)^2$ different probabilities.

Reasoning in terms of a POVM $\{\mu(a)\}_{a=1}^M$ rather than a channel is, in this context, completely equivalent, in the sense that ``quantum POVM reconstruction'' of $\{\mu(a)\}_a$ amounts to process tomography of a corresponding entanglement breaking channel.
Such a reconstruction requires estimation of $M\dim(\calX)^2$ probabilities.

\textcolor{red}{\emph{(Training QRCs for more general tasks)}}
An underlying assumption in the above discussion is that our goal is to retrieve a \emph{linear} functional of the input states.
We furthermore showed how it is not possible to retrieve different types of functions of the state (at least not precisely).
Properties which are expressible as polynomial functionals of states --- that is, functions of the form $\rho\mapsto \Tr(\calO\rho^{\otimes n})$ for some $n\in\NN$ --- can similarly be retrieved using $n$ copies of the input state $\rho$.

More generally, suppose we encoded some type of classical information into a state $\rho$, and we wish to train the apparatus to retrieve some function of it. This is a setup that is found often in the QRC literature.
We can model this situation saying that the input state is parametrised by some $\alpha$: $\rho_\alpha$.
For example, we might have $\rho_\alpha\equiv\ketbra{\psi_\alpha}{\psi_\alpha}$ with $\ket{\psi_\alpha}\equiv \sqrt{\alpha}\ket0+\sqrt{1-\alpha}\ket1$, with $\alpha\in\{0,1\}$ a bit.
Training the reservoir results in some observable $\calO$, and thus the QRC effectively implements a function of the form
\begin{equation}
	\alpha\mapsto \Tr(\calO\rho_\alpha).
\end{equation}
During the training, the QRC is trained to implement some target function $f:\alpha\mapsto f(\alpha)\in\RR$.
It follows that this task is achievable \emph{iff} $f$ is, or can be approximated with, $\alpha\mapsto \Tr(\calO\rho_\alpha)$.

This does not address the issue of being able to efficiently encode classical information into a quantum state, which is a notoriously delicate issue, but that is a story for another day.


\subsection{Entanglement detection with QRC}

% \textcolor{red}{\emph{(Entanglement witnesses with QRC)}}
One problem associated with experimentally measuring an entanglement witness $\calW$ is how to do it when only \emph{local} measurements are available in a given experimental platform.
Indeed, a witness is an intrinsically nonlocal operator $\calW\in\Herm(\calX\otimes\calY)$.
To avoid having to perform projections of the experimental state with entangled states, one can resort to decomposing $\calW$ in terms of a basis of \emph{local} Hermitian operators, which can then be measured sequentially:
\begin{equation}
	\calW = \sum_{ij} c_{ij} \sigma_i^\calX\otimes\sigma_j^\calY.
\end{equation}
This works, albeit with the downside of having to perform many measurements in several different measurement bases.

% In the context of QRC, we have to 

% Entanglement witnesses avoiding problem of local measurements?
% Problem amounts to measuring observables.

How do we achieve this with photonics? Reservoir is OAM?
We can use the optical setup to obtain two photons entangled in their OAM and polarization degrees of freedom.
This is the input state. To probe its entanglement, we would then need to have these interact nonlocally with something else (the ``reservoir'').
This requires the photons to interact again.

Can we restrict to doing local measurements on reservoir by having it entangled beforehand?
Probably with a measurement-based-like approach.
In the context of photonics, this still involves having the photons interact with each other again after they were entangled.

Witnesses \emph{tailored} to a given reservoir?

\section{Shadow tomography \emph{a la} Huang et al.}

These are schemes to assess efficiently a number of (often, but not necessarily, linear) functions of a state $\rho$, using a relatively small number of measurements.
It is worth stressing that these schemes do not require to assess expectation values, but rather operate directly on the observed configurations sampled in different measurement bases~\parencite{aaronson2017shadow,huang2020predicting}.
One of the main results, as reported in~\parencite{huang2020predicting}, is that one can reliably estimate $M$ observables of a state $\rho$, up to additive error $\epsilon$, from $\calO( \|\cdot\|_{\rm sh}\log(M)/\epsilon^2)$ measurements, where here $\|\cdot\|_{\rm sh}$ denotes a constant factor that depends on which observables one is trying to estimate.
Most notably, the dimension of the state does not affect the required number of observations.

% Discorso sul fare shadow tomography nel caso di mappe. Il calcolo viene usando mappe random e l'aggiunta per ``invertire'' il risultato ottenuto.

\subsection{Review of scheme \textit{a la} Huang \emph{et al.}}

\heading{Goal}
We are given an unknown $N$-qubit system $\rho$ that we aim to characterise. The goal of shadow tomography is to infer accurately a number of observables (or other functions of) $\rho$ from a relatively small number of measurements on independent copies of $\rho$.
Crucially, the protocol does not require estimating the \textit{outcome probabilities} corresponding to each measurements. On the contrary, a ``measurement'' means here to observe a single outcome in each measurement basis. That is the information used to reconstruct the state.

\heading{Measurement setup}
Each measurement is performed in the computational basis after evolving $\rho$ through a unitary $U$ drawn from some distribution $\calP(U)$.
In other words, each measurement is characterised by a unitary $U$; the measurement outcome is some $\ket b$ sampled according to the distribution corresponding to the diagonal elements of $U\rho U^\dagger$, meaning that the probability that $\ket b$ is the measurement outcome, when the unitary $U$ was used, is $(U\rho U^\dagger)_{bb}\equiv \mel{b}{U\rho U^\dagger}{b}$.

\heading{Introduction of map describing the estimation protocol}
More precisely, rather than only storing the sequence of outcomes $\ket b$ obtained at each iteration, the protocol involves storing the so-called \textit{classical snapshots} $\hat\sigma\equiv U^\dagger \ketbra b U$ corresponding to the observed $\ket b$.
We then ask: \textit{on average, what do classical snapshots look like?} The answer is encoded in the channel
\begin{equation}
    \calM(\rho)
    = \int d\calP(U)\,
    \sum_b P_\rho(b|U) U^\dagger \ketbra b U,
\end{equation}
where the integration over the unitaries is taken with respect to some probability measure $\calP(U)$, and $P_\rho(b|U)\equiv \mel{b}{U\rho U^\dagger}{b}$. We can also write this as $\calM(\rho)=\EE[\hat\sigma]$, observing that both the sum over $U$ and that over $b$ can be understood as averages over some distribution.
The central idea of the scheme is then to
\begin{itemize}
    \item observe that $\calM$ is a quantum channel, and thus in particular \emph{a linear map}, which is invertible if the unitary ensemble is chosen appropriately;
    \item obtain an estimate of $\hat\sigma$ by repeating the experiment $M$ times, and thus obtain an estimate for $\rho\approx \calM^{-1}(\EE[\hat\sigma])$.
    The linearity of $\calM^{-1}$ implies that $\hat\rho\equiv \calM^{-1}(\hat\sigma)$ is an unbiased estimator for $\rho$.
\end{itemize}

\heading{Inverting $\calM$}
It is worth stressing that the scheme operates under the assumption that we know what ensemble of unitaries is being used. This means that the map $\calM$ is also known, and thus its inverse $\calM^{-1}$ can be calculated. These calculations are done \textit{before} any experiment is performed.
In practice, the reconstruction of $\rho$ is done by estimating $\hat\sigma$ via repeated measurements and then applying $\calM^{-1}$ to it.

It is also worth noting that even if $\calM$ is a quantum channel and is invertible as a quantum map, that does not mean that its inverse $\calM^{-1}$ is also a quantum channel.
Indeed, this is in general not the case, as unitary channels are the only quantum channels whose inverse is also a quantum channel~\parencite{watrous2018theory}.

\heading{Retrieval of nonlinear functionals}
One does not need to only focus on reconstructing the state with this approach. The goal can be any linear or non-linear function of the state.
In~\parencite{hu2021hamiltoniandriven} they repot as an example that an expectation value $\Tr[\calO(\rho\otimes\rho)]$ can be estimated as
\begin{equation}
\begin{gathered}
    \Tr[\calO(\rho\otimes\rho)]
    =\Tr[\calO(\EE[\hat\rho]\otimes\EE[\hat\rho])] \\
    \simeq
    \frac{1}{M(M-1)}
    \sum_{i\neq j}\Tr[\calO(\hat\rho_i\otimes\hat\rho_j)].
\end{gathered}
\end{equation}
\highlight{(I don't fully understand this though: \emph{e.g.} why are the $i=j$ terms excluded?)}

\subsection{A different way to understand shadow tomography}

\heading{Notation}
A first observation is that the formalism with the channel $\calM$ can be reformulated in the language of POVMs.
To fix notation: the state to be reconstructed will be denoted with $\rho\in\rmD(\calX)$, $\calX$ being the underlying (generally finite-dimensional) Hilbert space; the unitaries applied to it are $U\in\rmU(\calX,\calX)$, with $\rmU(\calX,\calY)$ denoting the set of isometries $\calX\to\calY$; the set of possible outcomes in each basis are $b\in\Sigma$ for some finite register $\Sigma$.
A \emph{POVM} is defined as a collection $\mu\equiv\{\mu(a): a\in\Sigma\}\subset\Pos(\calX)$ of positive operators such that $\sum_{a\in\Sigma}\mu(a)=I_{\calX}$.
Let us denote with $\on{POVM}(\calX)$ the set of POVMs on $\calX$.

\heading{The goal}
The overarching goal is to build an \emph{estimator} $\hat\rho$ for the quantum state. This estimator is to be understood as our best guess for the underlying unknown state that resulted in an observed measurement.
Given such an estimator, we then define in the naive way estimators for the expectation value of target observables and similar quantities.
For example, if we want to estimate the expectation value $\Tr(\rho O)$ for some observable $O\in\Herm(\calX)$, we will then use the estimator $\hat o\equiv\Tr(\hat\rho O)$.
Moreover, importantly, we ideally want an estimator that allows to reconstruct some number $M$ of observables with $\calO(\log M)$ resources, and that does not depend on the dimension of the measured state itself.
It is worth stressing that this estimator is to be defined on \emph{individual measurement outcomes}. In other words, we want to be able to have a guess for the underlying state \emph{before being able to collect the measurement statistics associated with the measurement}.

\heading{Requirements on the estimator}
It follows that the estimator must depend on (1) the measurement that is being performed, and (2) the observed outcome of said measurement.
To not lose in generality, we make no assumption on the performed measurement, considering a generic POVM. Therefore, the estimator must be a map that, given a choice of POVM and of measurement outcome, returns a state:
\begin{equation}
	\hat\rho: \on{POVM}(\calX)\times \Sigma\ni (\mu,b)
	\mapsto \hat\rho_\mu(b) \in \rmD(\calX).
\end{equation}
We write the $\mu$ dependence via subscript only for the purpose of notational clarity.
The most natural requirement for such an estimator is for it to be \emph{unbiased}. This means that we want, for any $\mu\in\on{POVM}(\calX)$ and state $\rho\in\rmD(\calX)$, the following:
\begin{equation}\label{shadow:eq:unbiasedness_requirement}
	\EE_\rho[\hat\rho_\mu] \equiv
	\underset{b\sim p(\mu,\rho)}{\EE}[\hat\rho_\mu(b)] = \rho,
\end{equation}
where $p(\mu,\rho)$ is the probability distribution associated with measuring the state $\rho$ with the POVM $\mu$:
$p(\mu,\rho) = (\Tr(\mu(b)\rho))_{b\in\Sigma}$.

\heading{Consequences of unbiasedness requirement}
Making~\cref{shadow:eq:unbiasedness_requirement} explicit, we get
\begin{equation}\label{shadow:eq:explicit_unbiasedness_condition}
	\sum_{b\in\Sigma}
	\Tr(\mu(b) \rho) \hat\rho_\mu(b)
	\equiv \sum_{b\in\Sigma}
	\langle\mu(b),\rho\rangle\hat\rho_\mu(b) = \rho,
\end{equation}
where we highlighted the (Hilbert-Schmidt) inner product structure in the space of operators: $\langle A,B\rangle\equiv\Tr(A^\dagger B)$ for any pair of linear operators $A,B$.
We now recognise that~\cref{shadow:eq:explicit_unbiasedness_condition}, if we think of the Hermitian operators as vectors in the underlying Hilbert space $\Herm(\calX)$, amounts to finding a decomposition of the form
\begin{equation}\label{shadow:eq:dual_frame}
	\sum_i \langle \bs u_i, \bs v\rangle \bs w_i = \bs v,
\end{equation}
where we identified $\bs v\simeq\rho$, $\bs u_i\simeq \mu(b)$, and $\bs w_i\simeq \hat\rho_\mu(b)$.
As we show in the exercise below, the solution for $\hat\rho_\mu$ is
\begin{equation}
	\hat\rho_\mu(a) = \sum_b \mu(b) (g^{-1})_{ba},
\end{equation}
where $g\equiv g_\mu$ is the matrix $g_{ab}\equiv\langle\mu(a),\mu(b)\rangle$.
In the language of~\cite{huang2020predicting}, $\hat\rho_\mu(a)$ is the \emph{classical shadow} associated with the measurement outcome $a$ (given the knowledge that the underlying measurement is $\mu$).

\begin{exercise}
	Let $\{\bs u_i\}_{i=1}^n\subset\RR^n$ be a linearly independent set. Suppose we want to find $\bs w_i\in\RR^n$ such that, for all $\bs v\in\RR^n$, we have the decomposition
	\begin{equation}\label{shadow:eq:dual_basis_condition}
		\sum_i \langle \bs u_i, \bs v\rangle \bs w_i = \bs v.
	\end{equation}
	% This is the decomposition of a vector $\bs v$ in terms of a set of vectors $\{\bs w_i\}_i$, with coefficients $\langle \bs u_i,\bs v\rangle$.
	% Our task is then, given $\{\bs u_i\}_i$, to find a set $\{\bs w_i\}_i$ such that this relation is satisfies for all $\bs v$.
	% In the case of $\{\bs u_i\}_i$ being \emph{linearly independent},
	This amounts to a linear system with a unique solution: $\bs w_i$ must be chosen as the \emph{dual basis} of $\{\bs u_i\}_i$.
	More explicitly,~\cref{shadow:eq:dual_basis_condition} amounts in matrix notation to the linear system
	\begin{equation}
		W U^\dagger \bs v = \bs v,
	\end{equation}
	where $W\equiv\sum_i \bs w_i \bs e_i^\dagger$ and $U\equiv \sum_i \bs u_i \bs e_i^\dagger$ are the matrices whose $i$-th column is $\bs w_i$ and $\bs u_i$, respectively.
	Note that $U$ is invertible \emph{iff} $\{\bs u_i\}_i$ are linearly independent, and therefore the solution to the problem is trivially $W=(U^\dagger)^{-1}$.
	We can also observe that, leveraging the pseudo-inverse and observing that $U^+=U^{-1}$ for $U$ invertible, we can write
	\begin{equation}
		W = U (U^\dagger U)^{-1},
	\end{equation}
	where $U^\dagger U$ is the matrix of inner products: $(U^\dagger U)_{ij}=\langle \bs u_i, \bs u_j\rangle$.
	Note that, in different, more geometrically minded contexts, $U^\dagger U$ is referred to as a \emph{metric tensor} for the coordinate change.
\end{exercise}

\heading{General formalism: continuous frames}
The above formalism assumes a POVM with only a finite number of possible outcomes. Even more restrictive, it assumes that the components of the POVM are \emph{linearly independent}.
However, this restrictions can be lifted entirely via the machinery of \emph{frame theory}~\parencite{christensen2003introduction}.
Given a separable Hilbert space $\calH$, and a measure space $(X,\mu)$, we say that a family of vectors $\{f_x\}_{x\in X}$ is a \emph{continuous frame} if there are positive constants $A,B>0$ such that
\begin{equation}
	A\|f\|^2 \le \int_X d\mu(x)\, \lvert \langle f_x,f\rangle\rvert^2 f_x \le B \|f\|^2,
	\quad \forall f\in\calH.
\end{equation}
Intuitively, a frame is a generalisation of a basis: a collection of vectors which allows to decompose --- albeit possibly non-uniquely --- arbitrary elements of the underlying vector space.
One can show that if $\{f_x\}_{x\in X}$ is a frame, then any $f\in\calH$ can be decomposed as
\begin{equation}
	f = \int_X d\mu(x)\, \langle S^{-1}(f_x), f\rangle f_x
	=\int_X d\mu(x)\, \langle f_x, f\rangle S^{-1}(f_x),
\end{equation}
where $S:\calH\to\calH$ is the linear, bounded operator defined by
\begin{equation}
	S(f) \equiv \int_X d\mu(x)\, \langle f_x,f\rangle f_x.
\end{equation}
The vectors $\{S^{-1}(f_x)\}_x$, forming the so-called \emph{dual frame} of $\{f_x\}$, are biorthogonal to $\{f_x\}_x$. Intuitively, these correspond to the dual vectors we found in the previous section.

It is worth stressing that the discrete a finite-dimensional case discussed previously can be understood as a special case of the formalism with continuous frames, in the special case where $\calH$ is finite-dimensional and a discrete measure is used.
Furthermore, we will show in the following that the formalism used in~\parencite{huang2020predicting} amounts precisely to computing the estimators using the above machinery to compute the dual frame. The formalisms can be seen to be equivalent via the identifications
\begin{equation}
	f \to \rho, \qquad
	(b,U)\to x, \qquad
	\mu(b,U)\to f_x, \qquad
	S\to \calM.
\end{equation}

\heading{Estimators for observables}
Suppose now that the task is to estimate the expectation value of some observable $O\in\Herm(\calX)$.
A natural way to build a corresponding estimator is to leverage our $\hat\rho_\mu$, defining $\hat o_\mu(a)\equiv \Tr[O \hat\rho_\mu(a)]$.
The natural question to ask is then: how many samples/resources are needed to accurately estimate the true value of the quantity?
The answers to this question hinge on the \emph{variance} of the estimator.
For such variance, we find
\begin{equation}
	\Var_\rho[\hat o_\mu] \equiv
	\EE_\rho[\hat o_\mu^2] - \EE_\rho[\hat o_\mu]^2 =
	\sum_{a\in \Sigma} p(a|\mu,\rho) (\Tr[O \hat\rho_\mu(a)])^2
	- (\Tr[O\rho])^2,
\end{equation}
where we observed that $\hat o_\mu$ is also unbiased, and thus $\EE_\rho[\hat o_\mu]=\Tr[O\rho]$, and defined
$p(a|\mu,\rho)\equiv \Tr[\mu(a) \rho]$.
We then write
\begin{equation}
	\sum_{a\in \Sigma} p(a|\mu,\rho) (\Tr[O \hat\rho_\mu(a)])^2 =
	\Tr\left[
		\left( \sum_{a\in\Sigma} \mu(a) (\Tr[O \hat\rho_\mu(a)])^2 \right)
	\rho\right].
\end{equation}
Defining the operator
\begin{equation}
	A_\mu \equiv \sum_{a\in\Sigma} \mu(a)
	(\Tr[O\hat\rho_\mu(a)])^2,
\end{equation}
we thus arrive to the expression
\begin{equation}
	\Var_\rho[\hat o_\mu] =
	\Tr(A_\mu \rho) - (\Tr(O\rho))^2.
\end{equation}
A rough \emph{state-independent} upper bound for this variance is
\begin{equation}
	\Var_\rho[\hat o_\mu] \le
	\Tr(A_\mu \rho) \le \|A_\mu\|_{\rm op},
\end{equation}
where $\|A_\mu\|_{\rm op}$ is the operatorial norm of $A_\mu$, which equals its largest eigenvalue (note that $A_\mu$ is positive semidefinite).

\heading{Fisher information?}
We can also obtain a \emph{lower}-bound for the variance of the estimator, via the \emph{Fisher information}: the Cramer-Rao bound tells us (I think?) that
\begin{equation}
	\Var_\rho[\hat o_\mu] \ge \frac{1}{I(\rho)},
\end{equation}
where $I(\rho)$ is the Fisher information.

\heading{Translation of variance bounds into number of required samples}


\subsection{Huang et al.'s results in the new formalism}

We can recast the results of~\parencite{huang2020predicting} in our formalism as follows.
The POVM they consider is the following:
\begin{equation}
    \mu(b,U) \equiv \calP(U)\, U^\dagger\ketbra b U.
\end{equation}
Note that $\{\mu(b,U)\}_{b,U}$ is a POVM for any choice of ensemble for the unitaries.
Here, $(b,U)$ take on the role of what the single index $b$ was in the previous sections. In other words, this is now a POVM with a possibly infinite number of terms, corresponding to the possibly infinite number of possible unitaries $U$ to draw from.

\heading{Building  the estimator via direct analysis}
\highlight{(this paragraph is probably obsolete given the frame theory formalism)}
Our task is again to define an unbiased estimator for $\rho$ corresponding to this measurement setup.
This will be a function $(b,U)\mapsto \hat\rho(b,U)$ such that
$\hat\rho(b,U)\in\Herm(\calX)$ for any outcome $b\in\Sigma$ and unitary $U$ (drawn from some suitable probability distribution).
The unbiasedness requirement reads in this case
\begin{equation}
	\EE_{x\sim\rho}[\hat\rho(x)] \equiv
	\int dU  \sum_b \Tr[\mu(b,U)\rho] \,\hat\rho(b,U)
	= \int d\calP(U)  \sum_b \langle b|U\rho U^\dagger| b\rangle \,\hat\rho(b,U)
	= \rho,
\end{equation}
which we write concisely as $\rho =\mathbb E_{b,U\sim\rho}[\hat\rho(b,U)]$.
For the sake of clarity, let us now simplify the notation denoting $(b,U)\sim x$. We are then looking for a function $x\mapsto \hat\rho(x)$ such that
\begin{equation}
	\int dx \, p(x|\rho) \hat\rho(x) = \rho,
	\qquad p(x|\rho) \equiv \Tr(\mu(x)\rho).
\end{equation}
To solve this task, consider a function $x\mapsto\hat\sigma(x)$, define the linear mapping $\calM(\rho)=\EE_{x\sim\rho}[\hat\sigma(x)]$, and suppose $\hat\sigma$ is such that $\calM$ is invertible.
Note that the linearity of $\calM$ is consequence of the probabilities in quantum mechanics always being linear with respect to the density matrix $\rho$.
Then if we define $\hat\rho(x) \equiv \calM^{-1}(\hat\sigma(x))$, we get
\begin{equation}
	\EE[\hat\rho(x)] =
	\calM^{-1}(\EE_{x\sim\rho}[\hat\sigma(x)]) = \rho.
\end{equation}
We thus get an unbiased estimator $\hat\rho(x)$ for any suitable choice of $\hat\sigma$.

% In particular,~\parencite{huang2020predicting} use $\hat\sigma(x)=\mu(x)$.

% The expectation value of this $\hat f$ over the probability distribution corresponding to a state $\rho$ is what they denote with $\calM(\rho)$.

% \heading{Understanding Huang scenario as special case of framework defined above}
% \highlight{to do}



\printbibliography

\section*{List of Acronyms}
\begin{acronym}
	\acro{QM}{Quantum Mechanics}
	\acro{CP}{Completely Positive}
	\acro{LOCC}{Local Operations and Classical Communication}
	\acro{SLOCC}{Stochastic Local Operations and Classical Communication}
    \acro{SLM}{Spatial Light Modulator}
    \acro{OAM}{Orbital Angular Momentum}
    \acro{HOM}{Hong-Ou-Mandel}
    \acro{MUB}{Mutually Unbiased Basis}
    \acro{MIO}{Maximally Incoherent Operations}
    \acro{IO}{Incoherent Operations}
    \acro{SVD}{Singular Value Decomposition}
\end{acronym}

\end{document}